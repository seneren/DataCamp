{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an API?\n",
    "\n",
    "API stands for Application Programming Interface, and they act as messengers between software applications, taking a request to a system and receiving a response containing data or services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an example of how to use the API to generate text using the `gpt-4o-mini` model. You can change the prompt to generate different text.xt. The `max_tokens` parameter controls the length of the generated text. \n",
    "\n",
    "\n",
    "```python\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_tokens=100,\n",
    "  \n",
    "        # Enter your prompt\n",
    "        messages=[{\"role\": \"user\", \"content\": \"INSERT YOUR PROMPT HERE\"}]\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "For example replacing `INSERT YOUR PROMPT HERE` with `In two sentences, how can the OpenAI API be used to upskill myself?` will generate a response - On DataCamp's platform, we got the below response.\n",
    "\n",
    "\n",
    "```python\n",
    "    The OpenAI API can be used to access a wealth of information, allowing you to engage in personalized learning by asking questions and receiving detailed explanations on various topics. Additionally, it can facilitate practice and skill development through interactive exercises, coding assistance, or language learning, making the upskilling process both efficient and tailored to your needs.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making requests to the OpenAI API\n",
    "\n",
    "Depending on the model or services required, APIs have different access points for users, called _endpoints_.\n",
    "\n",
    "__API endpoints__\n",
    "\n",
    "Endpoints are like doors in a hospital. Depending on the treatment required, patients use different doors to reach different departments, and likewise, users make requests for different services to different API endpoints.\n",
    "\n",
    "__API authentication__\n",
    "\n",
    "Endpoints may also require authentication before accessing services. API authentication is usually in the form of providing a unique key containing assortment of characters.\n",
    "\n",
    "__API usage costs__\n",
    "\n",
    "It's important to note that many APIs, including the OpenAI API, have costs associated with using their services. For OpenAI, these costs are dependent on the model requested and the size of the model input and output. \n",
    "\n",
    "__Making a request__\n",
    "\n",
    "We'll use OpenAI's own Python library. We start by importing the OpenAI class from openai, which we'll use to instantiate an OpenAI API client. The client configures the environment for communicating with the API. Inside, we specify our API key. \n",
    "\n",
    "Now for the request code. We'll start by creating a request to the chat completions endpoint by calling the `.create()` method on `client.chat.completions`. The chat completions endpoint is used to send a series of messages representing a conversation to a model, which returns a response. Inside this method, we specify the model and the messages to send. The messages argument takes a list of dictionaries where content sent from the user role allows us to prompt the model. Here, we prompt the model to define the OpenAI API. \n",
    "\n",
    "```python\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI(api_key=\"ENTER YOUR KEY HERE\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is the OpenAI API?\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "```\n",
    "\n",
    "The API response:\n",
    "\n",
    "```python\n",
    "    ChatCompletion(id='chatcmpl-AEcQbQekIzxcxVAKYVAjgUAXokgrl',\n",
    "                choices=[Choice(finish_reason='length', index=0, logprobs=None,\n",
    "                                message=ChatCompletionMessage(content='The OpenAI API is a cloud-based service provided by OpenAI that allows developers to integrate advanced AI models into their applications', refusal=None, role='assistant', function_call=None, tool_calls=None))],\n",
    "                    created=1728047673,\n",
    "                    model='gpt-4o-mini-2024-07-18'\n",
    "                    object='chat.completion', service_tier=None, system_fingerprint='fp_f85bea6784',\n",
    "                    usage=CompletionUsage(completion_tokens=25, prompt_tokens=14, total_tokens=39\n",
    "                                        prompt_tokens_details={'cached_tokens': 0},\n",
    "                                        completion_tokens_details={'reasoning_tokens': 0}))\n",
    "```\n",
    "\n",
    "The response from API is a ChatCompletion object, which has attributes like id, choices, created, model, object, service_tier, system_fingerprint, and usage.\n",
    "\n",
    "We can see that the response message is under the .choices attribute, so we'll start by accessing it. _Attributes are accessed using a dot, then the name of the attribute._\n",
    "\n",
    "```python\n",
    "    print(response.choices)\n",
    "```\n",
    "\n",
    "```python\n",
    "    [Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='The OpenAI API is a cloud-based service provided by OpenAI that allows developers to integrate advanced AI models into their applications.', refusal=None, role='assistant', function_call=None, tool_calls=None))]\n",
    "```\n",
    "\n",
    "Notice from the square brackets at the beginning and end, that this is actually a list with a single element. Let's extract the first element to dig deeper. \n",
    "\n",
    "```python\n",
    "    print(response.choices[0])\n",
    "```\n",
    "\n",
    "and the output is:\n",
    "\n",
    "```python\n",
    "    Choice(finish_reason='length', index=0, logprobs=None,\n",
    "    message=ChatCompletionMessage(content='The OpenAI API is a cloud-based service provided by OpenAI that allows developers to integrate advanced AI models into their applications.', refusal=None, role='assistant', function_call=None, tool_calls=None))\n",
    "```\n",
    "\n",
    "Ok - we're left with a Choice object, which has its own set of attributes. The message is located underneath the `.message` attribute, which we can chain to our existing code. \n",
    "\n",
    "```python\n",
    "    print(response.choices[0].message)\n",
    "```\n",
    "\n",
    "Then we get\n",
    "\n",
    "```python\n",
    "    ChatCompletionMessage(content='The OpenAI API is a cloud-based service provided by OpenAI that allows developers to integrate advanced AI models into their applications.', refusal=None, role='assistant', function_call=None, tool_calls=None)\n",
    "```\n",
    "\n",
    "Almost there! Finally, we need to access the `ChatCompletionMessage`'s `.content` attribute. \n",
    "\n",
    "```python\n",
    "    print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "```python\n",
    "    The OpenAI API is a cloud-based service provided by OpenAI that allows developers to integrate advanced AI models into their applications.\n",
    "```\n",
    "\n",
    "There we have it - our model response as a string! We started off with a complex object, but by taking it one attribute at a time, we were able to get to the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and transforming text\n",
    "\n",
    "__Controlling response randomness__\n",
    "We can control the amount of randomness in the model's responses by adjusting the `temperature` parameter. A higher temperature means more randomness, while a lower temperature means more deterministic responses. The default temperature is 1. It ranges from 0 to 2.\n",
    "\n",
    "__Controlling response length__\n",
    "By default, the response from the API is quite short, which may be unsuitable for many use cases. We can control the length of the response by adjusting the `max_tokens` parameter. The default is `16`, but you can increase it to get longer responses.\n",
    "\n",
    "__Understanding tokens__\n",
    "Tokens are the basic units that the model processes. They can be words, parts of words, or even characters. \n",
    "\n",
    "Recall that the API usage costs are dependent on the model used and the amount of input and output text. Each model is actually priced based upon the _cost per number of tokens_, where input and generated tokens can be priced differently. So increasing `max_tokens` will likely _increase_ the usage cost for each request. When scoping the potential cost of new AI features, the first step is often a back-of-the-envelope calculation to determine the cost per unit time.\n",
    "\n",
    "Rough calculation\n",
    "\n",
    "`Cost/Time = Avg. Tokens Generated x Model Cost x 1000 x (Expected no. of requests/Time)`\n",
    "\n",
    "For example, if we expect to generate 1000 tokens per request, and we expect to make 1000 requests per day, then the cost per day would be:\n",
    "\n",
    "`Cost/Time = 1000 x 0.002 x 1000 x 1000 = $2000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis and classification\n",
    "\n",
    "### Classification tasks\n",
    "\n",
    "__Categorizing animals__\n",
    "\n",
    "```python\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"Classify the following animals into categories: zebra, crocodile, blue whale, polar bear, salmon, dog.\"}, max tokens=50])\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "The output would be something like:\n",
    "\n",
    "```python\n",
    "    Here are the animals classified into categories based on their general classifications:\n",
    "    Mammals: Zebra, Polar Bear, Dog\n",
    "    Fish: Salmon\n",
    "    Reptiles: Crocodile\n",
    "```\n",
    "\n",
    "This might be what we were looking for, but there's an almost infinite number of ways to categorize, so it's better to state the desired categories in the prompt.\n",
    "\n",
    "__Specifying groups__\n",
    " \n",
    "We can update the prompt to categorize animals into those with and without fur, and the model responds with the desired categories.\n",
    "`\"Classify the following animals into animals with fur and without: zebra, crocodile, blue whale, polar bear, salmon, dog.\"`\n",
    "\n",
    "The model responds with the desired categories:\n",
    "```python\n",
    "    Sure! Here is the classification of the animals you provided:\n",
    "\n",
    "    Animals with fur: Dog, Polar Bear, Zebra\n",
    "    Animals without fur: Crocodile, Dolphin, Salmon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot vs. one-shot vs. few-shot prompting\n",
    "\n",
    "* __Zero-shot__ prompting: no examples provided \n",
    "* __One-shot__ prompting: one example provided\n",
    "* __Few-shot__ prompting: multiple examples provided\n",
    "\n",
    "One-shot and few-shot prompting are examples of __in-context learning__ lechniques, where model learns from the context twe're providing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat completions with GPT\n",
    "\n",
    "_Single-turn tasks_ : There's one input and one output.\n",
    "* Text generation\n",
    "* Text transformation\n",
    "* Classification\n",
    "\n",
    "With Chat conversation models, it's possible to also have _multi-turn tasks_, so we can build on previous prompts depending on how the model responds.\n",
    "\n",
    "### Roles\n",
    "\n",
    "Roles are at the heart of how chat models function. Up until this point, we've only used the __user__ role, but there are also __system__ and __assistant__ roles.\n",
    "\n",
    " * __System__ role: controls assistant's behavior - allows us to specify a message to control the behavior of the assistant.\n",
    "   * For example, for a customer service chatbot, we coul dprovice a system message stating that the assistant is a polite and helpful customer service assistant.\n",
    " * __User__ role: instruct the assistant - provide an instruction to the assistant\n",
    " * __Assistant__ role: response to user instruction - is attached ti model responses\n",
    "   * can also be written by the user (us) to provide examples of desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prompt setup__\n",
    "\n",
    "To include additional messages, we extend the list to include multiple dictionaries each with their own role and content. These messages often start with the system role, which here, instructs the assistant to act as a data science tutor that speaks concisely.\n",
    "\n",
    "```python\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \n",
    "               \"content\": \"you are a data science tutor who scpeaks concisely.\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What is the difference between mutable and immutable objects?\"}]\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "```\n",
    "And the output would be: <br>\n",
    "`Mutable objects can be changed after creation, while immutable objects cannot be modified once they are created.`\n",
    "\n",
    "We can see that the assistant stayed true to the system message - only using a single sentence on its concise explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn completions with GPT\n",
    "\n",
    "for single-turn tasks, no content is sent to the assistant role - the model relies only on its existing knowledge, the beaviors sent to the system role, and the isntruction from the user.\n",
    "\n",
    "```python\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \n",
    "               \"content\": \"you are a data science tutor who scpeaks concisely.\"},\n",
    "               {\"role\": \"assistant\", \"content\": \"Lists are defined by enclosing a comma-separated sequence of objects inside square brackets [ ].\"} \n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What is the difference between mutable and immutable objects?\"}])\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "The model now not only has its pre-existing understanding, but also an ideal example to guide its response. \n",
    "\n",
    "  `Mutable objects are objects whose values can change after they are created. Examples of mutable objects in Python include lists, sets and dictionaries. Immutable objects are objects whose values cannot change after they are created. Examples of immutable objects in Python include strings, numbers and tuples.`\n",
    "\n",
    "  With an example to work with, the assistant provides a response in-line with the example.\n",
    "\n",
    "  __Storing responses__\n",
    "\n",
    "  Another common use for providing assistant messages is to store responses. Storing responses means that we can create a conversation history, which we can feed into the model to have conversations. This is exactly what goes on underneath AI chatbots like ChatGPT!\n",
    "\n",
    "  To code a conversation, we'll need to create a system so that when a user message is sent, and an assistant response is generated, they are def back into the messages and stored to be sent with the next user message. Then, when a new user message is provided, the model has the context from the conversation history to draw from.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Building a conversation__\n",
    "\n",
    "1. We start by defining a system message to set the assistant's behavior - you can also add user-assistant example messages here if you wish.\n",
    "\n",
    "```python\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a data science tutor who provides short, simple explanations.\"}\n",
    "    ]\n",
    "```\n",
    "\n",
    "2. Then we define a list of questions:\n",
    "\n",
    "```python\n",
    "    user_qs = [\"Why is Python so popular?\", \"Summarize this in one sentence.\"]\n",
    "```\n",
    "\n",
    "3. Here we aske why Python is popular, and then ask for a summary of the response, which requires context on the previous response. Because we want a response for each question, we start by looping over the `user_qs` list. And next, to convert the user questions into messages for the API, we create a dictionary and add it to the list of messages using the list append method.\n",
    "\n",
    "```python\n",
    "    for q in user_qs:\n",
    "\n",
    "        user_dict = {\"role\": \"user\", \"content\": q}\n",
    "        messages.append(user_dict)\n",
    "```\n",
    "\n",
    "4. We can now swend messages to the Chat Completions endpoint and the store the response.\n",
    "\n",
    "```python\n",
    "        response = client.ChatCompletion.create(\n",
    "            model= \"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "```\n",
    "\n",
    "5. We extract the assistant's message by subsetting from the API response, converting to a dctionary so it's in the messages format. then add it to the messages list for the next iteration.\n",
    "\n",
    "```python\n",
    "        assistant_dict = [{\"role\": \"assistant\", \"content\": response.choices[0].message.content}]\n",
    "        messages.append(assistant_dict)\n",
    "```\n",
    "\n",
    "6. Finally, we'll add two print statements (right after starting the for loop and right before the end of the loop) so the output is a\n",
    "\n",
    "```python\n",
    "    for q in user_qs:\n",
    "        print(\"User\": q)\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        messages.append(assistant_dict)\n",
    "        print(f\"Assistant: {response.choices[0].message.content}\")\n",
    "```\n",
    "\n",
    "So the full code looks like this:\n",
    "\n",
    "```python\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a data science tutor who provides short, simple explanations.\"}      # 1. \n",
    "    ]\n",
    "\n",
    "    user_qs = [\"Why is Python so popular?\", \"Summarize this in one sentence.\"]                                      # 2.\n",
    "\n",
    "    for q in user_qs:                                                                                               # 3.\n",
    "        print(\"User\": q)                                                                                            # 6.\n",
    "        user_dict = [{\"role\": \"user\", \"content\": q}]                                                                # 3.            \n",
    "        messages.append(user_dict)                                                                                  # 3.  \n",
    "\n",
    "        response = client.chat.completions.create(                                                                  # 4.\n",
    "            model=\"gpt-4o-mini\",                                                                                    # 4.    \n",
    "            messages=messages                                                                                       # 4.\n",
    "        )\n",
    "\n",
    "        assistant_dict = [{\"role\": \"assistant\", \"content\": response.choices[0].message.content}]                    # 5.\n",
    "        messages.append(assistant_dict)                                                                             # 5.\n",
    "        print(\"Assistant\": response.choices[0].message.content, \"\\n\")                                               # 6\n",
    "```\n",
    "\n",
    "We can see that we were successfully able to provide a follow-up correction to the model's response without having to repeat our question or the model's response.\n",
    "\n",
    "\n",
    "`User: Why is Python so popular?` <br>\n",
    "`Assistant: Python is popular for many reasons, including its simplicity, versatility, and wide range of available libraries. It has a relatively easy-to-learn syntax that makes it accessible to beginners and experts alike. It can be used for a variety of tasks, such as data analysis, web development, scientific computing, and machine learning. Additionally, Python has an active community of developers who contribute to its development and share their knowledge through online resources and forums.`\n",
    "\n",
    "`User: Summarize this in one sentence.` <br>\n",
    "`Assistant: Python is popular due to its simplicity, versatility, wide range of libraries, and active community of developers.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example\n",
    "\n",
    "```python\n",
    "client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}]\n",
    "user_msgs = [\"Explain what pi is.\", \"Summarize this in two bullet points.\"]\n",
    "\n",
    "for q in user_msgs:\n",
    "    print(\"User: \", q)\n",
    "    \n",
    "    # Create a dictionary for the user message from q and append to messages\n",
    "    user_dict = {\"role\": \"user\", \"content\": q}\n",
    "    messages.append(user_dict)\n",
    "    \n",
    "    # Create the API request\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages = messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Convert the assistant's message to a dict and append to messages\n",
    "    assistant_dict = {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "    messages.append(assistant_dict)\n",
    "    print(\"Assistant:\", response.choices[0].message.content, \"\\n\")\n",
    "```\n",
    "\n",
    "The output will be like:\n",
    "\n",
    "`User:  Explain what pi is.` <br>\n",
    "`Assistant: Pi (π) is a mathematical constant that represents the ratio of a circle's circumference to its diameter. Regardless of the size of the circle, this ratio always remains the same, making pi a fundamental element in geometry. `\n",
    "\n",
    "`The value of pi is approximately 3.14159, but it is an irrational number, meaning that it cannot be exactly expressed as a simple fraction, and its decimal representation goes on infinitely without repeating. Because of this property, pi is often approximated as 3.`\n",
    "\n",
    "`User:  Summarize this in two bullet points.` <br>\n",
    "`Assistant: - Pi (π) is the ratio of a circle's circumference to its diameter, approximately equal to 3.14159. - It is an irrational number, meaning its decimal representation is infinite and non-repeating.'`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Beyond Text Completions\n",
    "\n",
    "### Text Moderation\n",
    "\n",
    "Text moderation is the process of identifying text that is inappropriate for the context it is being used in. \n",
    "\n",
    "__Creating a moderations request__\n",
    "\n",
    "To create a request to the Moderations endpoint, we call the create method on `client.moderations`, and specify that we want the latest moderation model, which often performs the best. Next is the input, which is the content that the model will consider. This statement could easily be classed as violent by traditional moderation systems that worked by flagging particular keywords. \n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "Client = OpenAI(api_key=\"ENTER API KEY\")\n",
    "\n",
    "response = client.moderations.create(\n",
    "    model=\"text-moderation-latest\",\n",
    "    input=\"I could kill for a hamburger.\",\n",
    ")\n",
    "\n",
    "print(response.model_dump())\n",
    "```\n",
    "\n",
    "Let's see what OpenAI's moderation model makes of it.\n",
    "\n",
    "__Interpreting the results__\n",
    "\n",
    "<img src=\"./images/text-moderation-output.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "There are three useful indicators that can be used for moderation: \n",
    "* `categories`\n",
    "  * `true` / `false` values representing whether the model believed that the statement __violate__d any of the categories.\n",
    "* `category_scores`\n",
    "  * A score between `0` and `1`, indicating the model's confidence of a violation.\n",
    "* `flagged`\n",
    "  * A `true` / `false` value representing whether the model believed that the terms of use have been violated in any way.\n",
    "\n",
    "  Let's extract the `category_values` from the response for a closer look.\n",
    "\n",
    "\n",
    "```python\n",
    "    CategoryScores(harassment=2.775940447463654e-05,\n",
    "                   harassment_threatening=1.3526056363843963e-06,\n",
    "                   hate=2.733528674525587e-07,\n",
    "                   hate_threatening=4.930571506633896e-08,\n",
    "                   ...,\n",
    "                   violence=0.0500854030251503,\n",
    "                   ...)\n",
    "```\n",
    "\n",
    "__Interpreting the catefory scores__\n",
    "\n",
    "The `category_scores` are float values for each category indicating the model's confidence of a violation.<br>\n",
    "They can be extracted from the results attribute, and through that, the `category_scores` attribute.<br>\n",
    "The scores can be between `0` and `1`, where `0` means no violation and `1` means a violation. However, the score should not be interpreted as a probability.<br>\n",
    "\n",
    "The beauty of having access to these category scores means that we don't have to depend on the final true/false results outputted by the model, we can instead test the model on data from our own particular use case, and set our own thresholds based on the results. For some use cases, such as student communications in a school, strict thresholds may be chosen that flag more content, even if it means accidentally flagging some non-violations.  The goal here would be to minimize the number of missed violations, so-called __false negatives__. Other use cases, such as communications in law enforcement, may use more lenient thresholds so reports on crimes aren't accidentally flagged. Incorrectly flagging a crime report here would be an example of a __false positive__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-to-Text Transcription with Whisper\n",
    "\n",
    "### OpenAI's Whisper\n",
    "\n",
    "```python\n",
    "    audio_file = open(\"meeting_recording.mp3\", \"rb\")  # open the audio file, \"rb\" means \"read binary\"\n",
    "\n",
    "    transcript = openai.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)   # transcribe the audio file.\n",
    "\n",
    "    print(response)\n",
    "```\n",
    "\n",
    "What we would get:\n",
    "\n",
    "`Transcription(text=\"Welcome everyone to the June product monthly. We'll get started in...\")`\n",
    "\n",
    "`print(response.text)` would yield:\n",
    "\n",
    "`Welcome everyone to the June product monthly. We'll get started in...`\n",
    "\n",
    "### Speech Translation with Whisper\n",
    "\n",
    "```python\n",
    "    audio_file = open(\"non_english_audio.m4a\", \"rb\")  \n",
    "\n",
    "    transcript = openai.audio.translations.create(model=\"whisper-1\", file=audio_file)   # translate the audio file.\n",
    "\n",
    "    print(response)\n",
    "```\n",
    "\n",
    "the only difference from the Audio Whisper is the `translations.create` instead of `transcriptions.create`\n",
    "\n",
    "### Bringing prompts into the mix\n",
    "\n",
    "Improve response quality by:\n",
    "* Providing an example of desired style\n",
    "* provide additional context about transcript\n",
    "\n",
    "```python\n",
    "    audio_file = open(\"non_english_audio.m4a\", \"rb\")\n",
    "    prompt = \"The transcript is about AI trends and ChatGPT\"    # We assumed that the audio file is about AI trends and ChatGPT, as an example.\n",
    "\n",
    "    response = client.audio.translations.create(model=\"whisper-1\", file=audio_file, prompt=prompt)\n",
    "\n",
    "    print(response.text)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining models\n",
    "\n",
    "### Chanining Whisper with a chat model\n",
    "\n",
    "__Example: Extracting meeting attendees__\n",
    "\n",
    "We start by opening the audio file and assigning it audio_file. Next, we send the audio to the Whisper model and request a transcript with the transcribe method. To extract the transcript from the response, we extract the value from the text key.\n",
    "\n",
    "```python\n",
    "    audio_file = open(\"meeting_recording.mp4\", \"rb\")\n",
    "\n",
    "    audio_response = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "```\n",
    "\n",
    "Now that we have the meeting transcript, we can use it to create a prompt for the chat model. The prompt starts with an instruction to extract the attendee names from the start of the transcript, then we append the transcript to the end. \n",
    "\n",
    "```python\n",
    "    transcript = audio_response.text\n",
    "    prompt = \"Extract the attendee names from the start of this transcript: \" + transcript\n",
    "```\n",
    "\n",
    "We're now ready to send the prompt to the chat model. We create a request with to the Chat Completions endpoint using the create method. Inside, we specift the model to use and messages to send, which is kust the prompt in this case.\n",
    "\n",
    "```python\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "```\n",
    "Finally, we extract the response from the chat model.\n",
    "\n",
    "```python\n",
    "    print(chat_response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "Let's see the full code:\n",
    "\n",
    "```python\n",
    "    audio_file = open(\"meeting_recording.mp3\", \"rb\")\n",
    "\n",
    "    audio_response = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file\n",
    "    )\n",
    "    transcript = audio_response.text\n",
    "    prompt = \"Extract the attendee names from the start of this transcript: \" + transcript\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(chat_response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "And there we have it:\n",
    "\n",
    "`The meeting attendees were Otis, Paul, Elaine, Nicola, Alan, and Imran.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Identifying audio language\n",
    "\n",
    "```python\n",
    "    client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n",
    "\n",
    "    # Open the audio.wav file\n",
    "    audio_file = open(\"/audio-files/audio.wav\", \"rb\")\n",
    "\n",
    "    # Create a transcription request using audio_file\n",
    "    audio_response = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "\n",
    "    # Create a request to the API to identify the language spoken\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\":\"user\",\n",
    "            \"content\": audio_response.text\n",
    "        }]\n",
    "    )\n",
    "print(chat_response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "___The output is__:\n",
    "\n",
    "`Hallo! Es ist großartig zu hören, dass du in der Automobilindustrie arbeitest und dich für maschinelles Lernen interessierst. Die Nachfrage nach Elektrofahrzeugen (EVs) wächst stetig, und es gibt viele Faktoren, die dabei eine Rolle spielen, wie Umweltbewusstsein, staatliche Förderungen, technologische Fortschritte und Marktentwicklungen.`\n",
    "    \n",
    "`Hier sind einige Schritte, die dir helfen könnten, eine fundierte Vorhersage zur zukünftigen Nachfrage nach Elektrofahrzeugen zu treffen:`\n",
    "    \n",
    "`1. **Daten sammeln**: Beginne mit der Sammlung relevanter Daten. Dazu gehören Verkaufszahlen von Elektrofahrzeugen, Marktanteile, Preistrends Fertigungskapazitäten, förderliche gesetzliche Rahmenbedingungen und Verbraucherverhalten.`\n",
    "    \n",
    "`2. **Datenanalyse**: Untersuche die gesammelten Daten, um Trends zu identifizieren. Tools wie Pandas in Python können hier sehr nützlich sein.`\n",
    "    \n",
    "`3. **Feature Engineering**: Überlege, welche Faktoren (Features) die Nachfrage nach Elektrofahrzeugen beeinflussen können. Dazu können wirtschaftliche Indikatoren, Ölpreise, die Anzahl der Ladeinfrastruktur und technologische Entwicklungen gehören.`\n",
    "    \n",
    "`4. **Modellierung**: Nutze maschinelles Lernen, um ein Vorhersagemodell zu erstellen. Algorithmen wie lineare Regression, Entscheidungsbäume oder auch komplexere Modelle wie neuronale Netze können hier hilfreich sein.`\n",
    "    \n",
    "`5. **Evaluierung und Validierung**: Teste dein Modell mit einem separaten Datensatz, um die Genauigkeit der Vorhersagen zu überprüfen. Dazu kannst du Metriken wie das Root Mean Squared Error (RMSE) verwenden.`\n",
    "    \n",
    "`6. **Simulation von Szenarien**: Erstelle verschiedene Szenarien, um zu sehen, wie unterschiedliche Faktoren die Nachfrage beeinflussen könnten.`\n",
    "\n",
    "`7. **Aktualisierung und Anpassung**: Halte deine Modelle regelmäßig aktuell, da sich die Marktbedingungen schnell ändern können.`\n",
    "    \n",
    "`DataCamp bietet viele Kurse zum Thema Datenanalyse und maschinelles Lernen, die dir helfen können, das nötige Wissen und die praktischen Fähigkeiten zu entwickeln. Viel Erfolg bei deinem Vorhaben! Wenn du spezifische Fragen hast oder eine tiefere Diskussion über ein bestimmtes Thema führen möchtest, lass es mich wissen!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Creating meeting summaries\n",
    "\n",
    "```python\n",
    "client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n",
    "\n",
    "# Open the datacamp-q2-roadmap.mp3 file\n",
    "audio_file = open(\"../audio-files/datacamp-q2-roadmap.mp3\", \"rb\")\n",
    "\n",
    "# Create a transcription request using audio_file\n",
    "audio_response = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "prompt = \"summarize the following text into concise bullet points:\" + audio_response.text\n",
    "\n",
    "# Create a request to the API to summarize the transcript into bullet points\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(chat_response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "`- Technical Courses:` <br>\n",
    "    `- OpenAI API and Python courses focus on programming with GPT and Whisper (e.g., transcribing meeting notes).`\n",
    "\n",
    "`- Understanding Artificial Intelligence:` <br>\n",
    "    `- Aimed at a less technical audience to provide a broad overview of AI concepts beyond new models.`\n",
    "\n",
    "`- Artificial Intelligence Ethics:`    <br>\n",
    "    `- Emphasizes the importance of proper AI implementation to avoid harmful consequences for businesses and customers.` <br>\n",
    "    `- Encouraged for all to consider taking this course.`\n",
    "\n",
    "`- Data Literacy Courses:` <br>\n",
    "    `- Address communication`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
