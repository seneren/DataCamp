{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Semantic search and enriched embeddings**\n",
    "\n",
    "Popular embedding applications\n",
    "\n",
    "* Semantic search\n",
    "* Recommendation systems\n",
    "* Classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Semantic search**\n",
    "\n",
    "Recall that semantic search engines use embeddings to return the most semantically similar results to a search query. For example, a news website could enable semantic search by embedding news article information like the headline and topic. A user searching \"computer\" could then be returned a selection of computer-related headlines.\n",
    "\n",
    "<img src= './images/semantic-search-for-online-news-website.png' width=50% height=50%>\n",
    "\n",
    "There are three steps to semantic search:\n",
    "\n",
    "1. __Embed__ the search query and the texts to compare against\n",
    "2. Compute the __cosine distances__ between the embedded search query and other  embedded texts\n",
    "3. __Extract__ the texts with the smallest cosine distances.\n",
    "\n",
    "### **Enriched embeddings**\n",
    "\n",
    "Here's the headlines data we'll be working with\n",
    "\n",
    "```python\n",
    "    articles = [\n",
    "        {\"headline\": \"Economic Growth Continues Amid Global Uncertainty\",\n",
    "         \"topic\": \"Business\",\n",
    "         \"keywords\": [\"economy\",\"business\",\"finance\"]},\n",
    "        ...\n",
    "        {\"headline\": \"1.5 Billion Tune-in to the World Cup Final\",\n",
    "         \"topic\": \"Sport\",\n",
    "         \"keywords\": [\"soccer\",\"world cup\",\"tv\"]}\n",
    "    ]\n",
    "```\n",
    "\n",
    "We'll embed the headline, topic, and keywords for each article. To do thus, we'll combine the information from each article into a single string that reflects the information stored in the dictionary, and the keywords are delimited by comma and space.\n",
    "\n",
    "### **Combining features with f-string**\n",
    "\n",
    "To combine these featurees for each article, we'll define a fucntion. This function uses `f-string`, or _formatted string_ to return the desired string structure.\n",
    "\n",
    "```python\n",
    "    articles = [..., {\"headline\": \"1.5 Billion Tune-in to the World Cup Final\",\n",
    "                      \"topic\": \"Sport\",\n",
    "                      \"keywords\": [\"soccer\",\"world cup\",\"tv\"]}]\n",
    "\n",
    "    def create_article_text(article):\n",
    "        return f\"\"\"Headline: {article['headline']}.\n",
    "        Topic: {article['topic']}.\n",
    "        Keywords: {', '.join(article['keywords'])}\"\"\"\n",
    "\n",
    "    print(create_article_text(articles[-1]))\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "        `Headline: 1.5 Billion Tune-in to the World Cup Final.` <br>\n",
    "        `Topic: Sport. ` <br>\n",
    "        `Keywords: soccer, world cup, tv`\n",
    "\n",
    "### **Creating enriched embeddings**\n",
    "\n",
    "To apply the function and combine the features for each article, we use a list comprehension, calling our function on each article in articles. Finally, to embed these strings, we call the `create_embeddings` function on the result. The function `create_embeddings` is defined in the notebook [`1 - What are Embeddings`](./1%20-%20What%20are%20Embeddings.ipynb). Let's recall the function, note that this creates a list of embeddings for each input using the OpenAI API.\n",
    "\n",
    "```python\n",
    "    def create_embeddings(text):\n",
    "        response = client.embeddings/create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        )\n",
    "    response_dict = response.model_dump(\n",
    "\n",
    "    return [data['embedding'] for data in response_dict['data']]\n",
    "    )\n",
    "```\n",
    "\n",
    "```python\n",
    "    articles_text = [create_article_text(article) for article in articles]\n",
    "    articles_embeddings = create_embeddings(articles_text)\n",
    "\n",
    "    print(articles_embeddings)\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "        `[[-0.019609929993748665, -0.03331860154867172, ...],` <br>\n",
    "        `...` <br>\n",
    "        `[..., -0.014373429119586945, -0.005235843360424042]]`\n",
    "\n",
    "### **Computing distances**\n",
    "\n",
    "Now we can compute cosine distances. \n",
    "\n",
    "We'll define a function called `find_n_closest`, that takes a `query_vector` (the embedded search query), and `embeddings` to compare against (our embedded articles), and returns the `n` most similar results based on their cosine distances. \n",
    "\n",
    "For each embedding, we calculate the cosine distance to the `query_vector`, and store it in a dictionary along with the embedding's index, which we append to a list called `distances`. \n",
    "\n",
    "To sort the distances list by the `distance` key in each dictionary, we use the sorted function and its key argument. The key argument takes a function to evaluate each dictionary in distances and sort by; in this case, it's a `lambda` function that accesses the distance key from each dictionary. \n",
    "\n",
    "```python\n",
    "    from scipy.spatial import distance\n",
    "\n",
    "    def find_n_closest(query_vector, embeddings, n=3):\n",
    "        distances = []\n",
    "        for index, embedding in enumerate(embeddings):\n",
    "            dist = distance.cosine(query_vector, embedding)\n",
    "            distances.append({'distance': dist, 'index': index})\n",
    "\n",
    "        distances_sorted = sorted(distances, key=lambda x: x['distance'])\n",
    "        return distances_sorted[0:n]\n",
    "```\n",
    "\n",
    "We will query our embeddings using the text, \"AI\". \n",
    "\n",
    "First, we embed the search query using our `create_embeddings` function and extract its embeddings by 0-indexing. \n",
    "\n",
    "Next, we use the `find_n_closest` function to find the `3` closest `hits` based on our article_embeddings. \n",
    "\n",
    "Finally, to extract the most similar headlines, we loop through each hit, using the hit's index to subset the corresponding headline, and print. \n",
    "\n",
    "```python\n",
    "    query_text = \"AI\"\n",
    "    query_vector = create_embeddings(query_text)[0]\n",
    "\n",
    "    hits = find_n_closest(query_vector, article_embeddings)\n",
    "\n",
    "    for hit in hits:\n",
    "        article = articles[hit['index']]\n",
    "        print(article['headline'])\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "<img src = './images/search_result.png' width = 70% height = 70%>\n",
    "\n",
    "As we'd expect, the top result scpecifically mentions AI, and the others are on similar topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recommendation System**\n",
    "\n",
    "Recommendation systems work almost exactly the same as semantic search engines! We have a group of items that we potentially want to recommend, and at least one data point that we want to recommend based on. \n",
    "\n",
    "1. Embed the potential recommendations and the data point we have\n",
    "2. Calculate cosine distances\n",
    "3. Recommend the item or items that are closest in the vector space.\n",
    "\n",
    "This similarity in approach means that a lot of the code is exactly the same.\n",
    "\n",
    "### **Example: Recommmend articles**\n",
    "\n",
    "We're returning to the news article dataset to design a recommendation system that recommends the three most similar articles to the one currently being read.\n",
    "\n",
    "```python\n",
    "    articles = [\n",
    "        {\"headline\": \"Economic Growth Continues Amid Global Uncertainty\",\n",
    "         \"topic\": \"Business\",\n",
    "         \"keywords\": [\"economy\",\"business\",\"finance\"]},\n",
    "        ...\n",
    "        {\"headline\": \"1.5 Billion Tune-in to the World Cup Final\",\n",
    "         \"topic\": \"Sport\",\n",
    "         \"keywords\": [\"soccer\",\"world cup\",\"tv\"]}\n",
    "    ]\n",
    "```\n",
    "\n",
    "This recommendation will be based on the article's headline, topic, and keywords, stored in a dictionary called `current_article`: \n",
    "\n",
    "```python\n",
    "    current_article = {\n",
    "        \"headline\": \"How NVIDIA GPUs Could Decide Who Wins the AI Race\",\n",
    "        \"topic\": \"Tech\",\n",
    "        \"keywords\": [\"ai\",\"business\",\"computers\"]\n",
    "    }\n",
    "```\n",
    "\n",
    "To prepare both of these objects for embedding, we'll first need to combine the features into a single string for each article.\n",
    "\n",
    "We'll use `create_article_text` function. Let's recall the function:\n",
    "\n",
    "```python\n",
    "    def create_article_text(article):\n",
    "        return f\"\"\"Headline: {article['headline']}.\n",
    "        Topic: {article['topic']}.\n",
    "        Keywords: {', '.join(article['keywords'])}\"\"\"\n",
    "```\n",
    "\n",
    "### **Combining Features**\n",
    "\n",
    "To combine the features, we call the function on each article in articles using a list comprehension, and do the same for the current article. By printing, we can see that the function correctly formatted the current article's information.\n",
    "\n",
    "```python\n",
    "    articles_text = [create_article_text(article) for article in articles]\n",
    "    current_article_text = create_article_text(current_article)\n",
    "    print(current_article_text)\n",
    "```\n",
    "<img src= '/Users/erensen/Downloads/DataCamp/Introduction to Embeddings with the OpenAI API/images/current_article.png' width=65% height=65%>\n",
    "\n",
    "### **Creating Embeddings**\n",
    "\n",
    "Next, we embed both sets of article strings using the `create_embeddings` function from earlier, which allows us to create requests to the OpenAI embedding model in a more repeatable way.\n",
    "\n",
    "```python\n",
    "    def create_embeddings(text):\n",
    "        response = client.embeddings/create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        )\n",
    "    response_dict = response.model_dump(\n",
    "\n",
    "    return [data['embedding'] for data in response_dict['data']]\n",
    "    )\n",
    "```\n",
    "\n",
    "### **Finding the most similar article**\n",
    "\n",
    "Finally, to compute the cosine distances and extract the nearest distances, we'll use our `find_n_closest` function, which computes the cosine distances, sorts them, and returns the n smallest and their index. We'll call this function on both sets of embedded articles, and then loop through the results, returning the headline of each article in the three most similar articles.\n",
    "\n",
    "```python\n",
    "    def find_n_closest(embedding, embeddings, n=3):\n",
    "        distances=[]\n",
    "        for index, embedding in enumerate(embeddings):\n",
    "            dist = spatial.distance.cosine(query_vector, embedding)\n",
    "            distances.append('distance': dist, 'index': index)\n",
    "        distances_sorted = sorted(distances, key=lambda x: x['distance'])\n",
    "        return distances_sorted[:n]\n",
    "\n",
    "hits = find_n_closest(query_embedding, embeddings)\n",
    "for hit in hits:\n",
    "    article = articles[hit['index']]\n",
    "    print(article['headline'])\n",
    "```\n",
    "\n",
    "output: <br>\n",
    "<img src = './images/recommend-articles.png' width=65% height=65%>\n",
    "\n",
    "This is a good start, but a more sophisticated system would use not only the current article to base the recommendations on, _but also the other articles in the user's history_.\n",
    "\n",
    "### **Adding user history**\n",
    "\n",
    "Let's consider that a user has visited two articles, stored in `user_history`\n",
    "\n",
    "```python\n",
    "    user_history = [\n",
    "        {\"headline\": \"How NVIDIA GPUs Could Decide Who Wins the AI Race\",\n",
    "         \"topic\": \"Tech\",\n",
    "         \"keywords\": [\"ai\", \"business\", \"computer\"]},\n",
    "        {\"headline\": \"Tech Giant Buys 49% Stake In AI Startup\",\n",
    "         \"topic\": \"Tech\",\n",
    "         \"keywords\": [\"business\", \"AI\"]}\n",
    "    ]\n",
    "```\n",
    "\n",
    "How can we provide recommendations based on multiple data points?\n",
    "\n",
    "### **Recommandation on multiple data points**\n",
    "\n",
    "This is the situation we have, where the user has seen two articles, embedded in blue, and we want to recommend the most similar article. Articles they haven't seen yet are shown in red. \n",
    "\n",
    "<img src = './images/situation.png' width=25% height=25%>\n",
    "\n",
    "To find the most similar vector to two vectors, \n",
    "1. We'll combine the two vectors into one by taking the mean. \n",
    "2. Then, we'll compute cosine distances as we did before,\n",
    "3. Then\n",
    "    * recommend the closest vector. <br>\n",
    "\n",
    "    <img src = './images/take-mean.png' width=25% height=25%>\n",
    "\n",
    "    * If the nearest point has already been viewed, we'll make sure to return the nearest unseen article.\n",
    "\n",
    "    <img src = './images/recommend-the-unrecommended.png' width=25% height=25%>\n",
    "\n",
    "The first two steps to embed the user_history are the same as before, combining the features for each article, and embedding the resulting strings. The only difference is that we take the mean to aggregate the two vectors into one that we can compare with the other articles. \n",
    "\n",
    "For the articles to recommend, we filter the list so it only contains articles _not_ in the `user_history`. \n",
    "\n",
    "Then, as before, we combine the features and embed the text.\n",
    "\n",
    "```python\n",
    "    def create_article_text(article):\n",
    "        return f\"\"\"Headline: {article['headline']}\n",
    "    Topic: {article['topic']}\n",
    "    Keywords: {\", \".join(article['keywords'])}\"\"\"\n",
    "\n",
    "    history_texts = [create_article_text(article) for article in user_history]\n",
    "    history_embeddings = create_embeddings(history_texts)\n",
    "\n",
    "    mean_history_embedding = np.mean(history_embeddings, axis=0)\n",
    "\n",
    "    article_filtered = [article for article in articles if article not in user_history]\n",
    "    article_texts = [create_article_text(article) for article in article_filtered]\n",
    "    article_embeddings = create_embeddings(article_texts)\n",
    "```\n",
    "\n",
    "Finally, we compute the cosine distances using the same find_n_closest function, only this time, passing it the mean of the embedded user history. Then, we subset the filtered article list to find the most similar articles. \n",
    "\n",
    "```python\n",
    "    hits = find_n_closest(mean_history_embedding, article_embeddings)\n",
    "    \n",
    "    for hit in hits:\n",
    "        article = articles_filtered[hit['index']]\n",
    "        print(article['headline'])\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "<img src= './images/most-similar-articles.png' width=60% height=60%>\n",
    "\n",
    "Notice that the article headlined, \"Tech Giant Buys 49% Stake In AI Startup\" wasn't recommended, as the user had already seen it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In this exercise, you'll make a recommendation system for an online retailer that sells a variety of products. This system recommends three similar products to users who visit a product page but don't purchase, based on the last product they visited.\n",
    "You've been provided with a list of dictionaries of `products` available in `/datasets/`, and a dictionary for the last product the user visited, stored in `last_product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot Building Kit\n",
      "LEGO Space Shuttle\n",
      "Designer Makeup Brush Set\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from scipy.spatial import distance      # to calculate cosine distance\n",
    "import numpy as np                      # to calculate the mean of the embedded user_history\n",
    "import json                             # to load the products.json file\n",
    "import os                               # to get the current working directory\n",
    "from dotenv import load_dotenv          # to load the .env file\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key= api_key)\n",
    "\n",
    "with open('./datasets/products.json', 'r') as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "with open('./datasets/last_product.json', 'r') as file:\n",
    "    last_product = json.load(file)\n",
    "\n",
    "# Create a function to create embeddings for product_descriptions - returns a list of embeddings for each product description\n",
    "def create_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        model = \"text-embedding-3-small\",\n",
    "        input = text\n",
    "    )\n",
    "    response_dict = response.model_dump()\n",
    "    return [data['embedding'] for data in response_dict['data']]\n",
    "\n",
    "# Create a list of product descriptions from the products list using list comprehension\n",
    "product_descriptions = [product['short_description'] for product in products]\n",
    "response = client.embeddings.create(\n",
    "    model = \"text-embedding-3-small\",\n",
    "    input = product_descriptions\n",
    ")\n",
    "response_dict = response.model_dump()\n",
    "\n",
    "# Extract the embeddings from response_dict and store in products.\n",
    "# Now the products list has an additional key 'embedding' for each product, placed after the 'features' key.\n",
    "for i, product in enumerate(products):\n",
    "    product['embedding'] = response_dict['data'][i]['embedding']\n",
    "\n",
    "# Create two lists: one for categories and one for embeddings\n",
    "categories = [product['category'] for product in products]\n",
    "embeddings = [product['embedding'] for product in products]\n",
    "\n",
    "# Define a function to combine the relevant features into a single string\n",
    "def create_product_text(product):\n",
    "    return f\"\"\"Title: {product['title']}\n",
    "    Description: {product['short_description']}\n",
    "    Category: {product['category']}\n",
    "    Features: {\"; \".join(product['features'])}\"\"\"\n",
    "\n",
    "# Combine the features for each product\n",
    "product_texts = [create_product_text(product) for product in products]\n",
    "\n",
    "# Create the embeddings from product_texts\n",
    "product_embeddings = create_embeddings(product_texts)\n",
    "\n",
    "\n",
    "# Returns the n closest distances and their indexes between query_vector and embeddings, based on cosine distance\n",
    "def find_n_closest(query_vector, embeddings, n=3):\n",
    "    distances = []\n",
    "    for index, embedding in enumerate(embeddings):\n",
    "        dist = distance.cosine(query_vector, embedding)       # Calculate the cosine distance between the query vector and embedding\n",
    "        distances.append({\"distance\": dist, \"index\": index})  # Append the distance and index to distances\n",
    "    distances_sorted = sorted(distances, key= lambda x: x['distance'])      # Sort distances by the distance key\n",
    "\n",
    "    # Return the first n elements in distances_sorted\n",
    "    return distances_sorted[:n]\n",
    "\n",
    "# Create a list of product texts using list comprehension\n",
    "last_product_text = create_product_text(last_product)\n",
    "product_texts = [create_product_text(product) for product in products]\n",
    "\n",
    "# Embed last_product_text and product_texts\n",
    "last_product_embeddings = create_embeddings(last_product_text)[0]   # 0-index because we are embedding a single text\n",
    "product_embeddings = create_embeddings(product_texts)\n",
    "\n",
    "# Find the three smallest cosine distances and their indexes\n",
    "hits = find_n_closest(last_product_embeddings, product_embeddings)\n",
    "\n",
    "for hit in hits:\n",
    "  product = products[hit['index']]\n",
    "  print(product['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "For many recommendation cases, such as film or purchase recommendation, basing the next recommendation on one data point will be insufficient. In these cases, you'll need to embed all or some of the user's history for more accurate and relevant recommendations.\n",
    "\n",
    "In this exercise, you'll extend your product recommendation system to consider all of the products the user has previously visited, which are stored in a list of dictionaries called `user_history` under `/datasets/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot Building Kit\n",
      "Interactive Robot Pet\n",
      "LEGO Space Shuttle\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from scipy.spatial import distance      # to calculate cosine distance\n",
    "import numpy as np                      # to calculate the mean of the embedded user_history\n",
    "import json                             # to load the products.json file\n",
    "import os                               # to get the current working directory\n",
    "from dotenv import load_dotenv          # to load the .env file\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key= api_key)\n",
    "\n",
    "with open('./datasets/products.json', 'r') as file:\n",
    "    products = json.load(file)\n",
    "    \n",
    "with open('./datasets/user_history.json', 'r') as file:\n",
    "    user_history = json.load(file)\n",
    "    \n",
    "# Create a function to create embeddings for product_descriptions - returns a list of embeddings for each product description\n",
    "def create_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "        model = \"text-embedding-3-small\",\n",
    "        input = text\n",
    "    )\n",
    "    response_dict = response.model_dump()\n",
    "    return [data['embedding'] for data in response_dict['data']]\n",
    "\n",
    "# Create a list of product descriptions from the products list using list comprehension\n",
    "product_descriptions = [product['short_description'] for product in products]\n",
    "response = client.embeddings.create(\n",
    "    model = \"text-embedding-3-small\",\n",
    "    input = product_descriptions\n",
    ")\n",
    "response_dict = response.model_dump()\n",
    "\n",
    "# Extract the embeddings from response_dict and store in products.\n",
    "# Now the products list has an additional key 'embedding' for each product, placed after the 'features' key.\n",
    "for i, product in enumerate(products):\n",
    "    product['embedding'] = response_dict['data'][i]['embedding']\n",
    "\n",
    "# Create two lists: one for categories and one for embeddings\n",
    "categories = [product['category'] for product in products]\n",
    "embeddings = [product['embedding'] for product in products]\n",
    "\n",
    "# Define a function to combine the relevant features into a single string\n",
    "def create_product_text(product):\n",
    "    return f\"\"\"Title: {product['title']}\n",
    "    Description: {product['short_description']}\n",
    "    Category: {product['category']}\n",
    "    Features: {\"; \".join(product['features'])}\"\"\"\n",
    "\n",
    "# Combine the features for each product\n",
    "product_texts = [create_product_text(product) for product in products]\n",
    "\n",
    "# Create the embeddings from product_texts\n",
    "product_embeddings = create_embeddings(product_texts)\n",
    "\n",
    "\n",
    "# Returns the n closest distances and their indexes between query_vector and embeddings, based on cosine distance\n",
    "def find_n_closest(query_vector, embeddings, n=3):\n",
    "    distances = []\n",
    "    for index, embedding in enumerate(embeddings):\n",
    "        dist = distance.cosine(query_vector, embedding)       # Calculate the cosine distance between the query vector and embedding\n",
    "        distances.append({\"distance\": dist, \"index\": index})  # Append the distance and index to distances\n",
    "    distances_sorted = sorted(distances, key= lambda x: x['distance'])      # Sort distances by the distance key\n",
    "\n",
    "    # Return the first n elements in distances_sorted\n",
    "    return distances_sorted[:n]\n",
    "\n",
    "# Prepare and embed the user_history, and calculate the mean embeddings\n",
    "history_texts = [create_product_text(product) for product in user_history]\n",
    "history_embeddings = create_embeddings(history_texts)\n",
    "mean_history_embeddings = np.mean(history_embeddings, axis=0)\n",
    "\n",
    "# Extract the titles of products in the user's history\n",
    "history_titles = [product['title'] for product in user_history]\n",
    "\n",
    "# Filter products to remove any in user_history based on title\n",
    "products_filtered = [product for product in products if product['title'] not in history_titles]\n",
    "\n",
    "# Combine product features and embed the resulting texts\n",
    "product_texts = [create_product_text(product) for product in products_filtered]\n",
    "product_embeddings = create_embeddings(product_texts)\n",
    "\n",
    "hits = find_n_closest(mean_history_embeddings, product_embeddings)\n",
    "\n",
    "for hit in hits:\n",
    "    product = products_filtered[hit['index']]\n",
    "    print(product['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Embeddings for classification tasks**\n",
    "\n",
    "Classification tasks can take different forms, but generally, they involve assigning labels to items. Common tasks include __categorization__, such as categorizing headlines into different topics; and __sentiment analysi__s, such as classifying reviews as positive or negative. Embeddings can be used for both of these cases, utilizing the model's ability to capture semantic meaning.\n",
    "\n",
    "We'll be using a type of a classification called __zero-shot classification__, which  means that the classifications won't be based on labeled examples.\n",
    "\n",
    "Let's look at how that works with an example on classyfing news articles by topic.\n",
    "\n",
    "Process:\n",
    "1. Embed class descriptions\n",
    "  * Here we use four classes: tech, science, sports, and business.\n",
    "  * We embed these labels and use them as reference points to base the classificaton on\n",
    "2. Embed the articles to classify\n",
    "3. Calculate cosine distances to each embedded label\n",
    "4. Assign the article the label with the smallest cosine distance.\n",
    "\n",
    "```python\n",
    "topics = [\n",
    "    {'label': 'Tech'},\n",
    "    {'label': 'Science'},\n",
    "    {'label': 'Sports'},\n",
    "    {'label': 'Business'},\n",
    "]\n",
    "```\n",
    "\n",
    "We'll categorize using the label itself, so the first step is to extract the labels as a single list and use these as the class descriptions. Then embed each topic label using the `create_embeddings` custom function that makes a call to the OpenAI embedding model.\n",
    "\n",
    "```python\n",
    "class_descriptions = [topic['label'] for topic in topics]\n",
    "class_embeddings = create_embeddings(class_descriptions)\n",
    "```\n",
    "\n",
    "Here's the article we want to classify:\n",
    "\n",
    "```python\n",
    "article = {\"headline\": \"How NVIDIA GPUs Could Decide Who Wins the AI Race\",\n",
    "           \"keywords\": [\"ai\", \"business\", \"computers\"]\n",
    "```\n",
    "\n",
    "The first step is to combine the headline and the keywordd onformation into a single string that we can embed. We do this by defining a custom function that uses F-string to concatenate the headline and the keywords inside a nicely formatted string.\n",
    "\n",
    "```python\n",
    "def create_article_text(article):\n",
    "    return f\"\"\"Headline: {article['headline']}\n",
    "    Keywords: {', '.join(article['keywords'])}\"\"\"\n",
    "```\n",
    "\n",
    "Finally, we embed the text by calling `create_embeddings()` again, remembering to zero-index the list returned so we have a single list of numbers.\n",
    "\n",
    "```python\n",
    "article_text = create_article_text(article)\n",
    "article_embedding = create_embeddings(article_text)[0]\n",
    "```\n",
    "\n",
    "Now that we have the embeddings, it's time for the cosine distance calculations. The following block is a modified version of the `find_n_closest()` custom function, where instead of returning n results, we only want one: the nearest label. This means that instead of sorting by distance, we can find the minimum using the min function. Calling this function will return the distance and index of this label.\n",
    "\n",
    "```python\n",
    "def find_closest(query_vector, embeddings):\n",
    "    distances=[]\n",
    "    for index, embedding in enumerate(embeddings):\n",
    "        dist = distance.cosine(query_vector, embedding)\n",
    "        distances.append({\"distance\": dist, \"index\": index})\n",
    "    return min(distances, key=lambda x: x[\"distance\"])\n",
    "\n",
    "closest = find_closest(article_embeddings, class_embeddings)\n",
    "```\n",
    "\n",
    "Finally, we can use this index to subset the topics dictionary and extract the label. Printing the result, returns the Business label.\n",
    "\n",
    "```python\n",
    "label = topics[closest['index']['label']]\n",
    "print(label)\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "`Business`\n",
    "\n",
    "But this does not seem right!. If we take another look at the article we're classifying, we can see that the headline indicates that the focus of the article is on tech; it's likely that the model captured the business keyword which resulted in the mislabeling!\n",
    "\n",
    "The limitation in our approach that led to this was the clss descriptions lacked detail. The word \"business\" or \"tech\" does not contain much meaning on its own for the model to capture, so a better approach would be use more detailed class descriptions.\n",
    "\n",
    "Let's try this again, but instead of using the labels as the descriptions, we use short descriptions to represent each class. \n",
    "\n",
    "```python\n",
    "topics = [\n",
    "    {'label': 'Tech', 'description': 'A news article about technology'},\n",
    "    {'label': 'Science', 'description': 'A news article about science'},\n",
    "    {'label': 'Sport', 'description': 'A news article about sports'},\n",
    "    {'label': 'Business', 'description': 'A news article about business'},\n",
    "]\n",
    "```\n",
    "\n",
    "The steps here are almost identical. We extract the descriptions in a single list, this time using the description key, and embed them. The rest of the code is the same! \n",
    "\n",
    "```python\n",
    "class_descriptions = [topic['description'] for topic in topics]\n",
    "class_embeddings = create_embeddings(class_descriptions)\n",
    "\n",
    "[...]\n",
    "\n",
    "label = topics [closest['index']]['label']\n",
    "print(label)\n",
    "```\n",
    "\n",
    "This time, when we print the result, we cans ee that the model classified correctly.\n",
    "\n",
    "Output: <br>\n",
    "`Tech`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example**\n",
    "\n",
    "You've been provided with a small sample of restaurant reviews, stored in reviews, and sentiment labels stored in sentiments:\n",
    "\n",
    "```python\n",
    "sentiments = [{'label': 'Positive'},\n",
    "              {'label': 'Neutral'},\n",
    "              {'label': 'Negative'}]\n",
    "\n",
    "reviews = [\"The food was delicious!\",\n",
    "           \"The service was a bit slow but the food was good\",\n",
    "           \"The food was cold, really disappointing!\"]\n",
    "```\n",
    "\n",
    "You'll use zero-shot classification to classify the sentiment of these reviews by embedding the reviews and class labels.\n",
    "\n",
    "The create_embeddings() function you created previously is also available to use.\n",
    "\n",
    "__Instruction__\n",
    "\n",
    "* Create a list of class descriptions from the labels in the `sentiments` dictionary using a list comprehension.\n",
    "* Embed `class_descriptions` and `reviews` using the `create_embeddings()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [{'label': 'Positive'},\n",
    "              {'label': 'Neutral'},\n",
    "              {'label': 'Negative'}]\n",
    "\n",
    "reviews = [\"The food was delicious!\",\n",
    "           \"The service was a bit slow but the food was good\",\n",
    "           \"The food was cold, really disappointing!\"]\n",
    "\n",
    "# Create a list of class descriptions from the sentiment labels\n",
    "class_descriptions = [sentiment['label'] for sentiment in sentiments]\n",
    "\n",
    "# Embed the class_descriptions and reviews\n",
    "class_embeddings = create_embeddings(class_descriptions)\n",
    "review_embeddings = create_embeddings(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've calculated the embeddings, it's time to compute the cosine distances and extract the most similar label.\n",
    "\n",
    "You'll do this by defining a function called `find_closest()`, which can be used to compare the embeddings between one vector and multiple others, and return the nearest distance and its index. You'll then loop over the reviews and and use `find_closest()` to find the closest distance for each review, extracting the classified label using the index.\n",
    "\n",
    "The `class_embeddings` and `review_embeddings` objects you created in the last exercise are available for you to use, as well as the `reviews` and `sentiments`.\n",
    "\n",
    "__Instructions__\n",
    "* Define a function called `find_closest()` that returns the distance and index of the most similar embedding to the `query_vector`.\n",
    "* Use `find_closest()` to find the closest distance between each review's embeddings and the `class_embeddings`.\n",
    "* Use the '`index'` of closest to subset `sentiments` and extract the `'label'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The food was delicious!\" was classified as Positive\n",
      "\"The service was a bit slow but the food was good\" was classified as Negative\n",
      "\"The food was cold, really disappointing!\" was classified as Negative\n"
     ]
    }
   ],
   "source": [
    "# Define a function to return the minimum distance and its index\n",
    "def find_closest(query_vector, embeddings):\n",
    "  distances = []\n",
    "  for index, embedding in enumerate(embeddings):\n",
    "    dist = distance.cosine(query_vector, embedding)\n",
    "    distances.append({\"distance\": dist, \"index\": index})\n",
    "  return min(distances, key=lambda x: x[\"distance\"])\n",
    "\n",
    "for index, review in enumerate(reviews):\n",
    "  # Find the closest distance and its index using find_closest()\n",
    "  closest = find_closest(review_embeddings[index], class_embeddings)\n",
    "  # Subset sentiments using the index from closest\n",
    "  label = sentiments[closest['index']]['label']\n",
    "  print(f'\"{review}\" was classified as {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the last predicted labels (second one) didn't seem representative of the review; this was probably down to the lack of information being captured when we're only embedding the class labels. This time, descriptions of each class will be embedded instead, so the model better \"understands\" that you're classifying restaurant reviews.\n",
    "\n",
    "The following objects are available for you to use:\n",
    "\n",
    "```python\n",
    "sentiments = [{'label': 'Positive',\n",
    "               'description': 'A positive restaurant review'},\n",
    "              {'label': 'Neutral',\n",
    "               'description':'A neutral restaurant review'},\n",
    "              {'label': 'Negative',\n",
    "               'description': 'A negative restaurant review'}]\n",
    "\n",
    "reviews = [\"The food was delicious!\",\n",
    "           \"The service was a bit slow but the food was good\",\n",
    "           \"The food was cold, really disappointing!\"]\n",
    "```\n",
    "\n",
    "__Instructions__\n",
    "* Extract a list containing the sentiment descriptions and embed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The food was delicious!\" was classified as Positive\n",
      "\"The service was a bit slow but the food was good\" was classified as Neutral\n",
      "\"The food was cold, really disappointing!\" was classified as Negative\n"
     ]
    }
   ],
   "source": [
    "sentiments = [{'label': 'Positive',\n",
    "               'description': 'A positive restaurant review'},\n",
    "              {'label': 'Neutral',\n",
    "               'description':'A neutral restaurant review'},\n",
    "              {'label': 'Negative',\n",
    "               'description': 'A negative restaurant review'}]\n",
    "\n",
    "reviews = [\"The food was delicious!\",\n",
    "           \"The service was a bit slow but the food was good\",\n",
    "           \"The food was cold, really disappointing!\"]\n",
    "\n",
    "# Extract and embed the descriptions from sentiments\n",
    "class_descriptions = [sentiment['description'] for sentiment in sentiments]\n",
    "class_embeddings = create_embeddings(class_descriptions)\n",
    "review_embeddings = create_embeddings(reviews)\n",
    "\n",
    "def find_closest(query_vector, embeddings):\n",
    "  distances = []\n",
    "  for index, embedding in enumerate(embeddings):\n",
    "    dist = distance.cosine(query_vector, embedding)\n",
    "    distances.append({\"distance\": dist, \"index\": index})\n",
    "  return min(distances, key=lambda x: x[\"distance\"])\n",
    "\n",
    "for index, review in enumerate(reviews):\n",
    "  closest = find_closest(review_embeddings[index], class_embeddings)\n",
    "  label = sentiments[closest['index']]['label']\n",
    "  print(f'\"{review}\" was classified as {label}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
