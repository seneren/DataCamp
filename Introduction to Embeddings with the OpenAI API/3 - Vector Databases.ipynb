{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vector databases for embedding systems**\n",
    "\n",
    "<img src = './images/limits-of-current-approach.png' width=50% height=50%>\n",
    "\n",
    "So far, we've created embeddings using the OpenAI API and stored them in-memory. \n",
    "\n",
    "* Loading all the embeddings into memory (1536 floats ~ 13kB/embedding), which becomes impractical to load for 100,000s or millions of embeddings. \n",
    "* Recalculated these embeddings with every query rather than storing them for later use. \n",
    "* We computed cosine distances for every embedded document and sorted the results, which are both slow processes that scale linearly. \n",
    "\n",
    "To enable embeddings applications with larger datasets in production, we'll need a better solution: __vector databases__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vector databases**\n",
    "\n",
    "Here's a typical embeddings application: \n",
    "\n",
    "<img src = './images/embedding-app-arch.png' width=50% height=50%>\n",
    "\n",
    "* Embedded documents are _stored_ and _queried_ from the __vector database__\n",
    "\n",
    "- The documents to query are embedded and stored in the vector database. \n",
    "- A query is sent from the application interface, embedded, and used to query the embeddings in the database. This query can be a semantic search query or data to base recommendations on. \n",
    "- Finally, these results are returned by to the user via the application interface. \n",
    "\n",
    "Because the embedded documents are stored in the vector database, they don't have to created with each query or stored in-memory. Additionally, due to the architecture of the database, the similarity calculation is computed much more efficiently.\n",
    "\n",
    "### **NoSQL databases vs SQL databases**\n",
    "\n",
    "The majority of vector databases are what's called NoSQL databases, which contrasts conventional databases.\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <div style=\"flex: 50%; padding: 5px; border-right: 2px solid DodgerBlue;\">\n",
    "        **NoSQL Database**  <br>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src='./images/nosql-db.png' width=75% height=80%>\n",
    "        </div>\n",
    "        NoSQL databases don't use tables\n",
    "        <ul>\n",
    "            <li>More flexible structure that allows for <i>faster querying</i></li>\n",
    "            <li>Three examples are shown above: including key:value, document, and graph databases</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div style=\"flex: 50%; padding: 5px;\">\n",
    "        **SQL/Relational Database**  <br>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src='./images/sql-db.png' width=75% height=80%>\n",
    "        </div>\n",
    "        <ul>\n",
    "            <li>Structured data into tables, rows. and columns</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### **The vector database landscape**\n",
    "\n",
    "<img src='./images/vector-db-options.png' width=60% height=60%>\n",
    "\n",
    "When deciding which database solution to go with, there are several factors to consider.\n",
    "\n",
    "### **Which solution is best?**\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <div style=\"flex: 50%; padding: 5px; border-right: 2px solid DodgerBlue;\">\n",
    "        <ul>\n",
    "           <li><b>Database management</b></li>\n",
    "           <ul>\n",
    "               <li>Managed &#x2192; more expensive but lowers workload.</li>\n",
    "               <li>Self-managed &#x2192; cheaper but requires time and expertise\n",
    "            </ul>\n",
    "        </ul>\n",
    "        <ul>\n",
    "            <li><b>Open source or commercial?</b></li>\n",
    "            <ul>\n",
    "               <li>Open source &#x2192; flexible and cost-effective if budgets are tight</li>\n",
    "               <li>Commercial &#x2192; offers better support, more advanced features, and compliance</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div style=\"flex: 50%; padding: 5px;\">\n",
    "        <ul>\n",
    "            <li><b>Data models</b>: does the type of data lend itself to a particular database type?</li>\n",
    "            <li><b> Specific features</b>: does your use case depend on specific functionality, like embedding and storung both text and images for a multi-modal application?</li>\n",
    "        </ul>\n",
    "        In this course, we'll be using Chroma, as it's open-source and quick to set up.\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src='./images/chroma.png' width=50% height=50%>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating vector databases with ChromaDB**\n",
    "\n",
    "ChromaDB has two modes:\n",
    "\n",
    "* __Local mode__:\n",
    "  * Great for development and prototyping. Everything runs on our local machine, inside Python.\n",
    "* __Client/Server mode __:\n",
    "  *  Made for production. Requires running a separate process for the chroma server.\n",
    "\n",
    "We'll be using the local mode.\n",
    "\n",
    "### **Connecting to the database**\n",
    "\n",
    "In order to connect and query the database, we need to create a client: We import the chroma and create a persistend cliend by calling the `PersistentClient()` function. Persisten client saves the database files to disk at the path specified.\n",
    "\n",
    "```python\n",
    "import chromadb\n",
    "\n",
    "client = chroma.PersistentClient(path='/path/to/save/to')\n",
    "```\n",
    "\n",
    "### **Creating a collection**\n",
    "\n",
    "To add embeddings to the database, we must first create a collection. Collections are analogous to tables, where we can create as many as we want to store our data. To create the collection, we use the `.create_collection()` method. When creating a collection, we need to pass the name of our collection, which is used as a reference, and the function for creating the embeddings; here, we specify the OpenAI embedding function and API key.\n",
    "\n",
    "```python\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=\"my_collection\",\n",
    "    embedding_function=OpenAIEmbeddingFunction(\n",
    "        model_name=\"text-embedding-3-small\",\n",
    "        api_key=\"<OPENAI_API_KEY>\"\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### **Inspecting the collection**\n",
    "\n",
    "The `list_collections` method lists all of the collections in the database, so we can verify our collection was created.\n",
    "\n",
    "```python\n",
    "client.list_collections()\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "`[Collection(name=my_collection)]`\n",
    "\n",
    "### **Inserting embeddings**\n",
    "\n",
    "We are now ready to add embeddings into the collection. We can do so with the `collection.add` method.\n",
    "\n",
    "* IDs must be provided\n",
    "* Embeddings will be created by the collection.\n",
    "\n",
    "Since the collection is already aware of the embedding function, it will embed the source texts automatically using the function specified. Most of the time, we'll insert multiple documents at once, which we can do by passing multiple ids and documents.\n",
    "\n",
    "**Single Document**\n",
    "\n",
    "```python\n",
    "collection.add(ids=['my-doc'], documents=['This is a the source text'])\n",
    "```\n",
    "\n",
    "**Multiple Documents**\n",
    "\n",
    "```python\n",
    "collection.add(ids=['my-doc1', 'my-doc2'], documents=['This is document 1', 'This is document 2'])\n",
    "```\n",
    "\n",
    "### **Inspecting the collection**\n",
    "\n",
    "After inserting documents, we can inspect the collection with twi methods\n",
    "\n",
    "1. __`collection.count()` method__: Returns the total number of documents in the collection.\n",
    "\n",
    "```python\n",
    "collection.count()\n",
    "```\n",
    "Output: <br>\n",
    "`3`\n",
    "\n",
    "2. __`collection.peek()` method__: Returns the first __10__ items in the collection.__\n",
    "\n",
    "```python\n",
    "collection.peek()\n",
    "```\n",
    "Output: <br>\n",
    "<img src='./images/collection-peek.png' width=50% height=50%>\n",
    "\n",
    "We can also retrieve particular items by their ID using the `.get()` method.\n",
    "\n",
    "```python\n",
    "collection.get(ids=['s59'])\n",
    "```\n",
    "Output: <br>\n",
    "<img src='./images/retrieve-item.png' width=50% height=50%>\n",
    "\n",
    "### **Netflix Dataset**\n",
    "In the following exercises, we';; insert a dataset of Netflix titles into a Chroma dataase. for each title, we'll embed a source text including the title, description, and categories. \n",
    "\n",
    "<img src='./images/netflix-dataset.png' width=50% height=50%>\n",
    "\n",
    "While this is not a massive dataset, we must not forget that each of these texts is going to be sent to the OpenAI embedding endpoint and therefore cost money. Before inserting a sizable dataset into a collection, it's important to get an idea of the cost.\n",
    "\n",
    "### **Cost Estimation**\n",
    "\n",
    "* Embedding model (`text-embedding-3-small`) costs $0.00002/1k tokens ($0.02/1M tokens)\n",
    "\n",
    "OpenAI provides the cost per thousand tokens on their model pricing page, which means we can find the total cost\n",
    "\n",
    "`cost = 0.00002 * len(tokens)/1000`\n",
    "\n",
    "We can count tokens with `tiktoken` library. (to install `pip install tiktoken`)\n",
    "\n",
    "### **Estimating embedding cost**\n",
    "\n",
    "Tiktoken can convert any text into tokens. \n",
    "\n",
    "First, we use the `encoding_for_model` function to get a token encoder for the embedding model we're using. To calculate the total number of tokens, we use the following code. This reads: for each text in documents, encode it using the encoder and take the `length` to obtain the number of tokens in the text. Finally, `sum` the results. This code is much more concise and efficient than looping through the documents.\n",
    "\n",
    "Finally, we calculate the price by multiplying `total_tokens` by `cost_per_1k_tokens` over `1000`, and print the result.\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model('text-embeddding-3-small')\n",
    "\n",
    "total_tokens = sum(len(end.encode(text)) for text in documents)\n",
    "\n",
    "cost_per_1k_tokens = 0.00002\n",
    "\n",
    "print('Total tokens:\", total_tokens)\n",
    "print('Cost:', cost_per_1k_tokens * total_tokens/1000)\n",
    "```\n",
    "\n",
    "Then it would give:\n",
    "```\n",
    "Total tokens: 444463\n",
    "Cost: 0.0888926\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a persistent client to save the database files to disk; you can leave out the file path for these exercises.\n",
    "* Create a database collection called `netflix_titles` that uses the OpenAI embedding function.\n",
    "* List all of the collections in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['netflix_titles']\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import os                               # to get the current working directory\n",
    "from dotenv import load_dotenv          # to load the .env file\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Create a persistant client\n",
    "client = chromadb.PersistentClient(path=\"./datasets/\")\n",
    "\n",
    "# Create a netflix_title collection using the OpenAI Embedding function\n",
    "collection = client.create_collection(\n",
    "    name=\"netflix_titles\",\n",
    "    embedding_function=OpenAIEmbeddingFunction(model_name=\"text-embedding-3-small\", api_key=\"<OPENAI_API_TOKEN>\")\n",
    ")\n",
    "\n",
    "# List the collections\n",
    "print(client.list_collections())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a database and collection to store the Netflix films and TV shows, we can begin embedding data.\n",
    "\n",
    "Before embedding a large dataset, it's important to do a cost estimate to ensure you don't go over any budget restraints. Because OpenAI models are priced by number of tokens inputted, we'll use OpenAI's tiktoken library to count the number of tokens and convert them into a dollar cost.\n",
    "\n",
    "You've been provided with document texts as `documents`, which has been extracted from `netflix_titles_1000.csv`. Here is the first document from `documents`:\n",
    "```\n",
    "Title: Dick Johnson Is Dead (Movie)\n",
    "Description: As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.\n",
    "Categories: Documentaries\n",
    "```\n",
    "\n",
    "For later use, you've also been provided with document IDs.\n",
    "\n",
    "You'll now iterate over the list, encode each document, and count the total number of tokens. Finally, you'll use the model's pricing to convert this into a cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 documents:\n",
      "Title: Dick Johnson Is Dead (Movie)\n",
      "Description: As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.\n",
      "Categories: Documentaries\n",
      "\n",
      "Title: Blood & Water (TV Show)\n",
      "Description: After crossing paths at a party, a Cape Town teen sets out to prove whether a private-school swimming star is her sister who was abducted at birth.\n",
      "Categories: International TV Shows, TV Dramas, TV Mysteries\n",
      "\n",
      "Title: Ganglands (TV Show)\n",
      "Description: To protect his family from a powerful drug lord, skilled thief Mehdi and his expert team of robbers are pulled into a violent and deadly turf war.\n",
      "Categories: Crime TV Shows, International TV Shows, TV Action & Adventure\n",
      "\n",
      "Title: Jailbirds New Orleans (TV Show)\n",
      "Description: Feuds, flirtations and toilet talk go down among the incarcerated women at the Orleans Justice Center in New Orleans on this gritty reality series.\n",
      "Categories: Docuseries, Reality TV\n",
      "\n",
      "Title: Kota Factory (TV Show)\n",
      "Description: In a city of coaching centers known to train India’s finest collegiate minds, an earnest but unexceptional student and his friends navigate campus life.\n",
      "Categories: International TV Shows, Romantic TV Shows, TV Comedies\n",
      "\n",
      "Last 5 documents:\n",
      "Title: Zodiac (Movie)\n",
      "Description: A political cartoonist, a crime reporter and a pair of cops investigate San Francisco's infamous Zodiac Killer in this thriller based on a true story.\n",
      "Categories: Cult Movies, Dramas, Thrillers\n",
      "\n",
      "Title: Zombie Dumb (TV Show)\n",
      "Description: While living alone in a spooky town, a young girl befriends a motley crew of zombie children with diverse personalities.\n",
      "Categories: Kids' TV, Korean TV Shows, TV Comedies\n",
      "\n",
      "Title: Zombieland (Movie)\n",
      "Description: Looking to survive in a world taken over by zombies, a dorky college student teams with an urban roughneck and a pair of grifter sisters.\n",
      "Categories: Comedies, Horror Movies\n",
      "\n",
      "Title: Zoom (Movie)\n",
      "Description: Dragged from civilian life, a former superhero must train a new crop of youthful saviors when the military preps for an attack by a familiar villain.\n",
      "Categories: Children & Family Movies, Comedies\n",
      "\n",
      "Title: Zubaan (Movie)\n",
      "Description: A scrappy but poor boy worms his way into a tycoon's dysfunctional family, while facing his fear of music and the truth about his past.\n",
      "Categories: Dramas, International Movies, Music & Musicals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "ids = []\n",
    "documents = []\n",
    "\n",
    "with open('./datasets/netflix_titles.csv') as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "  for i, row in enumerate(reader):\n",
    "    ids.append(row['show_id'])\n",
    "    text = f\"Title: {row['title']} ({row['type']})\\nDescription: {row['description']}\\nCategories: {row['listed_in']}\"\n",
    "    documents.append(text)\n",
    "\n",
    "# Print first 5 and last 5 documents\n",
    "print(\"First 5 documents:\")\n",
    "for doc in documents[:5]:\n",
    "    print(doc)\n",
    "    print()\n",
    "\n",
    "print(\"Last 5 documents:\")\n",
    "for doc in documents[-5:]:\n",
    "    print(doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 444463\n",
      "Cost: 0.00888926\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the encoder for the OpenAI text-embedding-3-small model\n",
    "enc = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# Encode each text in documents and calculate the total tokens\n",
    "total_tokens = sum(len(enc.encode(text)) for text in documents)\n",
    "\n",
    "cost_per_1k_tokens = 0.00002\n",
    "\n",
    "# Display number of tokens and cost\n",
    "print('Total tokens:', total_tokens)\n",
    "print('Cost:', cost_per_1k_tokens * total_tokens/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means, for each request, the cost will be $0.00888926.\n",
    "\n",
    "Time to add those Netflix films and TV shows to your collection.\n",
    "\n",
    "__Instructions__\n",
    "\n",
    "* Recreate your `netflix_titles` collection.\n",
    "* Add the documents and their IDs to the collection.\n",
    "* Print the number of documents in `collection` and the first ten items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of documents: 8807\n",
      "First ten documents: {'ids': ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10'], 'embeddings': array([[ 0.02242333,  0.05174443, -0.02444355, ...,  0.02265772,\n",
      "        -0.00398463, -0.02413103],\n",
      "       [-0.00256908,  0.09567138, -0.04806154, ...,  0.01961534,\n",
      "         0.03606874, -0.04447048],\n",
      "       [-0.015072  ,  0.05057291, -0.04685031, ..., -0.00316648,\n",
      "         0.00111224, -0.04591966],\n",
      "       ...,\n",
      "       [-0.02682706,  0.05365412, -0.02775045, ...,  0.03700871,\n",
      "        -0.02228298, -0.02446997],\n",
      "       [ 0.01401989,  0.0206609 , -0.0120415 , ...,  0.01189178,\n",
      "         0.01038392, -0.04914984],\n",
      "       [ 0.00980071,  0.07244343, -0.03348716, ...,  0.01767378,\n",
      "         0.02326618, -0.0031016 ]]), 'documents': ['Title: Dick Johnson Is Dead (Movie)\\nDescription: As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.\\nCategories: Documentaries', 'Title: Blood & Water (TV Show)\\nDescription: After crossing paths at a party, a Cape Town teen sets out to prove whether a private-school swimming star is her sister who was abducted at birth.\\nCategories: International TV Shows, TV Dramas, TV Mysteries', 'Title: Ganglands (TV Show)\\nDescription: To protect his family from a powerful drug lord, skilled thief Mehdi and his expert team of robbers are pulled into a violent and deadly turf war.\\nCategories: Crime TV Shows, International TV Shows, TV Action & Adventure', 'Title: Jailbirds New Orleans (TV Show)\\nDescription: Feuds, flirtations and toilet talk go down among the incarcerated women at the Orleans Justice Center in New Orleans on this gritty reality series.\\nCategories: Docuseries, Reality TV', 'Title: Kota Factory (TV Show)\\nDescription: In a city of coaching centers known to train India’s finest collegiate minds, an earnest but unexceptional student and his friends navigate campus life.\\nCategories: International TV Shows, Romantic TV Shows, TV Comedies', 'Title: Midnight Mass (TV Show)\\nDescription: The arrival of a charismatic young priest brings glorious miracles, ominous mysteries and renewed religious fervor to a dying town desperate to believe.\\nCategories: TV Dramas, TV Horror, TV Mysteries', \"Title: My Little Pony: A New Generation (Movie)\\nDescription: Equestria's divided. But a bright-eyed hero believes Earth Ponies, Pegasi and Unicorns should be pals — and, hoof to heart, she’s determined to prove it.\\nCategories: Children & Family Movies\", 'Title: Sankofa (Movie)\\nDescription: On a photo shoot in Ghana, an American model slips back in time, becomes enslaved on a plantation and bears witness to the agony of her ancestral past.\\nCategories: Dramas, Independent Movies, International Movies', \"Title: The Great British Baking Show (TV Show)\\nDescription: A talented batch of amateur bakers face off in a 10-week competition, whipping up their best dishes in the hopes of being named the U.K.'s best.\\nCategories: British TV Shows, Reality TV\", \"Title: The Starling (Movie)\\nDescription: A woman adjusting to life after a loss contends with a feisty bird that's taken over her garden — and a husband who's struggling to find a way forward.\\nCategories: Comedies, Dramas\"], 'uris': None, 'data': None, 'metadatas': [None, None, None, None, None, None, None, None, None, None], 'included': [<IncludeEnum.embeddings: 'embeddings'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import os                               # to get the current working directory\n",
    "from dotenv import load_dotenv          # to load the .env file\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Create a persistant client\n",
    "client = chromadb.PersistentClient(path=\"./datasets/\")\n",
    "\n",
    "ids = []\n",
    "documents = []\n",
    "\n",
    "with open('./datasets/netflix_titles.csv') as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "  for i, row in enumerate(reader):\n",
    "    ids.append(row['show_id'])\n",
    "    text = f\"Title: {row['title']} ({row['type']})\\nDescription: {row['description']}\\nCategories: {row['listed_in']}\"\n",
    "    documents.append(text)\n",
    "\n",
    "# Recreate the netflix_titles collection\n",
    "collection = client.create_collection(\n",
    "  name=\"netflix_titles\",\n",
    "  embedding_function=OpenAIEmbeddingFunction(\n",
    "    model_name=\"text-embedding-3-small\", \n",
    "    api_key=api_key)\n",
    ")\n",
    "\n",
    "\"\"\"The following code line may not work because since file is big, it tries to add too many documents at once. \n",
    "Splitting the documents into smaller batches will work.\"\"\"\n",
    "# collection.add(ids=ids, documents=documents)\n",
    "\n",
    "# Add documents in batches of 100\n",
    "batch_size = 1000\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch_ids = ids[i:i + batch_size]\n",
    "    batch_docs = documents[i:i + batch_size]\n",
    "    collection.add(ids=batch_ids, documents=batch_docs)\n",
    "\n",
    "\n",
    "# Print the collection size and first ten items\n",
    "print(f\"No. of documents: {collection.count()}\")\n",
    "print(f\"First ten documents: {collection.peek()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Querying and updating the database**\n",
    "\n",
    "### **Querying the database**\n",
    "\n",
    "Similar to what we did manually in the previous chapter, we'll build a semantic search application, but this time, using a vector database. The approach is exactly the same: we have a query string and we want to find similar titles in our collection. \n",
    "\n",
    "Previously, we had to embed the query string to get a query vector, which was used to find similar embeddings in the dataset:\n",
    "\n",
    "<img src=\"./images/previously.png\" width=50% height=50%>\n",
    "\n",
    "With Chroma, we'll let the collection do the embedding, so we can pass our query string directly and Chroma will take care of creating the embedding and performing the search:\n",
    "\n",
    "<img src=\"./images/now-chroma.png\" width=50% height=50%>\n",
    "\n",
    "First, we need to retrieve our collection, which we can do with `client.get_collection()`, specifying the name of the collection to retrieve. Recall that when we created the collection, we specified the embedding function to use, and it's also really important to specify the same function when retrieving the collection. This way, Chroma will use the same embedding function to create the _query vector_.\n",
    "\n",
    "```python\n",
    "from chromadb.utils.embedding_function import OpenAiEmbeddingFunction\n",
    "\n",
    "collection = client.get_collection(\n",
    "    name='netflix_titles_test1',\n",
    "    embedding_function= OpenAIEmbeddingFunction(\n",
    "        api_key=\"<OPENAI_API_KEY>\"\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### **Querying the collection**\n",
    "\n",
    "To query the collection, we call collection.query, passing our query string to `query_texts`. Note that this parameter is _plural_, so even if we have a single query string, we pass a list. To specify how many items to retrieve, we can use the `n_results` parameter. \n",
    "\n",
    "```python\n",
    "result = collection.query(\n",
    "    query_texts=[\"movies where people sing a lot\"],\n",
    "    n_results=3\n",
    ")\n",
    "```\n",
    "\n",
    "Let's run the code below and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  'ids': [['s1995', 's2213', 's4068']],\n",
      "  'embeddings': None,\n",
      "  'documents': [\n",
      "    'Title: Sing On! (TV Show)\n",
      "Description: In this fun, fast-paced music contest hosted by Tituss Burgess, players sing their hearts out and try to hit the right notes to win up to $60,000.\n",
      "Categories: Reality TV',\n",
      "    'Title: Sing On! Spain (TV Show)\n",
      "Description: In this fast-paced, high-energy karaoke competition, singers from all walks of life battle it out for up to 30,000 euros!\n",
      "Categories: International TV Shows, Reality TV, Spanish-Language TV Shows',\n",
      "    'Title: Quién te cantará (Movie)\n",
      "Description: When a near-drowning leaves a famous singer from the '90s with amnesia, she hires a karaoke singer who can imitate her to prep her for a comeback tour.\n",
      "Categories: Dramas, Independent Movies, International Movies',\n",
      "  ],\n",
      "  'uris': None,\n",
      "  'data': None,\n",
      "  'metadatas': [[None, None, None]],\n",
      "  'distances': [[1.0084547996520996, 1.0408340692520142, 1.046926498413086]],\n",
      "  'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variable\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "collection = client.get_collection(\n",
    "    name='netflix_titles',\n",
    "    embedding_function= OpenAIEmbeddingFunction(\n",
    "        model_name=\"text-embedding-3-small\",\n",
    "        api_key=api_key\n",
    "    )\n",
    ")\n",
    "\n",
    "result = collection.query(\n",
    "    query_texts=[\"movies where people sing a lot\"],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "\"\"\"To display the output in a more readable format,\n",
    "we can format the dictionary output with each key-value pair on a new line as follows:\"\"\"\n",
    "\"\"\"If you want to see the raw result, you can do print(result)\"\"\"\n",
    "\n",
    "# Format the dictionary output with each key-value pair on a new line\n",
    "formatted_output = \"{\\n\"\n",
    "for key, value in result.items():\n",
    "    if key == 'documents':\n",
    "        # Format each document on a new line\n",
    "        formatted_output += f\"  '{key}': [\\n\"\n",
    "        for doc in value[0]:\n",
    "            formatted_output += f\"    '{doc}',\\n\"\n",
    "        formatted_output += \"  ],\\n\"\n",
    "    else:\n",
    "        formatted_output += f\"  '{key}': {value},\\n\"\n",
    "formatted_output += \"}\"\n",
    "\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Query Result (dict)**\n",
    "\n",
    "let's break the output down:\n",
    "\n",
    "`query()` returns a dictionary with the following keys:\n",
    "- `ids`: a list of the IDs of the documents that were returned\n",
    "- `embeddings`: The embeddings of the returned items\n",
    "- `documents`: The source texts if the returned items\n",
    "- `metadatas`: The metadatas of the returned items\n",
    "- `distances`: The distances between the query and the returned items\n",
    "- `uris`: The URIs of the returned items\n",
    "- `data`: The data of the returned items\n",
    "- `included`: The list of lists of included items, excluding `uris` and `data`.\n",
    "\n",
    "\n",
    "The embeddings entry is emty, simply because Chroma doesn't return them by default. Also, each of these entries has the same format; let's look at ids\n",
    "\n",
    "### **Query results (list of lists)**\n",
    "\n",
    "ids contains a list of lists. The reason for this is that the query method accepts a list of query texts, even though we used one query text. - meaning we could use multiple query texts. So, the result follow the same structure:\n",
    "\n",
    "* First list corresponds to the first query_text\n",
    "* Multiple query texts will return multiple lists - If we had multiple query texts, we would get back as many lists.\n",
    "\n",
    " In this list, we find a format similar to the parameters of the add() method: the first id corresponds to the first document, metadatas, and distances:\n",
    "\n",
    " <img src='./images/query_result.png' width=50% height=50%>\n",
    "\n",
    "\n",
    "### **Updating a collection**\n",
    "\n",
    "Items in a collection can be updated with the `update` method. The syntax is similar to `collection.add()`; in this example, we'll update the texts for items `id-1` and `id-2`. \n",
    "\n",
    "* Include _only_ the fields to update, other fields will be unchanged\n",
    "* Collection will automatically create embeddings\n",
    "\n",
    "```python\n",
    "collecton.update(\n",
    "    ids=[\"id-1\", \"id-2\"],\n",
    "    documents=[\"New document 1\", \"New document 2\"],\n",
    ")\n",
    "```\n",
    "\n",
    "Alternatively,  if we're not sure if the IDs are already present in the table, use the `upsert` method. `upsert` will add the IDs to the collection if they aren't present, and update them if they are - a combination of the update and add methods.\n",
    "\n",
    "```python\n",
    "collection.upsert(\n",
    "    ids=[\"id-1\", \"id-2\"],\n",
    "    documents=[\"New document 1\", \"New document 2\"],\n",
    ")\n",
    "```\n",
    "\n",
    "### **Deleting**\n",
    "\n",
    "__Delete items from a collection__\n",
    "\n",
    "```python\n",
    "collection.delete(ids=[\"id-1\", \"id-2\"])\n",
    "```\n",
    "\n",
    "__Delete all collections and items__\n",
    "* __Warning__: This will delete everything in the database!\n",
    "\n",
    "```python\n",
    "client.reset()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've created and populated the `netflix_titles_test1` collection, it's time to query it!\n",
    "\n",
    "You'll use it to provide recommendations for films and TV shows about dogs to one of your colleagues who loves dogs!\n",
    "\n",
    "You've been also provided with two new Netflix titles stored in `new_data`. \n",
    "\n",
    "You'll either add or update these IDs in the database depending on whether they're already present in the collection.\n",
    "\n",
    "__Instructions__\n",
    "\n",
    "* Retrieve the netflix_titles collection, specifying the OpenAI embedding function so the query is embedded using the same function as the documents.\n",
    "* Extract the IDs and documents from `new_data`, and use a single method to update them in the `netflix_titles_test1` collection if they already exist and add them if they don't\n",
    "* After you've added/updated the items, delete the item with ID 's95'.\n",
    "* Query the collection for \"films about dogs\" and return three results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete of nonexisting embedding ID: s95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  'ids': [['s2057', 's830', 's500']],\n",
      "  'embeddings': None,\n",
      "  'documents': [\n",
      "    'Title: Hotel for Dogs (Movie)\n",
      "Description: Placed in a foster home that doesn't allow pets, 16-year-old Andi and her younger brother Bruce turn an abandoned hotel into a home for their dog.\n",
      "Categories: Children & Family Movies, Comedies',\n",
      "    'Title: Dog Gone Trouble (Movie)\n",
      "Description: The privileged life of a pampered dog named Trouble is turned upside-down when he gets lost and must learn to survive on the big-city streets.\n",
      "Categories: Children & Family Movies, Comedies',\n",
      "    'Title: Dogs (TV Show)\n",
      "Description: These six intimate stories explore the abiding emotional bonds that form between dogs and their caregivers, no matter the circumstances.\n",
      "Categories: Docuseries',\n",
      "  ],\n",
      "  'uris': None,\n",
      "  'data': None,\n",
      "  'metadatas': [[None, None, None]],\n",
      "  'distances': [[0.8885394334793091, 0.8959437608718872, 0.9038522839546204]],\n",
      "  'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import os                               # to get the current working directory\n",
    "from dotenv import load_dotenv         # to load the environment variables from the .env file\n",
    "\n",
    "load_dotenv()                         # load the environment variables from the .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  # get the API key from the environment variables\n",
    "\n",
    "new_data= [{\"id\": \"s1001\", \"document\": \"Title: Cats & Dogs (Movie)\\nDescription: A look at the top-secret, high-tech espionage war going on between cats and dogs, of which their human owners are blissfully unaware.\"},\n",
    " {\"id\": \"s6884\", \"document\": 'Title: Goosebumps 2: Haunted Halloween (Movie)\\nDescription: Three teens spend their Halloween trying to stop a magical book, which brings characters from the \"Goosebumps\" novels to life.\\nCategories: Children & Family Movies, Comedies'}]\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./datasets/\")\n",
    "\n",
    "# Retrieve the netflix_titles collection\n",
    "collection = client.get_collection(\n",
    "  name=\"netflix_titles\",\n",
    "  embedding_function=OpenAIEmbeddingFunction(model_name=\"text-embedding-3-small\", api_key=api_key)\n",
    ")\n",
    "\n",
    "\n",
    "# Update or add the new documents\n",
    "collection.upsert(\n",
    "    ids=[doc['id'] for doc in new_data],\n",
    "    documents=[doc['document'] for doc in new_data]\n",
    ")\n",
    "\n",
    "# Delete the item with ID \"s95\"\n",
    "collection.delete(ids=['s95'])\n",
    "\n",
    "# Query the collection for \"films about dogs\"\n",
    "result = collection.query(\n",
    "  query_texts=['films about dogs'],\n",
    "  n_results=3\n",
    ")\n",
    "\n",
    "# Format the dictionary output with each key-value pair on a new line\n",
    "formatted_output = \"{\\n\"\n",
    "for key, value in result.items():\n",
    "    if key == 'documents':\n",
    "        # Format each document on a new line\n",
    "        formatted_output += f\"  '{key}': [\\n\"\n",
    "        for doc in value[0]:\n",
    "            formatted_output += f\"    '{doc}',\\n\"\n",
    "        formatted_output += \"  ],\\n\"\n",
    "    else:\n",
    "        formatted_output += f\"  '{key}': {value},\\n\"\n",
    "formatted_output += \"}\"\n",
    "\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multiple queries and filtering**\n",
    "\n",
    "### **Movie recommendations based on multiple datapoints**\n",
    "\n",
    "In the previous chapter, we used embeddings to make recommendations based on multiple data points. Let's do the same with the Netflix dataset and Chroma. We'll recommend movies related to other titles that a user has seen. Let's assume a user has seen a horror film and a kid's TV show:\n",
    "\n",
    "- Terrifier (id: 's8170')\n",
    "- Strawbery Shortcake: Berry Bitty Adventures (id: 's8103')\n",
    "\n",
    "It's an odd combination, but hopefully it will help differentiate the recommendations.\n",
    "\n",
    " we'll use the embedded texts of the reference items as queries. First, we're using `collection.get` to retrieve both of our reference texts. Notice that we're only extracting and storing the documents from these items in `reference_texts`. Since `collection.query` supports multiple query texts, we can pass our `reference_texts` directly; we'll ask for three results.\n",
    "\n",
    " ```python\n",
    "reference_ids = ['s8170', 's8103']\n",
    "\n",
    "reference_texts = collection.get(ids=reference_ids)['documents']\n",
    "\n",
    "result = collection.query(\n",
    "    query_texts= reference_texts,\n",
    "    n_results=3\n",
    ")\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  'ids': [['s8103', 's2968', 's3085'], ['s8170', 's5333', 's1368']],\n",
      "  'embeddings': None,\n",
      "  'documents': [\n",
      "    [\n",
      "      'Title: Strawberry Shortcake: Berry Bitty Adventures (TV Show)\n",
      "Description: Join Strawberry Shortcake and her berry best friends in the whimsical land of Berry Bitty City, where they learn about teamwork and decision-making.\n",
      "Categories: Kids' TV',\n",
      "      'Title: Shopkins (TV Show)\n",
      "Description: Tiny grocery store items come to life as the Shopkins, who have fun adventures with each other at Small Mart and the magical town called Shopville.\n",
      "Categories: Kids' TV',\n",
      "      'Title: Rainbow Ruby (TV Show)\n",
      "Description: Ruby makes magical journeys with her teddy bear Choco to Rainbow Village, where her toys come to life – and where there's always a problem to solve!\n",
      "Categories: Kids' TV',\n",
      "    ],\n",
      "    [\n",
      "      'Title: Terrifier (Movie)\n",
      "Description: On Halloween night, inside a dilapidated apartment building, Art the Clown stalks his victims, slicing and slaughtering in terrifying silence.\n",
      "Categories: Horror Movies, Independent Movies, Thrillers',\n",
      "      'Title: Demonic (Movie)\n",
      "Description: When amateur ghost hunters visit an abandoned house, their investigation turns into a massacre, leaving questions for a detective and a psychologist.\n",
      "Categories: Horror Movies, Independent Movies',\n",
      "      'Title: Hell Fest (Movie)\n",
      "Description: A serial killer picks off a group of friends, one by one, as they make their way through a hell-themed amusement park.\n",
      "Categories: Horror Movies',\n",
      "    ],\n",
      "  ],\n",
      "  'uris': None,\n",
      "  'data': None,\n",
      "  'metadatas': [[None, None, None], [None, None, None]],\n",
      "  'distances': [[8.088971412689716e-07, 0.790494978427887, 0.8519140481948853], [4.081573024450336e-06, 0.7495567798614502, 0.7873989939689636]],\n",
      "  'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "reference_ids = ['s8170', 's8103']\n",
    "\n",
    "reference_texts = collection.get(ids=reference_ids)['documents']\n",
    "\n",
    "result = collection.query(\n",
    "    query_texts= reference_texts,\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "# Format the dictionary output with each key-value pair on a new line\n",
    "formatted_output = \"{\\n\"\n",
    "for key, value in result.items():\n",
    "    if key == 'documents':\n",
    "        # Format each document on a new line\n",
    "        formatted_output += f\"  '{key}': [\\n\"\n",
    "        for doc_list in value:\n",
    "            formatted_output += \"    [\\n\"\n",
    "            for doc in doc_list:\n",
    "                formatted_output += f\"      '{doc}',\\n\"\n",
    "            formatted_output += \"    ],\\n\"\n",
    "        formatted_output += \"  ],\\n\"\n",
    "    else:\n",
    "        formatted_output += f\"  '{key}': {value},\\n\"\n",
    "formatted_output += \"}\"\n",
    "\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multiple query text result**\n",
    "\n",
    "With multiple query texts, we still get back a single dictionary. The difference is that we now get back multiple lists inside each entry's list. \n",
    "\n",
    "Looking at documents, for instance, we see a second list that matches our horror movie and a first list that seems to match the children's title. Even though we asked for 3 results, we're getting 6: three for each query text. \n",
    "\n",
    "Notice how the titles we used in the query were most highly recommended, which makes sense, as they are the most similar to the query. These could be removed in postprocessing by extracting the documents from the documents key and filtering out documents that are also in the `reference_texts`. Another great way of filtering results is by utilizing metadata.\n",
    "\n",
    "### **Adding metadata**\n",
    "\n",
    "So far, we've only worked with the IDs and documents from the `netflix_titles.csv`, but additional information, like the     (whether the title is a film or TV show) and   is also available. This data could be very useful if, for example, we only wanted to recommend films released recently. \n",
    "\n",
    "We've edited the code to load the CSV by creating a list to store the `metadatas`, and populating it with the type and `release_year` from each row of the file, stored together in a dictionary. Like before, we also create a list of IDs so we can add the metadatas to the existing items.\n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "ids=[]\n",
    "metadatas=[]\n",
    "\n",
    "with open('netflix_titles.csv') as csvfile:\n",
    "    reader= csv.DictReader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        ids.append(row['show_id'])\n",
    "        metadatas.append({\n",
    "            'type': row['type'],\n",
    "            'release_year': int(row['release_year'])\n",
    "        })\n",
    "```\n",
    "\n",
    "### **Adding and querying metadatas**\n",
    "\n",
    "We can update the items with their metadatas using the `update` method, this time, specifying the metadatas argument.\n",
    "\n",
    "We can now use metadatas to filter our query. We are going to make the same search as before, but we'll include a `where` clause to indicate we only want to retrieve items where the `\"type\"` metadata is `\"Movie\"`.\n",
    "\n",
    "```python\n",
    "collection.update(ids=ids, metadatas=metadatas)\n",
    "\n",
    "result = collection.query(\n",
    "    query_texts=reference_text,\n",
    "    n_results=3,\n",
    "    where={\n",
    "        \"type\": \"Movie\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### **Where operators**\n",
    "\n",
    "The `where` filter used above is actually a shortcut to mean _equals_, which we cand efine explicitly using:\n",
    "```python\n",
    "where={\n",
    "    \"type\": \"Movie\"\n",
    "}\n",
    "```\n",
    "\n",
    "is same as\n",
    "\n",
    "```python\n",
    "where={\n",
    "    \"type\": {\n",
    "        \"$eq\": \"Movie\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "There are a few operators we can use to support different comparisons:\n",
    "\n",
    "- `$eq` equal to (string, int, float)\n",
    "- `$ne` not equal to (string, int, float)\n",
    "- `$gt` greater than (int, float)\n",
    "- `$gte` greater than or equal to (int, float)\n",
    "- `$lt` less than (int, float)\n",
    "- `$lte` less than or equal to (int, float)\n",
    "\n",
    "### **Multiple where filters**\n",
    "\n",
    "`where` filters can be combined with logical operators. In our case, we want titles of type movie AND released after a ceetain year. To do this. we combine two where filters with an __$and__ operator:\n",
    "\n",
    "```python\n",
    "where={\n",
    "    \"$and\": [\n",
    "        {\n",
    "            \"type\": {\n",
    "                \"$eq\": \"Movie\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"release_year\": {\n",
    "                \"$gt\": 2020\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "To filter for results that meet at least one condition, we can use __$or__ instead of __$and__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Batch size 8807 exceeds maximum batch size 5461",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m         ids\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshow_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m         metadatas\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelease_year\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelease_year\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m         })\n\u001b[0;32m---> 15\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m result \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m     18\u001b[0m     query_texts\u001b[38;5;241m=\u001b[39mreference_texts,\n\u001b[1;32m     19\u001b[0m     n_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m }\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/chromadb/api/models/Collection.py:299\u001b[0m, in \u001b[0;36mCollection.update\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m update_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_update_request(\n\u001b[1;32m    291\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    292\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     uris\u001b[38;5;241m=\u001b[39muris,\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadatas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muris\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/chromadb/telemetry/opentelemetry/__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/chromadb/api/segment.py:103\u001b[0m, in \u001b[0;36mrate_limit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rate_limit_enforcer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrate_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/chromadb/rate_limit/simple_rate_limit/__init__.py:24\u001b[0m, in \u001b[0;36mSimpleRateLimitEnforcer.rate_limit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/chromadb/api/segment.py:481\u001b[0m, in \u001b[0;36mSegmentAPI._update\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    479\u001b[0m coll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collection(collection_id)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mhint_use_collection(collection_id, t\u001b[38;5;241m.\u001b[39mOperation\u001b[38;5;241m.\u001b[39mUPDATE)\n\u001b[0;32m--> 481\u001b[0m \u001b[43mvalidate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_max_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m records_to_submit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    486\u001b[0m     _records(\n\u001b[1;32m    487\u001b[0m         t\u001b[38;5;241m.\u001b[39mOperation\u001b[38;5;241m.\u001b[39mUPDATE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_embedding_record_set(coll, records_to_submit)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/chromadb/api/types.py:833\u001b[0m, in \u001b[0;36mvalidate_batch\u001b[0;34m(batch, limits)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_batch\u001b[39m(\n\u001b[1;32m    823\u001b[0m     batch: Tuple[\n\u001b[1;32m    824\u001b[0m         IDs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     limits: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    831\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m limits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 833\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    834\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeds maximum batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    835\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Batch size 8807 exceeds maximum batch size 5461"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "ids=[]\n",
    "metadatas=[]\n",
    "\n",
    "with open('./datasets/netflix_titles.csv') as csvfile:\n",
    "    reader= csv.DictReader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        ids.append(row['show_id'])\n",
    "        metadatas.append({\n",
    "            'type': row['type'],\n",
    "            'release_year': int(row['release_year'])\n",
    "        })\n",
    "\n",
    "collection.update(ids=ids, metadatas=metadatas)\n",
    "\n",
    "result = collection.query(\n",
    "    query_texts=reference_texts,\n",
    "    n_results=3,\n",
    "    where={\n",
    "    \"$and\": [\n",
    "        {\n",
    "            \"type\": {\n",
    "                \"$eq\": \"Movie\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"release_year\": {\n",
    "                \"$gt\": 2020\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get Max batch size error, indicating that the batch size for updating the collection exceeds the allowed limit. This is a common issue when working with large datasets and APIs that have batch size restrictions. To resolve this issue, you can update the collection in smaller batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Update of nonexisting embedding ID: s95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  'ids': [['s131', 's812', 's883'], ['s300', 's1109', 's464']],\n",
      "  'embeddings': None,\n",
      "  'documents': [\n",
      "    [\n",
      "      'Title: Barbie Big City Big Dreams (Movie)\n",
      "Description: At a summer performing arts program in New York City, Barbie from Malibu meets Barbie from Brooklyn, and the two become fast friends.\n",
      "Categories: Children & Family Movies, Music & Musicals',\n",
      "      'Title: Super Monsters: Once Upon a Rhyme (Movie)\n",
      "Description: From Goldilocks to Hansel and Gretel, the Super Monsters reimagine classic fairy tales and favorite nursery rhymes with a musical, magical spin!\n",
      "Categories: Children & Family Movies',\n",
      "      'Title: Jungle Beat: The Movie (Movie)\n",
      "Description: When a lost and lonely alien crash-lands on Earth, his new crew of talking animal friends helps him get back home — and try to save the world!\n",
      "Categories: Children & Family Movies, Comedies',\n",
      "    ],\n",
      "    [\n",
      "      'Title: The Swarm (Movie)\n",
      "Description: A single mother breeds locusts as high-protein food, but has trouble getting them to reproduce — until she finds they have a taste for blood.\n",
      "Categories: Horror Movies, Independent Movies, International Movies',\n",
      "      'Title: Irul (Movie)\n",
      "Description: When a car breakdown forces a couple to seek shelter in a nearby home, conversations with the stranger inside soon suggest there’s a killer among them.\n",
      "Categories: International Movies, Thrillers',\n",
      "      'Title: A Classic Horror Story (Movie)\n",
      "Description: In this gruesome suspense film, strangers traveling in southern Italy become stranded in the woods, where they must fight desperately to get out alive.\n",
      "Categories: Horror Movies, International Movies',\n",
      "    ],\n",
      "  ],\n",
      "  'uris': None,\n",
      "  'data': None,\n",
      "  'metadatas': [[{'release_year': 2021, 'type': 'Movie'}, {'release_year': 2021, 'type': 'Movie'}, {'release_year': 2021, 'type': 'Movie'}], [{'release_year': 2021, 'type': 'Movie'}, {'release_year': 2021, 'type': 'Movie'}, {'release_year': 2021, 'type': 'Movie'}]],\n",
      "  'distances': [[1.1448920965194702, 1.1504124402999878, 1.1504299640655518], [0.8908153176307678, 0.9085100889205933, 0.9213742613792419]],\n",
      "  'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the maximum batch size\n",
    "MAX_BATCH_SIZE = 5461\n",
    "\n",
    "ids=[]\n",
    "metadatas=[]\n",
    "\n",
    "with open('./datasets/netflix_titles.csv') as csvfile:\n",
    "    reader= csv.DictReader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        ids.append(row['show_id'])\n",
    "        metadatas.append({\n",
    "            'type': row['type'],\n",
    "            'release_year': int(row['release_year'])\n",
    "        })\n",
    "\n",
    "# Update the collection in batches\n",
    "for i in range(0, len(ids), MAX_BATCH_SIZE):\n",
    "    batch_ids = ids[i:i + MAX_BATCH_SIZE]\n",
    "    batch_metadatas = metadatas[i:i + MAX_BATCH_SIZE]\n",
    "    collection.update(ids=batch_ids, metadatas=batch_metadatas)\n",
    "\n",
    "result = collection.query(\n",
    "    query_texts=reference_texts,\n",
    "    n_results=3,\n",
    "    where={\n",
    "    \"$and\": [\n",
    "        {\n",
    "            \"type\": {\n",
    "                \"$eq\": \"Movie\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"release_year\": {\n",
    "                \"$gt\": 2020\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    ")\n",
    "\n",
    "# Format the dictionary output with each key-value pair on a new line\n",
    "formatted_output = \"{\\n\"\n",
    "for key, value in result.items():\n",
    "    if key == 'documents':\n",
    "        # Format each document on a new line\n",
    "        formatted_output += f\"  '{key}': [\\n\"\n",
    "        for doc_list in value:\n",
    "            formatted_output += \"    [\\n\"\n",
    "            for doc in doc_list:\n",
    "                formatted_output += f\"      '{doc}',\\n\"\n",
    "            formatted_output += \"    ],\\n\"\n",
    "        formatted_output += \"  ],\\n\"\n",
    "    else:\n",
    "        formatted_output += f\"  '{key}': {value},\\n\"\n",
    "formatted_output += \"}\"\n",
    "\n",
    "# Print the formatted output\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In this exercise, you'll use the documents from two IDs in the netflix_titles collection to query the rest of the collection, returning the most similar results as recommendations.\n",
    "\n",
    "The `netflix_titles` collection is still available to use, and `OpenAIEmbeddingFunction()` has been imported.\n",
    "\n",
    "__Instructions__\n",
    "\n",
    "* Retrieve the documents from the collection for the IDs in `reference_ids`.\n",
    "* Query the collection using `reference_texts` to return three results for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Title: Searching For Sheela (Movie)\\nDescription: Journalists and fans await Ma Anand Sheela as the infamous former Rajneesh commune’s spokesperson returns to India after decades for an interview tour.\\nCategories: Documentaries, International Movies', 'Title: Shuddhi (Movie)\\nDescription: An American woman on a revenge mission travels to India and befriends two journalists seeking justice for violent crimes against women.\\nCategories: Dramas, International Movies, Thrillers', 'Title: Anaamika (Movie)\\nDescription: As a woman scours Hyderabad for her missing husband, she becomes entangled in a conspiracy that suggests there’s more to the mystery than meets the eye.\\nCategories: International Movies, Thrillers'], ['Title: Stowaway (Movie)\\nDescription: A three-person crew on a mission to Mars faces an impossible choice when an unplanned passenger jeopardizes the lives of everyone on board.\\nCategories: Dramas, International Movies, Thrillers', \"Title: Kidnapping Stella (Movie)\\nDescription: Snatched off the street and held for ransom, a bound and gagged woman uses her limited powers to derail her two masked abductors' carefully laid plans.\\nCategories: Dramas, International Movies, Thrillers\", \"Title: Holiday on Mars (Movie)\\nDescription: A scoundrel's mission to escape his family and remarry on Mars is hilariously scrubbed when a mishap with a black hole turns his son into an old man.\\nCategories: Comedies, International Movies\"]]\n"
     ]
    }
   ],
   "source": [
    "collection = client.get_collection(\n",
    "  name=\"netflix_titles\",\n",
    "  embedding_function=OpenAIEmbeddingFunction(model_name=\"text-embedding-3-small\", api_key=api_key)\n",
    ")\n",
    "\n",
    "reference_ids = ['s999', 's1000']\n",
    "\n",
    "# Retrieve the documents for the reference_ids\n",
    "reference_texts_new = collection.get(ids=reference_ids)['documents']\n",
    "\n",
    "# Query using reference_texts\n",
    "result = collection.query(\n",
    "  query_texts=reference_texts_new,\n",
    "  n_results=3\n",
    ")\n",
    "\n",
    "print(result['documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having metadata available to use in the database can unlock the ability to more easily filter results based on additional conditions. Imagine that the film recommendations you've be creating could access the user's set preferences and use those to further filter the results.\n",
    "\n",
    "In this exercise, you'll be using additional metadata to filter your Netflix film recommendations. The `netflix_titles` collection has been updated to add metadatas to each title, including the '`rating'`, the age rating given to the title, and `'release_year'`, the year the title was initially released.\n",
    "\n",
    "Here's a preview of an updated item:\n",
    "\n",
    "```python\n",
    "{'ids': ['s999'],\n",
    " 'embeddings': None,\n",
    " 'metadatas': [{'rating': 'TV-14', 'release_year': 2021}],\n",
    " 'documents': ['Title: Searching For Sheela (Movie)\\nDescription: Journalists and fans await Ma Anand Sheela as the infamous former Rajneesh commune’s spokesperson returns to India after decades for an interview tour.\\nCategories: Documentaries, International Movies'],\n",
    " 'uris': None,\n",
    " 'data': None}\n",
    "```\n",
    "\n",
    "__Instructions__\n",
    "\n",
    "* Query two results from the collection using `reference_texts`.\n",
    "* Filter the results for titles with a `'G'` rating that were also released before `2019`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Update of nonexisting embedding ID: s95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Title: The Pirate Fairy (Movie)\\nDescription: In this spritely tale, Tinker Bell and her friends set out to find free-spirited fairy Zarina after she makes off with some precious Blue Pixie Dust.\\nCategories: Children & Family Movies', \"Title: Pup Star (Movie)\\nDescription: After a singing pup with big dreams of stardom gets dognapped and escapes with a friend's help, her journey home is a fun, music-filled adventure.\\nCategories: Children & Family Movies, Comedies\"], ['Title: Spookley the Square Pumpkin (Movie)\\nDescription: When a storm terrorizes his patch, a pumpkin cast out for his shape must step up to use his smarts to save the day – and to prove himself.\\nCategories: Children & Family Movies', 'Title: An American Tail: The Mystery of the Night Monster (Movie)\\nDescription: When a monster goes on a mouse-napping spree in New York, Fievel and his friends help a reporter get to the bottom of the mystery.\\nCategories: Children & Family Movies']]\n"
     ]
    }
   ],
   "source": [
    "reference_texts_g = [\"children's story about a car\", \"lions\"]\n",
    "\n",
    "# Define the maximum batch size\n",
    "MAX_BATCH_SIZE = 5461\n",
    "\n",
    "ids=[]\n",
    "metadatas=[]\n",
    "\n",
    "with open('./datasets/netflix_titles.csv') as csvfile:\n",
    "    reader= csv.DictReader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        ids.append(row['show_id'])\n",
    "        metadatas.append({\n",
    "            'rating': row['rating'],\n",
    "        })\n",
    "\n",
    "# Update the collection in batches\n",
    "for i in range(0, len(ids), MAX_BATCH_SIZE):\n",
    "    batch_ids = ids[i:i + MAX_BATCH_SIZE]\n",
    "    batch_metadatas = metadatas[i:i + MAX_BATCH_SIZE]\n",
    "    collection.update(ids=batch_ids, metadatas=batch_metadatas)\n",
    "\n",
    "\n",
    "# Query two results using reference_texts\n",
    "result = collection.query(\n",
    "  query_texts=reference_texts,\n",
    "  n_results=2,\n",
    "  # Filter for titles with a G rating released before 2019\n",
    "  where={\n",
    "    \"$and\": [\n",
    "        {\"release_year\": {\"$lt\": 2019}},\n",
    "        {\"rating\": {\"$eq\" : \"G\"}}\n",
    "    ]\n",
    "  }\n",
    ")\n",
    "\n",
    "print(result['documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
