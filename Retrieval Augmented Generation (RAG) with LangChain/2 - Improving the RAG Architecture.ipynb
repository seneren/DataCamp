{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Loading and splitting code files**\n",
    "\n",
    "In the previous chapter, we looked at PDF, CSV, and HTML files. Now we're going to extend this a little further to Python and Markdown files.\n",
    "\n",
    "### **Loading Markdown files (.md)**\n",
    "\n",
    "The `UnstructuredMarkdownLoader` class can be used to load markdown files the same way as other file formats we've looked at before: by instantiating the class on the file path, and using the `.load()` method to load it into memory. We could integrate these documents into a RAG application to read code documentation and make recommendations.\n",
    "\n",
    "__Note__: `UnstructuredMarkdownLoader` requires `markdown` package to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='ü¶úÔ∏èüîó LangChain\n",
      "\n",
      "‚ö° Build context-aware reasoning applications ‚ö°\n",
      "\n",
      "Looking for the JS/TS library? Check out LangChain.js.\n",
      "\n",
      "To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building, testing, and monitoring LLM applications. Fill out this form to speak with our sales team.\n",
      "\n",
      "Quick Install\n",
      "\n",
      "With pip: bash pip install langchain\n",
      "\n",
      "With conda: bash conda install langchain -c conda-forge\n",
      "\n",
      "ü§î What is LangChain?\n",
      "\n",
      "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
      "\n",
      "For these applications, LangChain simplifies the entire application lifecycle:\n",
      "\n",
      "Open-source libraries: Build your applications using LangChain's open-source building blocks, components, and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
      "\n",
      "Productionization: Inspect, monitor, and evaluate your apps with LangSmith so that you can constantly optimize and deploy with confidence.\n",
      "\n",
      "Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.\n",
      "\n",
      "Open-source libraries\n",
      "\n",
      "langchain-core: Base abstractions and LangChain Expression Language.\n",
      "\n",
      "langchain-community: Third party integrations.\n",
      "\n",
      "Some integrations have been further split into partner packages that only rely on langchain-core. Examples include langchain_openai and langchain_anthropic.\n",
      "\n",
      "langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
      "\n",
      "LangGraph: A library for building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.\n",
      "\n",
      "Productionization:\n",
      "\n",
      "LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.\n",
      "\n",
      "Deployment:\n",
      "\n",
      "LangGraph Cloud: Turn your LangGraph applications into production-ready APIs and Assistants.\n",
      "\n",
      "üß± What can you build with LangChain?\n",
      "\n",
      "‚ùì Question answering with RAG\n",
      "\n",
      "Documentation\n",
      "\n",
      "End-to-end Example: Chat LangChain and repo\n",
      "\n",
      "üß± Extracting structured output\n",
      "\n",
      "Documentation\n",
      "\n",
      "End-to-end Example: SQL Llama2 Template\n",
      "\n",
      "ü§ñ Chatbots\n",
      "\n",
      "Documentation\n",
      "\n",
      "End-to-end Example: Web LangChain (web researcher chatbot) and repo\n",
      "\n",
      "And much more! Head to the Tutorials section of the docs for more.\n",
      "\n",
      "üöÄ How does LangChain help?\n",
      "\n",
      "The main value props of the LangChain libraries are: 1. Components: composable building blocks, tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not 2. Off-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks\n",
      "\n",
      "Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.\n",
      "\n",
      "LangChain Expression Language (LCEL)\n",
      "\n",
      "LCEL is the foundation of many of LangChain's components, and is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains.\n",
      "\n",
      "Overview: LCEL and its benefits\n",
      "\n",
      "Interface: The standard Runnable interface for LCEL objects\n",
      "\n",
      "Primitives: More on the primitives LCEL includes\n",
      "\n",
      "Cheatsheet: Quick overview of the most common usage patterns\n",
      "\n",
      "Components\n",
      "\n",
      "Components fall into the following modules:\n",
      "\n",
      "üìÉ Model I/O\n",
      "\n",
      "This includes prompt management, prompt optimization, a generic interface for chat models and LLMs, and common utilities for working with model outputs.\n",
      "\n",
      "üìö Retrieval\n",
      "\n",
      "Retrieval Augmented Generation involves loading data from a variety of sources, preparing it, then searching over (a.k.a. retrieving from) it for use in the generation step.\n",
      "\n",
      "ü§ñ Agents\n",
      "\n",
      "Agents allow an LLM autonomy over how a task is accomplished. Agents make decisions about which Actions to take, then take that Action, observe the result, and repeat until the task is complete. LangChain provides a standard interface for agents, along with LangGraph for building custom agents.\n",
      "\n",
      "üìñ Documentation\n",
      "\n",
      "Please see here for full documentation, which includes:\n",
      "\n",
      "Introduction: Overview of the framework and the structure of the docs.\n",
      "\n",
      "Tutorials: If you're looking to build something specific or are more of a hands-on learner, check out our tutorials. This is the best place to get started.\n",
      "\n",
      "How-to guides: Answers to ‚ÄúHow do I‚Ä¶.?‚Äù type questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task.\n",
      "\n",
      "Conceptual guide: Conceptual explanations of the key parts of the framework.\n",
      "\n",
      "API Reference: Thorough documentation of every class and method.\n",
      "\n",
      "üåê Ecosystem\n",
      "\n",
      "ü¶úüõ†Ô∏è LangSmith: Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\n",
      "\n",
      "ü¶úüï∏Ô∏è LangGraph: Create stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.\n",
      "\n",
      "ü¶úüèì LangServe: Deploy LangChain runnables and chains as REST APIs.\n",
      "\n",
      "üíÅ Contributing\n",
      "\n",
      "As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\n",
      "\n",
      "For detailed information on how to contribute, see here.\n",
      "\n",
      "üåü Contributors' metadata={'source': './datasets/README.md'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = UnstructuredMarkdownLoader('./datasets/README.md')\n",
    "markdown_content = loader.load()\n",
    "\n",
    "print(markdown_content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Python files (.py)**\n",
    "\n",
    "Imagine we have a codebase and would like to have a way to talk with it and ask it questions about it. We could achieve this by integrating Python files into a RAG application. The `PythonLoader` class and the `.load()` method can be used to load these files into memory. The resulting documents have `.page_content` and metadata attributes for accessing the document's details. Remember parsing Python files can be tricky, because it has its own syntax with _imports_, _classes_, _functions_ and much more that need to be preserved during chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='from abc import ABC, abstractmethod\n",
      "\n",
      "class LLM(ABC):\n",
      "  @abstractmethod\n",
      "  def complete_sentence(self, prompt):\n",
      "    pass\n",
      "\n",
      "class OpenAI(LLM):\n",
      "  def complete_sentence(self, prompt):\n",
      "    return prompt + \" ... OpenAI end of sentence.\"\n",
      "  \n",
      "class Anthropic(LLM):\n",
      "  def complete_sentence(self, prompt):\n",
      "    return prompt + \" ... Anthropic end of sentence.\"\n",
      "\n",
      "class ChatBot:\n",
      "  def _get_llm(self, provider):\n",
      "    if provider == \"OpenAI\":\n",
      "      return OpenAI()\n",
      "    elif provider == \"Anthropic\":\n",
      "      return Anthropic()\n",
      "    \n",
      "  def chat(self, prompt, provider):\n",
      "    # Return an llm object, then call complete_sentence()\n",
      "    llm = self._get_llm(provider)\n",
      "    return llm.complete_sentence(prompt)' metadata={'source': './datasets/chatbot.py'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "loader = PythonLoader('./datasets/chatbot.py')\n",
    "\n",
    "python_data = loader.load()\n",
    "\n",
    "print(python_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting code files**\n",
    "\n",
    "We wil use our _best_ tool for document splitting: `RecursiveCharacterTextSplitter`. We set `chunk_size`, and `chunk_overlap` as it should be. Splitting the documents with `.split_documents()` method, we can print the content of each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "from abc import ABC, abstractmethod\n",
      "\n",
      "class LLM(ABC):\n",
      "  @abstractmethod\n",
      "  def complete_sentence(self, prompt):\n",
      "    pass\n",
      "\n",
      "Chunk 2:\n",
      "class OpenAI(LLM):\n",
      "  def complete_sentence(self, prompt):\n",
      "    return prompt + \" ... OpenAI end of sentence.\"\n",
      "  \n",
      "class Anthropic(LLM):\n",
      "\n",
      "Chunk 3:\n",
      "def complete_sentence(self, prompt):\n",
      "    return prompt + \" ... Anthropic end of sentence.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# The python file is already loaded in the previous code cell\n",
    "\n",
    "python_split = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=10)   # We didn't specify the separators, so it will use the default ones - which are [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "\n",
    "chunks = python_split.split_documents(python_data)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the split between chunks two and three splits the Anthropic class, and because chunks are processed separately, key context has been lost. Our current strategy is naive because it doesn't consider structures like classes and functions. Let's change this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting by language**\n",
    "\n",
    "We split the loaded Python file using `RecursiveCharacterTextSplitter` again, but this time, we will use the `.from_language()` method. This method has a language argument, which refers to coding languages, what we can set to `Language.PYTHON`, and the rest of the arguments stay the same.\n",
    "\n",
    "This will modify the default separators list from the hierarchy of apragraphs, sentences, and words, to try splitting on classes and function definitions before moving on to the standard separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "from abc import ABC, abstractmethod\n",
      "\n",
      "class LLM(ABC):\n",
      "  @abstractmethod\n",
      "  def complete_sentence(self, prompt):\n",
      "    pass\n",
      "\n",
      "Chunk 2:\n",
      "class OpenAI(LLM):\n",
      "  def complete_sentence(self, prompt):\n",
      "    return prompt + \" ... OpenAI end of sentence.\"\n",
      "\n",
      "Chunk 3:\n",
      "class Anthropic(LLM):\n",
      "  def complete_sentence(self, prompt):\n",
      "    return prompt + \" ... Anthropic end of sentence.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import Language    # We already imported the other required modules in the previous code cell\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=150, chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the splitter was able to split on class definitions, so all of that context is kept together.\n",
    "\n",
    "Note that this approach isn't final, and depending on the size of the classes and functions relative to the `chunk_size`, we may get differing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__import__('pysqlite3')\n",
      "import sys\n",
      "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "from langchain_chroma import Chroma\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_huggingface import HuggingFaceEmbeddings\n",
      "from langchain_community.document_loaders import PyPDFLoader\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain_core.runnables import RunnablePassthrough\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "import shutil\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
      "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
      "\n",
      "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
      "\n",
      "loader = PyPDFLoader(\"rag_paper.pdf\")\n",
      "documents = loader.load()\n",
      "# Split the documents into manageable chunks\n",
      "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
      "split_documents = text_splitter.split_documents(documents)\n",
      "\n",
      "# Initialize the all-MiniLM-L6-v2 embedding model\n",
      "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "\n",
      "# Create a Chroma vector store and embed the chunks\n",
      "vector_store = Chroma.from_documents(\n",
      "    documents=chunks, \n",
      "    embedding=embedding_model\n",
      ")\n",
      "\n",
      "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
      "Use the only the context provided to answer the following question. If you don't know the answer, reply that you are unsure.\n",
      "Context: {context}\n",
      "Question: {question}\n",
      "\"\"\")\n",
      "\n",
      "llm = ChatOpenAI(api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
      "\n",
      "# Convert the vector store into a retriever\n",
      "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
      "\n",
      "# Create the LCEL retrieval chain\n",
      "chain = (\n",
      "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
      "    | prompt\n",
      "    | llm\n",
      "    | StrOutputParser()\n",
      ")\n",
      "\n",
      "# Invoke the chain\n",
      "print(chain.invoke({\"question\": \"Who are the authors?\"}))\n"
     ]
    }
   ],
   "source": [
    "loader = PythonLoader('./datasets/rag.py')\n",
    "\n",
    "python_data = loader.load()\n",
    "\n",
    "print(python_data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "__import__('pysqlite3')\n",
      "import sys\n",
      "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
      "from langchain_chroma import Chroma\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_huggingface import HuggingFaceEmbeddings\n",
      "\n",
      "Chunk 2:\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_huggingface import HuggingFaceEmbeddings\n",
      "from langchain_community.document_loaders import PyPDFLoader\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain_core.runnables import RunnablePassthrough\n",
      "\n",
      "Chunk 3:\n",
      "from langchain_core.runnables import RunnablePassthrough\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "import shutil\n",
      "import getpass\n",
      "import os\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Python-aware recursive character splitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=300, chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Split the Python content into chunks\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Advanced splitting methods**\n",
    "\n",
    "### **Limitations of our current splitting strategies**\n",
    "\n",
    "- Splits are naive (not context-aware)\n",
    "  - Ignores context of surrounding text\n",
    "- Splits are made using characters, not tokens\n",
    "  - LLM's break text into tokens, or smaller units of text, for processing\n",
    "  - Splitting on characters can lead to a risk of exceeding the model __context window__\n",
    "    - __Context window__: is the maximum number of tokens/text units that can be processed at once\n",
    "\n",
    "We'll introduce methods to make our splitter more aware of the document's context and enable splitting on tokens:\n",
    "- `SemanticChunker`\n",
    "- `TokenTextSplitter`\n",
    "\n",
    "### **Splitting on tokens**\n",
    "\n",
    "When we split on tokens, the `chunk_size` and `chunk_overlap` refer to the _number of tokens_ in the chunk, rather than characters, so a `chunk_size` of five means we can have a _maximum of five tokens in the chunk_.\n",
    "\n",
    "<img src='./images/tokensplit.png' width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Mary had a little lamb, it's fleece was white\n",
      "\n",
      "Chunk 2:\n",
      " was white as snow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken      # We use this library to count tokens\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "example_string = \"Mary had a little lamb, it's fleece was white as snow.\"\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "splitter = TokenTextSplitter(encoding_name=encoding.name, chunk_size=10, chunk_overlap=2)\n",
    "\n",
    "chunks = splitter.split_text(example_string)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check if it is able to keep to the chunk_size 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "No. tokens: 10\n",
      "Mary had a little lamb, it's fleece was white\n",
      "\n",
      "Chunk 2:\n",
      "No. tokens: 5\n",
      " was white as snow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\nNo. tokens: {len(encoding.encode(chunk))}\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Semantic splitting**\n",
    "\n",
    "To perform semantic splitting, we'll need an embedding model to generate text embeddings to determine the shift in topic. We'll use a model from OpenAI. We instantiate the semantic splitting class, passing the embedding model. We pass two additional parameters: \n",
    "- `breakpoint_threshold_type`, which sets the metric at which embeddings are compared, and\n",
    "- `breakpoint_threshold_amount`, which sets the metric's threshold at which to perform the split.\n",
    "\n",
    "Like other splitters, we use the `.split_documents()` method to apply the splitter, in this case, __rag-paper.pdf__ academic paper. The semantic splitter reached the threshold of `0.8` and performed the splits; for the first chunk, splitting after the first two sentences of the abstract.\n",
    "\n",
    "__Note__: The SemanticChunker is a module from the `langchain_experimental` library, which needs to be installed.\n",
    "\n",
    "To read more about SemanticChunker, see [here](https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "Patrick Lewis‚Ä†‚Ä°, Ethan Perez‚ãÜ,\n",
      "Aleksandra Piktus‚Ä†, Fabio Petroni‚Ä†, Vladimir Karpukhin‚Ä†, Naman Goyal‚Ä†, Heinrich K√ºttler‚Ä†,\n",
      "Mike Lewis‚Ä†, Wen-tau Yih‚Ä†, Tim Rockt√§schel‚Ä†‚Ä°, Sebastian Riedel‚Ä†‚Ä°, Douwe Kiela‚Ä†\n",
      "‚Ä†Facebook AI Research; ‚Ä°University College London; ‚ãÜNew York University;\n",
      "plewis@fb.com\n",
      "Abstract\n",
      "Large pre-trained language models have been shown to store factual knowledge\n",
      "in their parameters, and achieve state-of-the-art results when Ô¨Åne-tuned on down-\n",
      "stream NLP tasks.' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './datasets/rag-paper.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "loader = PyPDFLoader('./datasets/rag-paper.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=openai.api_key, model=\"text-embedding-3-small\")\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_amount=0.8,   # this value is between 0 and 1. When 0, the chunker will not split the text. When 1, the chunker will split the text at every sentence.\n",
    "    breakpoint_threshold_type=\"gradient\"\n",
    ")\n",
    "\n",
    "chunks = semantic_splitter.split_documents(data)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Optimizing document retrieval**\n",
    "\n",
    "<img src='./images/r-in-rag.png' width=50%>\n",
    "\n",
    "So far, our document retrieval has consisted of a vector database containing embedded documents. The input to the RAG application is then used to query the vectors, using a distance metric to determine which vectors are closest and therefore most similar and relevant. This type of retrieval is known as **dense retrieval**.\n",
    "\n",
    "  <div style=\"display: flex;\">\n",
    "    <!-- Left Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "    <b>Dense Retrieval</b><br><br>\n",
    "    Encode chunks as a single vector with <b>non-zero</b> components, that is said to be \"dense\", that is, most of its component values are non-zero. <br><br>\n",
    "    <img src='./images/dense.png' width=58%>\n",
    "    <ul>\n",
    "      <li><b>Pros</b>: Capturing semantic meaning</li>\n",
    "      <li><b>Cons</b>: </li>\n",
    "        <ul>\n",
    "         <li>Computationally expensive</li>\n",
    "         <li>May struggle with capturing rare-words or highly specific technical terms</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    </div>\n",
    "    <!-- Right Column -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "    <b>Sparse Retrieval</b><br>\n",
    "    Encode using <b>word matching</b> with mostly <b>zero</b> components. It is a method of finding information by matching specific keywords or terms in a query with those in documents. The resulting vectors contain many zeros, with only a few non-zero terms, which is why they are said to be \"sparse\". \n",
    "    <div>\n",
    "    <img src='./images/sparse.png' width=90%>\n",
    "    <ul>\n",
    "      <li><b>Pros</b>: Precise, explainable, rare-word handling</li>\n",
    "      <li><b>Cons</b>: </li>\n",
    "        <ul>\n",
    "         <li>Generalizability</li>\n",
    "         <li>Not extracting the semantic meaning from the text</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### **Sparse retrieval methods**\n",
    "\n",
    "Two popular methods for encoding text into sparse vectors are:\n",
    "- **TF-IDF** (Term Frequency-Inverse Document Frequency) <br>\n",
    "     Encodes documents using the words that make the document unique: Creates a sparse vector that measures a term's frequency in a document and rarity in other documents. This helps in identifying words that best represent the document's unique content.\n",
    "- **BM25** (Best Matching 25)<br>\n",
    "     Helps mitigate high-frequency words from saturating the encoding: BM25 is an improvement on TD-IDF that prevents high-frequency words from being over-emphasized in the encoding.\n",
    "\n",
    "**BM25 retrieval**\n",
    "\n",
    "The BM25Retriever class can be used to create a retriever from documents or text, just like the retrievers we have already used. We can use the `.from_texts()` method to create the retriever from these strings. The k value sets the number of items returned by the retriever when invoked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Document:\n",
      "Python was created by Guido van Rossum and released in 1991.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "chunks = [\n",
    "    \"Python was created by Guido van Rossum and released in 1991.\",\n",
    "    \"Python is a popular language for machine learning (ML).\",\n",
    "    \"The PyTorch library is a popular Python library for AI and ML.\"\n",
    "]\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks, k=3)\n",
    "\n",
    "results = bm25_retriever.invoke(\"When was Python created?\")\n",
    "print(\"Most Relevant Document:\")\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all three statements again, we can see that BM25 returned the statement with similar terms to the input that were also unique to the other statements.\n",
    "\n",
    "**BM25 in RAG**\n",
    "\n",
    "We'll create a RAG system to integrate a DataCamp blog post on RAG with an LLM. \n",
    "\n",
    "The first step is the same as before, but using the `.from_documents()` method as we're dealing with document chunks and not strings this time. Then, we use the same LCEL syntax as a standard dense retrieval RAG to integrate the retriever with a prompt template and LLM. Remember that `RunnablePassthrough` allows us to insert the input unchanged into the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM hallucination can significantly impact a RAG (Retrieval-Augmented Generation) application by leading to the generation of inaccurate or misleading information. When an LLM produces content that is not grounded in the retrieved documents or factual data, it can create confusion and reduce the reliability of the insights derived from the application. This can be particularly problematic in contexts like market research, where accurate information is crucial for decision-making. To mitigate these issues, advanced techniques such as reranking and multi-step reasoning can be implemented to enhance the accuracy and relevance of the generated outputs, thereby addressing the challenges posed by hallucination.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_experimental.text_splitter import SemanticChunker \n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "html_loader = UnstructuredHTMLLoader('./datasets/what-is-rag-blog.html')\n",
    "\n",
    "document = html_loader.load()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=openai.api_key, model=\"text-embedding-3-small\")\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_amount=0.8,\n",
    "    breakpoint_threshold_type=\"gradient\"\n",
    ")\n",
    "\n",
    "chunks = semantic_splitter.split_documents(document)\n",
    "\n",
    "retriever = BM25Retriever.from_documents(\n",
    "    documents=chunks,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', api_key=openai.api_key, temperature=0)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"How can LLM hallucination impact a RAG application?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Introduction to RAG evaluation**\n",
    "\n",
    "Because our RAG architecture is made up of several processes, there are a few places where performance can be measured. \n",
    "\n",
    "<img src='./images/rag-eval.png' width=60%>\n",
    "\n",
    "- We can evaluate the retrieval process to check if the retrieved documents are relevant to the query\n",
    "- We can evaluate the generation process to see if the LLM hallucinated or misinterpreted the prompt\n",
    "- We can evaluate the final output to measure the performance of the whole system.\n",
    "\n",
    "\n",
    "### **Output accuracy: string evaluation**\n",
    "\n",
    "To perform string evaluation, we need to define a prompt template and large language model to use for evaluation. The prompt template instructs the model to compare the strings and evaluate the model output for correctness, returning correct or incorrect. The model temperature is also set to zero to minimize variability.\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:{query}\n",
    "Here is the real answer:{answer}\n",
    "You are grading the following predicted answer:{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "eval_llm = ChatOpenAI(temperature = 0, model = \"gpt-4o-mini\", openai_api_key=\"...\")\n",
    "```\n",
    "\n",
    "We initialize `LangChainStringEvaluator` from `LangSmith`, which is LangChain's platform for evaluating LLM applications. This evaluator first takes `\"qa\"`, which sets the evaluator to assess correctness, and also the LLM and prompt template to use. We then call the `.evaluate_strings()` method on the model prediction, reference answer, and input query to perform the evaluation.\n",
    "\n",
    "```python\n",
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\n",
    "    \"qa\",\n",
    "    config={\n",
    "        \"llm\":eval_llm,\n",
    "        \"prompt\":PROMPT\n",
    "    }\n",
    ")\n",
    "\n",
    "score = qa_evaluator.evaluator.evaluate_strings(\n",
    "    prediction=predicted_answer,\n",
    "    reference=ref_answer,\n",
    "    input=query\n",
    ")\n",
    "```\n",
    "\n",
    "A score of zero indicates that predicted response was incorrect when compared to the reference answer. \n",
    "\n",
    "```python\n",
    "print(f\"score: {score}\")\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "\n",
    "```python\n",
    "Score: {'reasoning': 'INCORRECT', 'value': 'INCORRECT', 'score': 0}\n",
    "```\n",
    "\n",
    "And we can see here that the model response was deemed incorrect, which makes sense on reviewing it again:\n",
    "\n",
    "```python\n",
    "query = \"What are the main components of RAG architecture?\"\n",
    "predicted_answer = \"Training and encoding\"\n",
    "ref_answer = \"Retrieval and Generation\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RAGAS framework**\n",
    "\n",
    "RAGAS was designed to evaluate both the retrieval and generation components of a RAG application. We will cover one metric for each component: faithfulness and context precision.\n",
    "\n",
    "<img src='./images/ragas-score.png' width=50%>\n",
    "\n",
    "\n",
    "- **Faithfulness**\n",
    "  Assesses whether the generated output represents the retrieved documents well. It is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{Faithfulness} = \\frac{\\text{Number of claims that can be inferred from the context}}{\\text{Total number of claims}}\n",
    "  $$\n",
    "\n",
    "  Because faithfulness is a proportion, _it is normalized to between zero and one_, where a higher score indicates greater faithfulness.\n",
    "\n",
    "Ragas integrates nicely with LangChain, and the first step involves defining the models for the evaluator to use: one for generation and another for embeddings. Next, we define an evaluation chain, passing it the faithfulness metric from ragas and the two models we defined.\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "from ragas.metrics import faithfulness\n",
    "\n",
    "llm = ChatOpenAI(model= \"gpt-4o-mini\", api_key=\"...\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"...\")\n",
    "\n",
    "faithfulness_chain = EvaluatorChain(\n",
    "    metric=faithfulness,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "```\n",
    "\n",
    "To evaluate a model's response, we instantiate the chain, passing it a dictionary with `\"question\"`, `\"answer\"`, and `\"contexts\"` keys. `\"question\"` is the query sent to the RAG application, `\"answer\"` is the response, and `\"contexts\"` are the document chunks available to the model. A perfect faithfulness score of one indicates that the model's response could be fully inferred from the context provided.\n",
    "\n",
    "```python\n",
    "eval_result = faithfulness_chain({\n",
    "    \"question\": \"How does the RAG model improve question answering with LLMs?\",\n",
    "    \"answer\": \"The RAG model improves question answering by combining the retrieval of documents...\",\n",
    "    \"contexts\": [\n",
    "        \"The RAG model integrates document retrieval with LLMs by first retrieving relevant passages...\",\n",
    "        \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources, allowing the...\",\n",
    "        ]\n",
    "})\n",
    "print(eval_result)\n",
    "```\n",
    "\n",
    "Output:<br>\n",
    "```python\n",
    "'faithfulness': 1.0\n",
    "```\n",
    "\n",
    "### **Context precision**\n",
    "\n",
    "Context precision measures how relevant the retrieved documents are to the query. \n",
    "\n",
    "A context precision score closer to one means the retrieved context is highly relevant. The only change we need to make to the faithfulness evaluation chain is to import and use the `content_precision` metric instead.\n",
    "\n",
    "```python\n",
    "from ragas.metrics import context_precision\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"...\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"...\")\n",
    "\n",
    "context_precision_chain = EvaluatorChain(\n",
    "    metric=context_precision,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "```\n",
    "\n",
    "The `context_precision_chain` similarly takes a dictionary with `\"question\"`, `\"contexts\"`, and `\"ground_truth\"` keys, representing the input query, the retrieved documents, and the ground truth document that should have been retrieved. Printing the results, we can see that we achieved a high context precision, indicating that the retrieval process is returning highly relevant documents.\n",
    "\n",
    "```python\n",
    "eval_result = context_precision_chain({\n",
    "    \"question\": \"How does the RAG model improve question answering with large language models?\"\n",
    "    \"ground_truth\": \"The RAG model improves question answering by combining the retrieval of...\",\n",
    "    \"contexts\": [\n",
    "        \"The RAG model integrates document retrieval with LLMs by first retrieving...\",\n",
    "        \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources...\",\"\n",
    "        ]\n",
    "})\n",
    "\n",
    "print(f\"Context Precision: {eval_result['context_precision']}\")\n",
    "```\n",
    "\n",
    "Output:<br>\n",
    "```python\n",
    "Context Precision: 0,999999995\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
