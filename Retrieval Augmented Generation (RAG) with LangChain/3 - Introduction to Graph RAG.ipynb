{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. From vectors to graphs**\n",
    "\n",
    "The RAG architectures we've explored involve embedding a user input and querying a vector store to return relevant documents based on their semantic similarity. Although powerful, this approach does have some limitations. \n",
    "\n",
    "Firstly, document embedding captures semantic meaning but struggles to capture themes and relationships between entities in the document corpus.\n",
    "\n",
    "Moreover, as the volume of the database grows, the retrieval process can become less efficient, as the computational load increases with the search space.\n",
    "\n",
    "Lastly, vector RAG systems don't easily accommodate structured or diverse data, which are harder to embed.\n",
    "\n",
    "<img src='./images/vector-rag-limitations.png' width=50%>\n",
    "\n",
    "###  **Graph Databases**\n",
    "\n",
    "We can address all of those challenges with graphs. Graphs are great at representing and storing diverse and interconnected information in a structured manner. \n",
    "\n",
    "Entities, like people, places, and sports teams We can address all of those challenges with graphs. Graphs are great at representing and storing diverse and interconnected information in a structured manner. Entities, like people, places, and sports teams are represented by labeled edges.\n",
    "\n",
    "Notice that edges are directional, so relationships can apply from one entity to another, but not necessarily the other way around. We'll look at this more closely in a later video.\n",
    "\n",
    "<img src='./images/graphdb.png' width=50%>\n",
    "\n",
    "### **Neo4j graph databases**\n",
    "\n",
    "Neo4j is a powerful graph database option designed to store and efficiently query complex relationships.\n",
    "\n",
    "Our entities are represented as nodes, where the color indicates the entity type, such as a person. The relationships are represented by edges with types like LOCATED and INTERESTED. \n",
    "\n",
    "Each node also has a type and unique identifier. Nodes can contain any number of properties represented as key:value pairs.\n",
    "\n",
    "<img src='./images/graph-neo4j.png' width=50%>\n",
    "\n",
    "### **Loading and chunking Wikipedia pages**\n",
    "\n",
    "So how do we go from the unstructured text data we've seen to a nice structured graph? There's a few ways to do this, but we'll be using LLMs. Let's load the Wikipedia results from searching `\"large language model\"` using the `WikipediaLoader` class, and split the first few documents into chunks. Each document has page content and metadata as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
      "The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ont' metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "raw_documents = WikipediaLoader(query=\"large language model\").load()\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(raw_documents[:3])\n",
    "\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **From text to graph**\n",
    "\n",
    "We begin by defining the LLM to use for the transformation, and use it to create an `LLMGraphTransformer`. Note that we use `temperature=0` to produce more deterministic graphs for greater reliability. The LLM creates structured graph documents by parsing and categorizing entities and their relationships, which it infers from the documents. The transformation is performed using the `.convert_to_graph_documents()` method on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Large Language Model', type='Machine learning model', properties={}), Node(id='Natural Language Processing', type='Task', properties={}), Node(id='Language Generation', type='Task', properties={}), Node(id='Parameters', type='Concept', properties={}), Node(id='Self-Supervised Learning', type='Learning method', properties={}), Node(id='Text', type='Data', properties={}), Node(id='Generative Pretrained Transformers', type='Model', properties={}), Node(id='Gpts', type='Model', properties={}), Node(id='Specific Tasks', type='Task', properties={}), Node(id='Prompt Engineering', type='Technique', properties={}), Node(id='Predictive Power', type='Concept', properties={}), Node(id='Syntax', type='Concept', properties={}), Node(id='Semantics', type='Concept', properties={}), Node(id='Ontology', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Natural Language Processing', type='Task', properties={}), type='DESIGNED_FOR', properties={}), Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Language Generation', type='Task', properties={}), type='DESIGNED_FOR', properties={}), Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Parameters', type='Concept', properties={}), type='HAS_MANY', properties={}), Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Self-Supervised Learning', type='Learning method', properties={}), type='TRAINED_WITH', properties={}), Relationship(source=Node(id='Self-Supervised Learning', type='Learning method', properties={}), target=Node(id='Text', type='Data', properties={}), type='ON', properties={}), Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Generative Pretrained Transformers', type='Model', properties={}), type='IS_A_TYPE_OF', properties={}), Relationship(source=Node(id='Generative Pretrained Transformers', type='Model', properties={}), target=Node(id='Gpts', type='Model', properties={}), type='IS_A_TYPE_OF', properties={}), Relationship(source=Node(id='Gpts', type='Model', properties={}), target=Node(id='Specific Tasks', type='Task', properties={}), type='FINE-TUNED_FOR', properties={}), Relationship(source=Node(id='Gpts', type='Model', properties={}), target=Node(id='Prompt Engineering', type='Technique', properties={}), type='GUIDED_BY', properties={}), Relationship(source=Node(id='Gpts', type='Model', properties={}), target=Node(id='Predictive Power', type='Concept', properties={}), type='ACQUIRE', properties={}), Relationship(source=Node(id='Predictive Power', type='Concept', properties={}), target=Node(id='Syntax', type='Concept', properties={}), type='REGARDING', properties={}), Relationship(source=Node(id='Predictive Power', type='Concept', properties={}), target=Node(id='Semantics', type='Concept', properties={}), type='REGARDING', properties={}), Relationship(source=Node(id='Predictive Power', type='Concept', properties={}), target=Node(id='Ontology', type='Concept', properties={}), type='REGARDING', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ont')), GraphDocument(nodes=[Node(id='Language Models', type='Concept', properties={}), Node(id='Ibm Alignment Models', type='Concept', properties={}), Node(id='Statistical Language Modelling', type='Concept', properties={}), Node(id='N-Gram Model', type='Concept', properties={}), Node(id='2017', type='Date', properties={}), Node(id='1990S', type='Date', properties={}), Node(id='2001', type='Date', properties={}), Node(id='0.3 Billion', type='Quantity', properties={})], relationships=[Relationship(source=Node(id='Ibm Alignment Models', type='Concept', properties={}), target=Node(id='Statistical Language Modelling', type='Concept', properties={}), type='PIONEERED', properties={}), Relationship(source=Node(id='N-Gram Model', type='Concept', properties={}), target=Node(id='Statistical Language Modelling', type='Concept', properties={}), type='TYPE_OF', properties={}), Relationship(source=Node(id='N-Gram Model', type='Concept', properties={}), target=Node(id='0.3 Billion', type='Quantity', properties={}), type='TRAINED_ON', properties={}), Relationship(source=Node(id='1990S', type='Date', properties={}), target=Node(id='Ibm Alignment Models', type='Concept', properties={}), type='TIME_PERIOD', properties={}), Relationship(source=Node(id='2001', type='Date', properties={}), target=Node(id='N-Gram Model', type='Concept', properties={}), type='TIME_PERIOD', properties={}), Relationship(source=Node(id='2017', type='Date', properties={}), target=Node(id='Language Models', type='Concept', properties={}), type='TIME_PERIOD', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n\\n== History ==\\n\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion')), GraphDocument(nodes=[Node(id='Statistical Language Modelling', type='Concept', properties={}), Node(id='Smoothed N-Gram Model', type='Concept', properties={}), Node(id='2001', type='Date', properties={}), Node(id='0.3 Billion Words', type='Quantity', properties={}), Node(id='State-Of-The-Art Perplexity', type='Concept', properties={}), Node(id='2000S', type='Date', properties={}), Node(id='Internet', type='Concept', properties={}), Node(id='Internet-Scale Language Datasets', type='Concept', properties={}), Node(id='Web As Corpus', type='Concept', properties={}), Node(id='Statistical Language Models', type='Concept', properties={}), Node(id='Symbolic Language Models', type='Concept', properties={}), Node(id='2009', type='Date', properties={}), Node(id='Language Processing Tasks', type='Concept', properties={}), Node(id='Neural Networks', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Statistical Language Modelling', type='Concept', properties={}), target=Node(id='Smoothed N-Gram Model', type='Concept', properties={}), type='PIIONEEERED', properties={}), Relationship(source=Node(id='Smoothed N-Gram Model', type='Concept', properties={}), target=Node(id='2001', type='Date', properties={}), type='DEVELOPED_IN', properties={}), Relationship(source=Node(id='Smoothed N-Gram Model', type='Concept', properties={}), target=Node(id='0.3 Billion Words', type='Quantity', properties={}), type='TRAINED_ON', properties={}), Relationship(source=Node(id='Smoothed N-Gram Model', type='Concept', properties={}), target=Node(id='State-Of-The-Art Perplexity', type='Concept', properties={}), type='ACHIEVED', properties={}), Relationship(source=Node(id='2000S', type='Date', properties={}), target=Node(id='Internet', type='Concept', properties={}), type='TIME_PERIOD', properties={}), Relationship(source=Node(id='Internet', type='Concept', properties={}), target=Node(id='Internet-Scale Language Datasets', type='Concept', properties={}), type='CONSTRUCTED', properties={}), Relationship(source=Node(id='Internet-Scale Language Datasets', type='Concept', properties={}), target=Node(id='Web As Corpus', type='Concept', properties={}), type='REFERRED_TO_AS', properties={}), Relationship(source=Node(id='Internet-Scale Language Datasets', type='Concept', properties={}), target=Node(id='Statistical Language Models', type='Concept', properties={}), type='TRAINED_ON', properties={}), Relationship(source=Node(id='2009', type='Date', properties={}), target=Node(id='Language Processing Tasks', type='Concept', properties={}), type='TIME_PERIOD', properties={}), Relationship(source=Node(id='Statistical Language Models', type='Concept', properties={}), target=Node(id='Symbolic Language Models', type='Concept', properties={}), type='DOMINATED_OVER', properties={}), Relationship(source=Node(id='Statistical Language Models', type='Concept', properties={}), target=Node(id='Large Datasets', type='Concept', properties={}), type='INGESTED', properties={}), Relationship(source=Node(id='Neural Networks', type='Concept', properties={}), target=Node(id='Statistical Language Models', type='Concept', properties={}), type='BECAME_DOMINANT_AFTER', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models because they can usefully ingest large datasets.\\n\\nAfter neural networks became dominant')), GraphDocument(nodes=[Node(id='Neural Networks', type='Technology', properties={}), Node(id='Image Processing', type='Field', properties={}), Node(id='Language Modelling', type='Field', properties={}), Node(id='Google', type='Organization', properties={}), Node(id='Neural Machine Translation', type='Technology', properties={}), Node(id='Transformers', type='Technology', properties={}), Node(id='Seq2Seq Deep Lstm Networks', type='Technology', properties={}), Node(id='2017 Neurips Conference', type='Event', properties={}), Node(id='Attention Is All You Need', type='Publication', properties={})], relationships=[Relationship(source=Node(id='Neural Networks', type='Technology', properties={}), target=Node(id='Image Processing', type='Field', properties={}), type='APPLIED_TO', properties={}), Relationship(source=Node(id='Neural Networks', type='Technology', properties={}), target=Node(id='Language Modelling', type='Field', properties={}), type='APPLIED_TO', properties={}), Relationship(source=Node(id='Google', type='Organization', properties={}), target=Node(id='Neural Machine Translation', type='Technology', properties={}), type='CONVERTED_TO', properties={}), Relationship(source=Node(id='Neural Machine Translation', type='Technology', properties={}), target=Node(id='Seq2Seq Deep Lstm Networks', type='Technology', properties={}), type='IMPLEMENTED_BY', properties={}), Relationship(source=Node(id='2017 Neurips Conference', type='Event', properties={}), target=Node(id='Google', type='Organization', properties={}), type='HOSTED_BY', properties={}), Relationship(source=Node(id='Google', type='Organization', properties={}), target=Node(id='Transformers', type='Technology', properties={}), type='INTRODUCED', properties={}), Relationship(source=Node(id='Transformers', type='Technology', properties={}), target=Node(id='Attention Is All You Need', type='Publication', properties={}), type='RELATED_TO', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' over symbolic language models because they can usefully ingest large datasets.\\n\\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. Because it preceded the existence of transformers, it was done by seq2seq deep LSTM networks.\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal')), GraphDocument(nodes=[Node(id='Transformer Architecture', type='Concept', properties={}), Node(id='Attention Is All You Need', type='Paper', properties={}), Node(id='Seq2Seq Technology', type='Concept', properties={}), Node(id='Attention Mechanism', type='Concept', properties={}), Node(id='Bahdanau Et Al.', type='Person', properties={}), Node(id='Bert', type='Model', properties={}), Node(id='2018', type='Year', properties={})], relationships=[Relationship(source=Node(id='Transformer Architecture', type='Concept', properties={}), target=Node(id='Attention Is All You Need', type='Paper', properties={}), type='INTRODUCED', properties={}), Relationship(source=Node(id='Attention Is All You Need', type='Paper', properties={}), target=Node(id='Seq2Seq Technology', type='Concept', properties={}), type='IMPROVED_UPON', properties={}), Relationship(source=Node(id='Attention Is All You Need', type='Paper', properties={}), target=Node(id='Attention Mechanism', type='Concept', properties={}), type='BASED_ON', properties={}), Relationship(source=Node(id='Attention Mechanism', type='Concept', properties={}), target=Node(id='Bahdanau Et Al.', type='Person', properties={}), type='DEVELOPED_BY', properties={}), Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='2018', type='Year', properties={}), type='INTRODUCED', properties={}), Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='Transformer Architecture', type='Concept', properties={}), type='BASED_ON', properties={}), Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='Academic And Research Usage', type='Concept', properties={}), type='DECLINED', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline')), GraphDocument(nodes=[Node(id='Bert', type='Model', properties={}), Node(id='Gpt-1', type='Model', properties={}), Node(id='Gpt-2', type='Model', properties={}), Node(id='Gpt-3', type='Model', properties={}), Node(id='Openai', type='Organization', properties={})], relationships=[Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='Gpt-1', type='Model', properties={}), type='COMPETES_WITH', properties={}), Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='Gpt-2', type='Model', properties={}), type='COMPETES_WITH', properties={}), Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='Gpt-3', type='Model', properties={}), type='COMPETES_WITH', properties={}), Relationship(source=Node(id='Gpt-1', type='Model', properties={}), target=Node(id='Gpt-2', type='Model', properties={}), type='PRECEDED_BY', properties={}), Relationship(source=Node(id='Gpt-2', type='Model', properties={}), target=Node(id='Openai', type='Organization', properties={}), type='DEVELOPED_BY', properties={}), Relationship(source=Node(id='Gpt-3', type='Model', properties={}), target=Node(id='Openai', type='Organization', properties={}), type='DEVELOPED_BY', properties={}), Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='2023', type='Event', properties={}), type='DECLINE_IN_USAGE', properties={}), Relationship(source=Node(id='Gpt-2', type='Model', properties={}), target=Node(id='2019', type='Event', properties={}), type='CAUGHT_ATTENTION', properties={}), Relationship(source=Node(id='Gpt-1', type='Model', properties={}), target=Node(id='2018', type='Event', properties={}), type='INTRODUCED', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in')), GraphDocument(nodes=[Node(id='Gpt-3', type='Technology', properties={}), Node(id='Gpt-4', type='Technology', properties={}), Node(id='Chatgpt', type='Technology', properties={}), Node(id='Api', type='Technology', properties={}), Node(id='2020', type='Year', properties={}), Node(id='2022', type='Year', properties={}), Node(id='2023', type='Year', properties={}), Node(id='2024', type='Year', properties={})], relationships=[Relationship(source=Node(id='Gpt-3', type='Technology', properties={}), target=Node(id='2020', type='Year', properties={}), type='RELEASED', properties={}), Relationship(source=Node(id='Gpt-4', type='Technology', properties={}), target=Node(id='2023', type='Year', properties={}), type='RELEASED', properties={}), Relationship(source=Node(id='Chatgpt', type='Technology', properties={}), target=Node(id='2022', type='Year', properties={}), type='RELEASED', properties={}), Relationship(source=Node(id='Gpt-3', type='Technology', properties={}), target=Node(id='Api', type='Technology', properties={}), type='AVAILABLE_VIA', properties={}), Relationship(source=Node(id='Gpt-4', type='Technology', properties={}), target=Node(id='2024', type='Year', properties={}), type='AVAILABLE_ONLY_VIA', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its')), GraphDocument(nodes=[Node(id='Gpt-4', type='Technology', properties={}), Node(id='Openai', type='Organization', properties={}), Node(id='Chatgpt', type='Technology', properties={}), Node(id='Openai O1', type='Technology', properties={}), Node(id='Robotics', type='Field', properties={}), Node(id='Software Engineering', type='Field', properties={}), Node(id='Societal Impact Work', type='Field', properties={})], relationships=[Relationship(source=Node(id='Gpt-4', type='Technology', properties={}), target=Node(id='Openai', type='Organization', properties={}), type='DEVELOPED_BY', properties={}), Relationship(source=Node(id='Gpt-4', type='Technology', properties={}), target=Node(id='Increased Accuracy', type='Attribute', properties={}), type='HAS_ATTRIBUTE', properties={}), Relationship(source=Node(id='Gpt-4', type='Technology', properties={}), target=Node(id='Holy Grail', type='Attribute', properties={}), type='HAS_ATTRIBUTE', properties={}), Relationship(source=Node(id='Gpt-4', type='Technology', properties={}), target=Node(id='Multimodal Capabilities', type='Attribute', properties={}), type='HAS_ATTRIBUTE', properties={}), Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Chatgpt', type='Technology', properties={}), type='RELEASED', properties={}), Relationship(source=Node(id='Chatgpt', type='Technology', properties={}), target=Node(id='Llm Usage', type='Effect', properties={}), type='LEADS_TO', properties={}), Relationship(source=Node(id='Llm Usage', type='Effect', properties={}), target=Node(id='Robotics', type='Field', properties={}), type='INCREASES_USAGE_IN', properties={}), Relationship(source=Node(id='Llm Usage', type='Effect', properties={}), target=Node(id='Software Engineering', type='Field', properties={}), type='INCREASES_USAGE_IN', properties={}), Relationship(source=Node(id='Llm Usage', type='Effect', properties={}), target=Node(id='Societal Impact Work', type='Field', properties={}), type='INCREASES_USAGE_IN', properties={}), Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Openai O1', type='Technology', properties={}), type='RELEASED', properties={}), Relationship(source=Node(id='Openai O1', type='Technology', properties={}), target=Node(id='Long Chains Of Thought', type='Attribute', properties={}), type='GENERATES', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before')), GraphDocument(nodes=[Node(id='Openai', type='Organization', properties={}), Node(id='Openai O1', type='Model', properties={}), Node(id='Gpt Series', type='Model', properties={}), Node(id='Bloom', type='Model', properties={}), Node(id='Llama', type='Model', properties={}), Node(id='Mistral Ai', type='Organization', properties={}), Node(id='Mistral 7B', type='Model', properties={}), Node(id='Mixt', type='Model', properties={})], relationships=[Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Openai O1', type='Model', properties={}), type='RELEASED', properties={}), Relationship(source=Node(id='Openai O1', type='Model', properties={}), target=Node(id='Gpt Series', type='Model', properties={}), type='COMPETES_WITH', properties={}), Relationship(source=Node(id='Bloom', type='Model', properties={}), target=Node(id='Llama', type='Model', properties={}), type='COMPETES_WITH', properties={}), Relationship(source=Node(id='Bloom', type='Model', properties={}), target=Node(id='Mistral Ai', type='Organization', properties={}), type='ASSOCIATED_WITH', properties={}), Relationship(source=Node(id='Llama', type='Model', properties={}), target=Node(id='Mistral Ai', type='Organization', properties={}), type='ASSOCIATED_WITH', properties={}), Relationship(source=Node(id='Mistral Ai', type='Organization', properties={}), target=Node(id='Mistral 7B', type='Model', properties={}), type='DEVELOPS', properties={}), Relationship(source=Node(id='Mistral Ai', type='Organization', properties={}), target=Node(id='Mixt', type='Model', properties={}), type='DEVELOPS', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=\" In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.\\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixt\")), GraphDocument(nodes=[Node(id='Mistral Ai', type='Organization', properties={}), Node(id='Mistral 7B', type='Model', properties={}), Node(id='Mixtral 8X7B', type='Model', properties={}), Node(id='Apache License', type='License', properties={}), Node(id='Deepseek', type='Organization', properties={}), Node(id='Deepseek R1', type='Model', properties={}), Node(id='Openai O1', type='Model', properties={}), Node(id='2023', type='Date', properties={}), Node(id='2025', type='Date', properties={})], relationships=[Relationship(source=Node(id='Mistral Ai', type='Organization', properties={}), target=Node(id='Mistral 7B', type='Model', properties={}), type='HAS_MODEL', properties={}), Relationship(source=Node(id='Mistral Ai', type='Organization', properties={}), target=Node(id='Mixtral 8X7B', type='Model', properties={}), type='HAS_MODEL', properties={}), Relationship(source=Node(id='Mistral 7B', type='Model', properties={}), target=Node(id='Apache License', type='License', properties={}), type='HAS_LICENSE', properties={}), Relationship(source=Node(id='Deepseek', type='Organization', properties={}), target=Node(id='Deepseek R1', type='Model', properties={}), type='HAS_MODEL', properties={}), Relationship(source=Node(id='Deepseek R1', type='Model', properties={}), target=Node(id='Openai O1', type='Model', properties={}), type='PERFORM_COMPARABLY_TO', properties={}), Relationship(source=Node(id='Deepseek R1', type='Model', properties={}), target=Node(id='2025', type='Date', properties={}), type='RELEASED_IN', properties={}), Relationship(source=Node(id='2023', type='Date', properties={}), target=Node(id='2025', type='Date', properties={}), type='TIME_PERIOD', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=\" have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.\\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate\")), GraphDocument(nodes=[Node(id='Llms', type='Concept', properties={}), Node(id='Lmms', type='Concept', properties={}), Node(id='2024', type='Date', properties={})], relationships=[Relationship(source=Node(id='Llms', type='Concept', properties={}), target=Node(id='Lmms', type='Concept', properties={}), type='ALSO_CALLED', properties={}), Relationship(source=Node(id='Lmms', type='Concept', properties={}), target=Node(id='2024', type='Date', properties={}), type='AS_OF', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).\\nAs of 2024, the largest ')), GraphDocument(nodes=[Node(id='Large Language Model', type='Machine learning model', properties={}), Node(id='Natural Language Processing', type='Task', properties={}), Node(id='Self-Supervised Learning', type='Learning method', properties={}), Node(id='Text', type='Data', properties={}), Node(id='Petaflop-Day', type='Measurement', properties={}), Node(id='Flop', type='Unit', properties={})], relationships=[Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Natural Language Processing', type='Task', properties={}), type='DESIGNED_FOR', properties={}), Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Self-Supervised Learning', type='Learning method', properties={}), type='TRAINED_WITH', properties={}), Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Text', type='Data', properties={}), type='TRAINED_ON', properties={}), Relationship(source=Node(id='Petaflop-Day', type='Measurement', properties={}), target=Node(id='Flop', type='Unit', properties={}), type='EQUIVALENT_TO', properties={})], source=Document(metadata={'title': 'List of large language models', 'summary': \"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/List_of_large_language_models'}, page_content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model')), GraphDocument(nodes=[Node(id='Flop', type='Unit', properties={}), Node(id='1 Day', type='Time', properties={}), Node(id='8.64E19', type='Number', properties={})], relationships=[Relationship(source=Node(id='1 Day', type='Time', properties={}), target=Node(id='Flop', type='Unit', properties={}), type='EQUALS', properties={}), Relationship(source=Node(id='8.64E19', type='Number', properties={}), target=Node(id='Flop', type='Unit', properties={}), type='VALUE', properties={})], source=Document(metadata={'title': 'List of large language models', 'summary': \"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/List_of_large_language_models'}, page_content=\"/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\\n\\n\\n== See also ==\\nList of chatbots\\nList of language model benchmarks\\n\\n\\n== Notes ==\\n\\n\\n== References ==\")), GraphDocument(nodes=[Node(id='Language Model', type='Concept', properties={}), Node(id='Natural Language', type='Concept', properties={}), Node(id='Speech Recognition', type='Task', properties={}), Node(id='Machine Translation', type='Task', properties={}), Node(id='Natural Language Generation', type='Task', properties={}), Node(id='Optical Character Recognition', type='Task', properties={}), Node(id='Route Optimization', type='Task', properties={}), Node(id='Handwriting Recognition', type='Task', properties={}), Node(id='Grammar Induction', type='Task', properties={}), Node(id='Information Retrieval', type='Task', properties={}), Node(id='Large Language Models', type='Concept', properties={}), Node(id='Transformers', type='Concept', properties={}), Node(id='Datasets', type='Concept', properties={}), Node(id='Public Internet', type='Concept', properties={}), Node(id='Recurrent Neural Networks', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Natural Language', type='Concept', properties={}), type='IS_A_MODEL_OF', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Speech Recognition', type='Task', properties={}), type='USEFUL_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Machine Translation', type='Task', properties={}), type='USEFUL_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Natural Language Generation', type='Task', properties={}), type='USEFUL_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Optical Character Recognition', type='Task', properties={}), type='USEFUL_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Route Optimization', type='Task', properties={}), type='USEFUL_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Handwriting Recognition', type='Task', properties={}), type='USEFUL_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Grammar Induction', type='Task', properties={}), type='USEFUL_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Information Retrieval', type='Task', properties={}), type='USEFUL_FOR', properties={}), Relationship(source=Node(id='Large Language Models', type='Concept', properties={}), target=Node(id='Transformers', type='Concept', properties={}), type='BASED_ON', properties={}), Relationship(source=Node(id='Large Language Models', type='Concept', properties={}), target=Node(id='Datasets', type='Concept', properties={}), type='TRAINED_ON', properties={}), Relationship(source=Node(id='Datasets', type='Concept', properties={}), target=Node(id='Public Internet', type='Concept', properties={}), type='SCRAPED_FROM', properties={}), Relationship(source=Node(id='Large Language Models', type='Concept', properties={}), target=Node(id='Recurrent Neural Networks', type='Concept', properties={}), type='SUPERSEDED_BY', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-')), GraphDocument(nodes=[Node(id='Noam Chomsky', type='Person', properties={}), Node(id='Language Models', type='Concept', properties={}), Node(id='Formal Grammars', type='Concept', properties={}), Node(id='Programming Languages', type='Concept', properties={}), Node(id='Statistical Approaches', type='Concept', properties={}), Node(id='Rule-Based Models', type='Concept', properties={}), Node(id='Recurrent Neural Network-Based Models', type='Concept', properties={}), Node(id='Word N-Gram Language Model', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Noam Chomsky', type='Person', properties={}), target=Node(id='Language Models', type='Concept', properties={}), type='CONTRIBUTED_TO', properties={}), Relationship(source=Node(id='Language Models', type='Concept', properties={}), target=Node(id='Formal Grammars', type='Concept', properties={}), type='BASED_ON', properties={}), Relationship(source=Node(id='Formal Grammars', type='Concept', properties={}), target=Node(id='Programming Languages', type='Concept', properties={}), type='FOUNDATIONAL_TO', properties={}), Relationship(source=Node(id='Statistical Approaches', type='Concept', properties={}), target=Node(id='Rule-Based Models', type='Concept', properties={}), type='SUPERSEDED', properties={}), Relationship(source=Node(id='Recurrent Neural Network-Based Models', type='Concept', properties={}), target=Node(id='Word N-Gram Language Model', type='Concept', properties={}), type='SUPERSEDED', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n\\n== History ==\\nNoam Chomsky did pioneering work on language models in the 1950s by developing a theory of formal grammars, which became fundamental to the field of programming languages.\\nIn 1980, statistical approaches were explored and found to be more useful for many purposes than rule')), GraphDocument(nodes=[Node(id='Statistical Approaches', type='Concept', properties={}), Node(id='Rule-Based Formal Grammars', type='Concept', properties={}), Node(id='Word N-Gram Language Models', type='Concept', properties={}), Node(id='Probabilities', type='Concept', properties={}), Node(id='Discrete Representations', type='Concept', properties={}), Node(id='Continuous Representations', type='Concept', properties={}), Node(id='Word Embeddings', type='Concept', properties={}), Node(id='Real-Valued Vector', type='Concept', properties={}), Node(id='Meaning Of The Word', type='Concept', properties={}), Node(id='Words', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Statistical Approaches', type='Concept', properties={}), target=Node(id='Rule-Based Formal Grammars', type='Concept', properties={}), type='MORE_USEFUL_THAN', properties={}), Relationship(source=Node(id='Word N-Gram Language Models', type='Concept', properties={}), target=Node(id='Probabilities', type='Concept', properties={}), type='MADE_ADVANCES_WITH', properties={}), Relationship(source=Node(id='Discrete Representations', type='Concept', properties={}), target=Node(id='Word Embeddings', type='Concept', properties={}), type='REPLACED_BY', properties={}), Relationship(source=Node(id='Continuous Representations', type='Concept', properties={}), target=Node(id='Real-Valued Vector', type='Concept', properties={}), type='TYPICALLY_IS', properties={}), Relationship(source=Node(id='Real-Valued Vector', type='Concept', properties={}), target=Node(id='Meaning Of The Word', type='Concept', properties={}), type='ENCODES', properties={}), Relationship(source=Node(id='Words', type='Concept', properties={}), target=Node(id='Meaning Of The Word', type='Concept', properties={}), type='RELATED_TO', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='.\\nIn 1980, statistical approaches were explored and found to be more useful for many purposes than rule-based formal grammars. Discrete representations like word n-gram language models, with probabilities for discrete combinations of words, made significant advances.\\nIn the 2000s, continuous representations for words, such as word embeddings, began to replace discrete representations. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that')), GraphDocument(nodes=[Node(id='Real-Valued Vector', type='Concept', properties={}), Node(id='Word Meaning', type='Concept', properties={}), Node(id='Vector Space', type='Concept', properties={}), Node(id='Plurality', type='Concept', properties={}), Node(id='Gender', type='Concept', properties={}), Node(id='Statistical Language Model', type='Concept', properties={}), Node(id='Ibm', type='Organization', properties={}), Node(id='Shannon-Style Experiments', type='Concept', properties={}), Node(id='Human Performance', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Real-Valued Vector', type='Concept', properties={}), target=Node(id='Word Meaning', type='Concept', properties={}), type='ENCODES', properties={}), Relationship(source=Node(id='Word Meaning', type='Concept', properties={}), target=Node(id='Vector Space', type='Concept', properties={}), type='EXPECTED_TO_BE_SIMILAR', properties={}), Relationship(source=Node(id='Word Meaning', type='Concept', properties={}), target=Node(id='Plurality', type='Concept', properties={}), type='COMMON_RELATIONSHIP', properties={}), Relationship(source=Node(id='Word Meaning', type='Concept', properties={}), target=Node(id='Gender', type='Concept', properties={}), type='COMMON_RELATIONSHIP', properties={}), Relationship(source=Node(id='Statistical Language Model', type='Concept', properties={}), target=Node(id='Ibm', type='Organization', properties={}), type='PROPOSED_BY', properties={}), Relationship(source=Node(id='Ibm', type='Organization', properties={}), target=Node(id='Shannon-Style Experiments', type='Concept', properties={}), type='PERFORMED', properties={}), Relationship(source=Node(id='Shannon-Style Experiments', type='Concept', properties={}), target=Node(id='Human Performance', type='Concept', properties={}), type='ANALYZED_BY', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning, and common relationships between pairs of words like plurality or gender .\\n\\n\\n== Pure statistical models ==\\nIn 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human')), GraphDocument(nodes=[Node(id='Language Modeling', type='Concept', properties={}), Node(id='Human Subjects', type='Person', properties={}), Node(id='Word N-Grams', type='Concept', properties={}), Node(id='Exponential', type='Concept', properties={}), Node(id='Maximum Entropy Language Models', type='Concept', properties={}), Node(id='Feature Functions', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Language Modeling', type='Concept', properties={}), target=Node(id='Human Subjects', type='Person', properties={}), type='IDENTIFIED_BY', properties={}), Relationship(source=Node(id='Language Modeling', type='Concept', properties={}), target=Node(id='Word N-Grams', type='Concept', properties={}), type='BASED_ON', properties={}), Relationship(source=Node(id='Maximum Entropy Language Models', type='Concept', properties={}), target=Node(id='Feature Functions', type='Concept', properties={}), type='ENCODE_RELATIONSHIP', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\n\\n\\n=== Models based on word n-grams ===\\n\\n\\n=== Exponential ===\\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is\\n\\n  \\n    \\n      \\n        P\\n      ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    \\n        P\\n        (\\n        \\n          w\\n          \\n            m\\n          \\n        \\n        ∣\\n   ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      \\n        ∣\\n        \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='\\n        ,\\n        …\\n        ,\\n        \\n          w\\n          \\n            m\\n            −\\n            1\\n  ')), GraphDocument(nodes=[Node(id='1', type='Number', properties={})], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    −\\n            1\\n          \\n        \\n        )\\n        =\\n        \\n          \\n            1\\n           ')), GraphDocument(nodes=[Node(id='Z', type='Entity', properties={}), Node(id='W', type='Entity', properties={})], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='        1\\n            \\n              Z\\n              (\\n              \\n                w\\n                ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='   w\\n                \\n                  1\\n                \\n              \\n              ,\\n             ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      ,\\n              …\\n              ,\\n              \\n                w\\n                \\n              ')), GraphDocument(nodes=[Node(id='M', type='Variable', properties={}), Node(id='1', type='Number', properties={})], relationships=[Relationship(source=Node(id='M', type='Variable', properties={}), target=Node(id='1', type='Number', properties={}), type='SUBTRACTION', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='     \\n                  m\\n                  −\\n                  1\\n                \\n              \\n     ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='              \\n              )\\n            \\n          \\n        \\n        exp\\n        \\u2061\\n        (\\n        ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='��\\n        (\\n        \\n          a\\n          \\n            T\\n          \\n        \\n        f\\n        (\\n      ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    f\\n        (\\n        \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n    ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      ,\\n        …\\n        ,\\n        \\n          w\\n          \\n            m\\n          \\n        \\n        )\\n  ')), GraphDocument(nodes=[Node(id='P(W_{M}|W_{1},...,W_{M-1})', type='Mathematicalexpression', properties={}), Node(id='Z(W_{1},...,W_{M-1})', type='Mathematicalexpression', properties={}), Node(id='A', type='Variable', properties={}), Node(id='F(W_{M})', type='Function', properties={})], relationships=[Relationship(source=Node(id='P(W_{M}|W_{1},...,W_{M-1})', type='Mathematicalexpression', properties={}), target=Node(id='Z(W_{1},...,W_{M-1})', type='Mathematicalexpression', properties={}), type='DEFINED_AS', properties={}), Relationship(source=Node(id='P(W_{M}|W_{1},...,W_{M-1})', type='Mathematicalexpression', properties={}), target=Node(id='A', type='Variable', properties={}), type='INCLUDES', properties={}), Relationship(source=Node(id='P(W_{M}|W_{1},...,W_{M-1})', type='Mathematicalexpression', properties={}), target=Node(id='F(W_{M})', type='Function', properties={}), type='INCLUDES', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='        \\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle P(w_{m}\\\\mid w_{1},\\\\ldots ,w_{m-1})={\\\\frac {1}{Z(w_{1},\\\\ldots ,w_{m-1})}}\\\\exp(a^{T}f(w_{')), GraphDocument(nodes=[Node(id='Z', type='Variable', properties={}), Node(id='W', type='Variable', properties={}), Node(id='A', type='Variable', properties={}), Node(id='F', type='Function', properties={}), Node(id='M', type='Variable', properties={})], relationships=[Relationship(source=Node(id='Z', type='Variable', properties={}), target=Node(id='W', type='Variable', properties={}), type='DEPENDS_ON', properties={}), Relationship(source=Node(id='F', type='Function', properties={}), target=Node(id='W', type='Variable', properties={}), type='DEPENDS_ON', properties={}), Relationship(source=Node(id='F', type='Function', properties={}), target=Node(id='M', type='Variable', properties={}), type='DEPENDS_ON', properties={}), Relationship(source=Node(id='A', type='Variable', properties={}), target=Node(id='F', type='Function', properties={}), type='APPLIES_TO', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='ots ,w_{m-1})}}\\\\exp(a^{T}f(w_{1},\\\\ldots ,w_{m}))}\\n  \\n\\nwhere \\n  \\n    \\n      \\n        Z\\n        (\\n        \\n          w\\n       ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n       ')), GraphDocument(nodes=[Node(id='W', type='Variable', properties={}), Node(id='M', type='Variable', properties={})], relationships=[Relationship(source=Node(id='W', type='Variable', properties={}), target=Node(id='M', type='Variable', properties={}), type='SUBTRACTION', properties={}), Relationship(source=Node(id='M', type='Variable', properties={}), target=Node(id='1', type='Constant', properties={}), type='SUBTRACTION', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='   ,\\n        \\n          w\\n          \\n            m\\n            −\\n            1\\n          \\n        \\n      ')), GraphDocument(nodes=[Node(id='Z(W_{1},\\nDots ,W_{M-1})', type='Function', properties={}), Node(id='Partition Function', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Z(W_{1},\\nDots ,W_{M-1})', type='Function', properties={}), target=Node(id='Partition Function', type='Concept', properties={}), type='IS', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle Z(w_{1},\\\\ldots ,w_{m-1})}\\n  \\n is the partition function, \\n  \\n    \\n      \\n        a\\n      \\n')), GraphDocument(nodes=[Node(id='A', type='Parameter vector', properties={}), Node(id='F(W)', type='Function', properties={})], relationships=[Relationship(source=Node(id='A', type='Parameter vector', properties={}), target=Node(id='F(W)', type='Function', properties={}), type='IS_USED_IN', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='   \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n is the parameter vector, and \\n  \\n    \\n      \\n        f\\n        (\\n        \\n          w\\n  ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n  ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='        ,\\n        \\n          w\\n          \\n            m\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle')), GraphDocument(nodes=[Node(id='Feature Function', type='Concept', properties={})], relationships=[Relationship(source=Node(id='Feature Function', type='Concept', properties={}), target=Node(id='F(W_{1},\\x7f,\\x7f,W_{M})', type='Concept', properties={}), type='IS_REPRESENTED_BY', properties={})], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a model of natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using words scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' )\\n      \\n    \\n    {\\\\displaystyle f(w_{1},\\\\ldots ,w_{m})}\\n  \\n is the feature function. In the simplest case, the feature'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm= ChatOpenAI(api_key=api_key, temperature=0, model_name=\"gpt-4o-mini\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, we can see how the model inferred many entities from the text and created nodes with ids and types to match. Relationships between the entities were also inferred and mapped using edges going from a source node to a target node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived Nodes:\n",
      "[Node(id='Albert Einstein', type='Person', properties={}), Node(id='Marie Curie', type='Person', properties={}), Node(id='Nobel Prize In Physics', type='Award', properties={}), Node(id='Nobel Prize In Chemistry', type='Award', properties={}), Node(id='Theory Of Relativity', type='Concept', properties={}), Node(id='Photoelectric Effect', type='Concept', properties={}), Node(id='Radioactivity', type='Concept', properties={}), Node(id='Radiation', type='Concept', properties={}), Node(id='Radium', type='Element', properties={}), Node(id='Polonium', type='Element', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='Henri Becquerel', type='Person', properties={})]\n",
      "\n",
      "Derived Edges:\n",
      "[Relationship(source=Node(id='Albert Einstein', type='Person', properties={}), target=Node(id='Theory Of Relativity', type='Concept', properties={}), type='KNOWN_FOR', properties={}), Relationship(source=Node(id='Albert Einstein', type='Person', properties={}), target=Node(id='Nobel Prize In Physics', type='Award', properties={}), type='AWARDED', properties={}), Relationship(source=Node(id='Albert Einstein', type='Person', properties={}), target=Node(id='Photoelectric Effect', type='Concept', properties={}), type='EXPLANATION_FOR', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Radioactivity', type='Concept', properties={}), type='KNOWN_FOR', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Nobel Prize In Physics', type='Award', properties={}), type='AWARDED', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Nobel Prize In Chemistry', type='Award', properties={}), type='AWARDED', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Radium', type='Element', properties={}), type='DISCOVERED', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Polonium', type='Element', properties={}), type='DISCOVERED', properties={}), Relationship(source=Node(id='Pierre Curie', type='Person', properties={}), target=Node(id='Nobel Prize In Physics', type='Award', properties={}), type='SHARED_WITH', properties={}), Relationship(source=Node(id='Henri Becquerel', type='Person', properties={}), target=Node(id='Nobel Prize In Physics', type='Award', properties={}), type='SHARED_WITH', properties={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load the document directly from the file\n",
    "loader = TextLoader(\"./datasets/famous_scientists.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatOpenAI(api_key=api_key, model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Instantiate the LLM graph transformer\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# Convert the text documents to graph documents\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(docs)\n",
    "print(f\"Derived Nodes:\\n{graph_documents[0].nodes}\\n\")\n",
    "print(f\"Derived Edges:\\n{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Storing and querying documents**\n",
    "\n",
    "We'll be using `Neo4j` to store and query our graph documents. Neo4j has both cloud-based and local verisons to suit your use cases. \n",
    "\n",
    "__Note 1__: _Firstly, `pip install neo4j`. To download and use locally, visit [here](https://neo4j.com/download/). Download to the 'Neo4j Server'. Then download apoc that is compatible with downloaded Neo4j Server version from [here](https://github.com/neo4j/apoc/releases). It is very important to follow installation instructions provided by Neo4j._\n",
    "\n",
    "__Note 2__: `from langchain_community.graphs import Neo4jGraph` no longer works. Instead, you need to install `langchain-neo4j` and import `Neo4jGraph` from there. Then you are good to go.\n",
    "\n",
    "For our purposes, we'll assume that database is already locally available.\n",
    "\n",
    "We instantiate a graph with the Neo4jGraph class, specifying the URL to the Neo4j database server, and the credentials needed to access it. In a production setting, these credentials should be saved as environment variables rather than being committed to a codebase for better security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "import os\n",
    "\n",
    "# Get the environment variables for Neo4j connection\n",
    "url = os.environ[\"NEO4J_URI\"]\n",
    "user = os.environ[\"NEO4J_USERNAME\"]\n",
    "password = os.environ[\"NEO4J_PASSWORD\"]\n",
    "\n",
    "# Create a Neo4jGraph instance\n",
    "graph = Neo4jGraph(url=url, username=user, password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Storing graph documents**\n",
    "\n",
    "Carrying on from the graph documents we created from Wikipedia results on large language models, we can add these graph documents to our database using the `.add_graph_documents()` method. \n",
    "\n",
    "- `include_source` link nodes to source documents with `MENTIONS` edge, enabling better traceability and context preservation.\n",
    "- `baseEntityLabel` add `__Entity__` label to each node, improving query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm = ChatOpenAI(api_key=api_key, temperature=0, model=\"gpt-4o-mini\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    include_source=True,\n",
    "    baseEntityLabel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what our graph looks like:\n",
    "\n",
    "<img src='./images/graph.jpeg' width=75%>\n",
    "\n",
    "Can play with the nodes and edges in the Neo4j browser - the code `MATCH (n) RETURN n LIMIT 40;` should be written in the query box to see the nodes.\n",
    "\n",
    "Red nodes are the source documents we specified when adding the graph documents; each one has a MENTIONS relationship from the source to the entity mentioned.\n",
    "\n",
    "We can also view the database schema with the `.get_schema` attribute. Here, we can see the different node types and relationships, including their direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Person {name: STRING, age: INTEGER}\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count(n)': 180}]\n"
     ]
    }
   ],
   "source": [
    "# Check if nodes were created\n",
    "result = graph.query(\"MATCH (n) RETURN count(n)\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Querying Neo4j - Cypher Query Language**\n",
    "\n",
    "Neo4j introduced Cypher Query Language in 2011 as a declarative query language for intuitively navigating and manipulating graph data using a SQL-like syntax.\n",
    "\n",
    "<img src='./images/cypher.png' width=50%>\n",
    "\n",
    "### **Querying the LLM graph**\n",
    "\n",
    "Let's query our database of Wikipedia results about LLMs.\n",
    "\n",
    "We'll query the database to find out who developed the GPT-4 model. Our query looks for a match between a model and an organization joined by the `DEVELOPED_BY` relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Organizatiion)} {position: line: 1, column: 56, offset: 55} for query: 'MATCH (gpt4:Model {id: \"Gpt-4\"})-[:DEVELOPED_BY]->(org:Organizatiion)\\n    RETURN org\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "results = graph.query(\n",
    "    \"\"\"MATCH (gpt4:Model {id: \"Gpt-4\"})-[:DEVELOPED_BY]->(org:Organizatiion)\n",
    "    RETURN org\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Creeating the Graph RAG chain**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
