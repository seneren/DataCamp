{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The LangChain Ecosystem**\n",
    "\n",
    "### **What is LangChain?**\n",
    "\n",
    "  <div style=\"display: flex;\">\n",
    "    <!-- Left Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "    <ul>\n",
    "      <li><b>An Open-source</b> framework for connecting:</li>\n",
    "        <ul>\n",
    "          <li>LLMs</li>\n",
    "          <li>Data sources</li>\n",
    "          <li>Other functionality under a <b>unified syntax</b></li>\n",
    "        </ul>\n",
    "      <li>Allows for scalability</li>\n",
    "      <li>Contains modular components</li>\n",
    "      <li>Supports <b>Python</b> and <b>Javascript</b></li>\n",
    "    </ul>\n",
    "    LangChain encompasses an entire ecosystem of tools, but in this course, we'll focus on the core components of the LangChain library: LLMs, including open-source and proprietary models, prompts, chains, agents, and document retrievers. \n",
    "    </div>\n",
    "    <!-- Right Column -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "    <div>\n",
    "    <img src='./images/components-of-lc.png' width=90%>\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### **Hugging Face**\n",
    "\n",
    "- __Open-source__ repository of models, datasets, and tools\n",
    "\n",
    "Accessing LLMs hosted on Hugging Face is free, but isong them in LangChain requires creating a Hugging Face API key.\n",
    "\n",
    "### **Standardizing syntax**\n",
    "\n",
    "Now we have our key, let's use LangChain to use a model from Hugging Face, and compare it to using an OpenAI model. LangChain has OpenAI and HuggingFace classes for interacting with the respective APIs. We'll define an unfinished sentence, and use both models to predict the next words. Finally, let's print the result to see the outputs. \n",
    "\n",
    "**Hugging Face (Falcon-7b)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " while also making a difference in the world?\n",
      "Absolutely! You can have fun while also making a difference in the world. Some ideas include volunteering with your local animal shelters or rescuing animals, doing a clean-up in your community or even participating in a charity run. The possibilities are endless!\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_api_key = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id= \"tiiuae/falcon-7b-instruct\",\n",
    "    huggingfacehub_api_token=hf_api_key\n",
    ")\n",
    "\n",
    "question = \"Can you still have fun\"\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenAI (gpt-3.5-turpo-instruct)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on a diet?\n",
      "\n",
      "Yes, absolutely! A diet doesn't have to be restrictive or boring. You can still enjoy all your favorite foods in moderation and find new, healthy recipes to try. Additionally, finding activities that you enjoy, such as hiking, dancing, or playing sports, can make staying active and following a diet more enjoyable. It's all about finding balance and making choices that are sustainable and make you feel good.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(\n",
    "    model= 'gpt-3.5-turbo-instruct',\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "question = \"Can you still have fun\"\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two different approaches - despite using completely different models from different APIs, LangChain unifies them both into a consistent, modular workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <!-- Left Column (Image) -->\n",
    "    <div style=\"width: 30%; padding-right: 10px;\">\n",
    "        <img src='./images/lc.png' style=\"width: 100%;\">\n",
    "    </div>\n",
    "    <!-- Right Column (Text) -->\n",
    "    <div style=\"width: 60%; padding-left: 10px;\">\n",
    "        LangChain is a fantastic tool for developing and orchestrating natural language systems. <br><br>\n",
    "        <b>Examples:</b>\n",
    "        <ul>\n",
    "            <li>Natural language conversations with documents</li>\n",
    "            <li>Automate tasks</li>\n",
    "            <li>Data analysis</li>\n",
    "        </ul>\n",
    "        LangChain makes implementing AI more intuitive and gives us greater control over the entire workflow.\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompting strategies for chatbots**\n",
    "\n",
    "### **Finding the right model**\n",
    "\n",
    "Thousands of LLMs are available in LangChain via the Hugging Face Hub API. To find language models specifically optimized for chat, search the models section of Hugging Face and filter on Question Answering. Then, take note of the model name so it can be referenced in the code. \n",
    "\n",
    "### **Prompt templates**\n",
    "\n",
    "Once we've selected a model, we can begin prompting it by utilizing prompt templates. Prompt templates act as _reusable recipes for generating prompts_ from user inputs in _a flexible and modular way_. Templates can include \n",
    "- instructions\n",
    "- examples\n",
    "- or any additional context that might help the model complete the task.\n",
    "\n",
    "Prompt templates are created using LangChain's `PromptTemplate` class. \n",
    "\n",
    "We start by creating a template string, which is structured to prompt the AI to answer a question. The curly braces indicate that we'll use dynamic insertion to insert a variable into the string later in the code. \n",
    "\n",
    "To convert this string into a prompt template, we pass it to `PromptTemplate`, specifying any variables representing inputs using the input_variables argument.\n",
    "To show how a variable will be inserted, call the `.invoke()` method on the prompt template and pass it a dictionary to map values to input variables. \n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "print(prompt_template.invoke({\"question\": \"what is LangChain?\"}))\n",
    "```\n",
    "\n",
    "We can see in the output how the question placeholder has been replaced. \n",
    "\n",
    "Output: <br>\n",
    "\n",
    "`text='You are an artificial intelligence assistant, answer the question. What is LangChain?'`\n",
    "\n",
    "### **Integrating PromptTemplate with LLMs**\n",
    "\n",
    "We start by choosing a question-answering LLM from Hugging Face. To integrate the `prompt_template` and model, we use _LangChain Expression Language_, or __LCEL__. Using a pipe creates a __chain__, which, in LangChain, are used to __connect a series of calls__ to different components into a sequence. To pass an input to this chain, we call the `.invoke()` method again with the same dictionary as before. This passes the question into the prompt template, then passes the prompt template into the model for a response.\n",
    "\n",
    "```python\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "llm_chain = prompt_template | llm\n",
    "\n",
    "question = \"What is LangChain?\"\n",
    "print(llm_chain.invoke({\"question\": question}))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangChain is a blockchain platform that allows users to create, manage, and monetize their own digital content. It utilizes smart contracts to ensure secure and transparent transactions, and offers a range of features for creators, including payments, loyalty rewards, and content distribution.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=hf_api_key)\n",
    "llm_chain = prompt_template | llm        # Connect the prompt template and the model\n",
    "\n",
    "question = \"What is LangChain?\"\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chat models**\n",
    "\n",
    "Chat models have a different prompt template class: `ChatPromptTemplate`. This allows us to specify a series of messages to pass to the chat model. This is structured as a list of tuples, where each tuple contains a role and message pair. This list is then passed to the `.from_messages()` method to create the template. \n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are soto zen master Roshi.\"),\n",
    "        (\"human\", \"What is the essence of Zen?\"),\n",
    "        (\"ai\", \"When you are hungry, eat. When you are tired, sleep.\"),\n",
    "        (\"human\", \"respond to the question: {question}\")\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "In the above example, we can see three roles being used: system, human, and ai.\n",
    "\n",
    "- The system role is used to define the model behavior\n",
    "- The human role is used for providing inputs\n",
    "- The ai role is used for defining outputs - which is often used to provide additional examples for the model.\n",
    "\n",
    "### **Integrating ChatPromptTemplate**\n",
    "\n",
    "The ChatOpenAI class is used to access OpenAI's chat models. When instantiating the model, make sure to provide an OpenAI API key. We create our chain again using an LCEL pipe, define a user input, and invoke the chain as before. \n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", api_key=api_key)\n",
    "\n",
    "llm_chain = prompt_template | llm\n",
    "question = \"What is the sound of one hand clapping?\"\n",
    "\n",
    "response = llm_chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original response is: \n",
      "The sound of one hand clapping is the inquiry itself. It invites you to look beyond duality, to experience the nature of sound and silence, form and emptiness. It is a koan, pointing you to the essence of direct experience, rather than a mere answer. Can you listen deeply?\n",
      "\n",
      "The response split into sentences is:\n",
      "The sound of one hand clapping is the inquiry itself.\n",
      "It invites you to look beyond duality, to experience the nature of sound and silence, form and emptiness.\n",
      "It is a koan, pointing you to the essence of direct experience, rather than a mere answer.\n",
      "Can you listen deeply?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are soto zen master Roshi.\"),\n",
    "        (\"human\", \"What is the essence of Zen?\"),\n",
    "        (\"ai\", \"When you are hungry, eat. When you are tired, sleep.\"),\n",
    "        (\"human\", \"respond to the question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", api_key=api_key)\n",
    "\n",
    "llm_chain = prompt_template | llm\n",
    "question = \"What is the sound of one hand clapping?\"\n",
    "\n",
    "response = llm_chain.invoke({\"question\": question})\n",
    "\n",
    "print(\"The original response is: \\n\" + response.content + \"\\n\" + \"\\nThe response split into sentences is:\")\n",
    "\n",
    "\"\"\"The following code is for how to split the response.content into sentences\"\"\"\n",
    "# Function to split the output text into sentences dynamically\n",
    "def split_into_sentences(text):\n",
    "    sentences = []\n",
    "    start = 0\n",
    "    for i, char in enumerate(text):\n",
    "        if char in '.!?':  # Detect sentence-ending punctuation\n",
    "            sentence = text[start:i+1].strip()  # Extract the sentence\n",
    "            sentences.append(sentence)\n",
    "            start = i+1  # Move the start index to the next character\n",
    "    return sentences\n",
    "\n",
    "# Split the response content into sentences\n",
    "sentences = split_into_sentences(response.content)\n",
    "\n",
    "# Print each sentence on a new line\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retaining learning effectively involves several strategies that can enhance your memory and understanding. Here are some tips to help you retain information better:\n",
      "\n",
      "1. **Active Engagement**: Instead of passively reading or listening, engage actively with the material. This can include discussing it with others, teaching it, or applying the concepts in practical situations.\n",
      "\n",
      "2. **Practice Retrieval**: Test yourself frequently on what you've learned. Use flashcards, take quizzes, or simply write down what you remember after studying. This reinforces your memory.\n",
      "\n",
      "3. **Spaced Repetition**: Study the material over spaced intervals rather than cramming. Reviewing information multiple times over days or weeks helps solidify it in your memory.\n",
      "\n",
      "4. **Organize Information**: Break information into manageable chunks and categorize it. Using mind maps or outlines can help you visualize relationships between concepts.\n",
      "\n",
      "5. **Use Mnemonics**: Create acronyms, rhymes, or visual images to help you remember complex information. Mnemonics make recall easier by providing mental cues.\n",
      "\n",
      "6. **Connect New Knowledge to Existing Knowledge**: Relate new information to what you already know. Creating associations can help strengthen your understanding and memory.\n",
      "\n",
      "7. **Stay Physically and Mentally Healthy**: Regular exercise, a balanced diet, sufficient sleep, and stress management techniques can enhance cognitive function and memory retention.\n",
      "\n",
      "8. **Utilize Different Learning Modalities**: Try to incorporate visual, auditory, and kinesthetic learning techniques. Use videos, podcasts, hands-on activities, and reading to reinforce your learning.\n",
      "\n",
      "9. **Review and Reflect**: Take time to review what youâ€™ve learned regularly. Reflect on it, think about how it applies to your life, and ask yourself questions to deepen your understanding.\n",
      "\n",
      "10. **Stay Curious and Motivated**: Find ways to make the material interesting and relevant to you. A genuine interest in the subject can increase your motivation to learn and retain information.\n",
      "\n",
      "By implementing these strategies, you can improve your ability to retain information and enhance your overall learning experience.\n"
     ]
    }
   ],
   "source": [
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\t\t\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain the prompt template and model, and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "response = llm_chain.invoke({\"question\": \"How can I retain learning?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Limitations of standard prompt templates**\n",
    "\n",
    "<div style=\"display: flex; aligm-items: flex-start;\">\n",
    "    <!-- Left Column (Text) -->\n",
    "    <div style=\"width: 60%; padding-right: 10px;\">\n",
    "        So far, we've used <code>PromptTemplate</code> and <code>ChatPromptTemplate</code> to create reusable templates for different prompt inputs. These classes are great for<br>\n",
    "        <ul>\n",
    "            <li>Handling small number of examples</li>\n",
    "        </ul>\n",
    "        However,<br>\n",
    "        <ul>\n",
    "            <li>They don't scale well to large numbers of examples from a dataset.</li>\n",
    "        </ul>\n",
    "         The <code>FewShotPromptTemplate</code> class allows us to convert datasets like on the right into prompt templates to provide more context to the model.\n",
    "    </div>\n",
    "    <!-- Right Column (Image) -->\n",
    "    <div style=\"width: 50%; padding-left: 10px;\">\n",
    "        <img src='./images/fewshotprompttemplate.png' style=\"width: 100%;\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### **Building an example set**\n",
    "\n",
    "Let's say we have a list of dictionaries containing questions and answers like in above examples list. If we have another data structure, like a pandas `DataFrame`, there's usually a simple transformation to get to this point, like the `.to_dict()` method in this case:\n",
    "\n",
    "```python\n",
    "# Convert DataFrame to list of dicts\n",
    "\n",
    "examples = df.to_list(orient=\"records\")\n",
    "```\n",
    "\n",
    "We need to decide how we want to structure the examples for the model. We create a prompt template, using the `PromptTemplate` class we've used before to specify how the questions and answers should be formatted. Invoking this template with an example question and answer, we can see the `\"Question\"` prefix was added, and a new line was inserted:\n",
    "\n",
    "<img src='./images/formatting-examples.png' width=60%>\n",
    "\n",
    "### **FewShotPromtTemplate**\n",
    "\n",
    "Now to put everything together! `FewShotPromptTemplate` takes the examples list of dictionaries we created, and the template for formatting the examples. Additionally, we can provide a `suffix`, which is used to format the user input, and specify what variable the user input will be assigned to. \n",
    "\n",
    "- `examples`: The list of dictionaries\n",
    "- `example_prompt`: Formatted template\n",
    "- `suffix`: suffix to add to the input\n",
    "- `input_variables`: list of variables to pass to the template\n",
    "\n",
    "```python\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "```\n",
    "\n",
    "### **Invoking the few-shot prompt template**\n",
    "\n",
    "```python\n",
    "prompt = prompt_template.invoke({\"input\": \"what is the name of Henry Campbell's dog?\"})\n",
    "\n",
    "print(prompt.text)\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "<img src='./images/invoking-few-shot.png' width=60%>\n",
    "\n",
    "### **Integration with a chain**\n",
    "\n",
    "Now let's test that this prompt template is actually functional in an LLM chain. We instantiate our model, and chain the prompt template and model together using the pipe operator from LCEL. The model response can be extract from the response object via the `.content` attribute, which shows the model was able to use the context provided in our few-shot prompt.\n",
    "\n",
    "<img src='./images/pluto.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many DataCamp courses has Jack completed?\n",
      "36\n",
      "\n",
      "Question: How much XP does Jack have on DataCamp?\n",
      "284,320XP\n",
      "\n",
      "Question: What technology does Jack learn about most on DataCamp?\n",
      "Python\n",
      "\n",
      "Question: What is Jack's favorite technology on DataCamp?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "# Create the examples list of dicts\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"How many DataCamp courses has Jack completed?\",\n",
    "        \"answer\": \"36\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How much XP does Jack have on DataCamp?\",\n",
    "        \"answer\": \"284,320XP\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What technology does Jack learn about most on DataCamp?\",\n",
    "        \"answer\": \"Python\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Complete the prompt for formatting answers\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "\n",
    "# Create the few-shot prompt\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "# Invoke the prompt template\n",
    "prompt = prompt_template.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"})\n",
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack's favorite technology on DataCamp is Python.\n"
     ]
    }
   ],
   "source": [
    "# Create an OpenAI chat LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
    "\n",
    "# Create and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "print(llm_chain.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"}).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
