{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A car dashboard with lots of new technical features.](dashboard.jpg)\n",
    "\n",
    "You're working for a well-known car manufacturer who is looking at implementing LLMs into vehicles to provide guidance to drivers. You've been asked to experiment with integrating car manuals with an LLM to create a context-aware chatbot. They hope that this context-aware LLM can be hooked up to a text-to-speech software to read the model's response aloud.\n",
    "\n",
    "As a proof of concept, you'll integrate several pages from a car manual that contains car warning messages and their meanings and recommended actions. This particular manual, stored as an HTML file, `mg-zs-warning-messages.html`, is from an MG ZS, a compact SUV. Armed with your newfound knowledge of LLMs and LangChain, you'll implement Retrieval Augmented Generation (RAG) to create the context-aware chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project Instructions**\n",
    "\n",
    "The car manual HTML document has been loaded for you as `car_docs`. Using Retrieval Augmented Generation (RAG) to make an LLM of your choice (OpenAI's `gpt-4o-mini` is recommended) aware of the contents of `car_docs`, answer the following user query:\n",
    "\n",
    "```python\n",
    "\"The Gasoline Particular Filter Full warning has appeared. What does this mean and what should I do about it?\"\n",
    "```\n",
    "\n",
    "- Store the answer to the user query in the variable `answer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to approach the project**\n",
    "\n",
    "1. Split the document\n",
    "   - Split the HTML document into chunks:\n",
    "     - Initializing a splitter:<br>\n",
    "       Use the `RecursiveCharacterTextSplitter` class from `langchain_text_splitters` for splitting documents. Its `chunk_size` argument sets how long should each text chunk should be, and `chunk_overlap` sets how much the chunks should overlap.\n",
    "     - Splitting the text: <br>\n",
    "       Use the `.split_documents()` method to actually split the texts.\n",
    "2. Store embeddings\n",
    "   - Embed and store the document chunks for retrieval:\n",
    "     - Where to store the embeddings: <br>\n",
    "       There are a few options when it comes to vector storage: you can create a vector database locally using _FAISS_ or _ChromaDB_, a cloud-based vector database like _Pinecone_, or even save the vectors in an easily retrievable file type like a JSON.\n",
    "     - Storing embeddings in a Chroma vector database: <br>\n",
    "       Use the `Chroma.from_documents()` method to store the document chunks, specifying the documents to store with the `documents` argument and the embedding function to use to `embedding`.\n",
    "3. Create a retriever\n",
    "   - Create a retriever to retrieve relevant documents from the vector store: <br>\n",
    "     - Create a Chroma retriever: <br>\n",
    "       Use the `.as_retriever()` method on the vectorstore you created.\n",
    "4. Initialize the LLM and prompt template\n",
    "   - Define an LLM and create a prompt template to set up the RAG workflow: <br>\n",
    "     - Initialize the LLM: <br>\n",
    "       To define the LLM, use `ChatOpenAI()` with the model argument set to `gpt-4o-mini`. The temperature argument should, by default, be zero. A higher temperature will give more creative outputs.\n",
    "     - Define a chat prompt template: <br>\n",
    "       - You can create an instance of the `ChatPromptTemplate` class and use the `.from_template()` method to convert a string into a chat prompt template.\n",
    "       - Specify variables to dynamically insert into the string using curly braces `{}`, e.g., `{context}` for inserting some context during the chain.\n",
    "5. Define RAG chain\n",
    "   - Define RAG chain to connect the retriever, question, prompt, and LLM.\n",
    "     - Defining the RAG chain using LangChain Expression Language (LCEL): <br>\n",
    "       To define our RAG chain, you can use the following syntax:\n",
    "        ```python\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        ```\n",
    "6. Invoke RAG chain\n",
    "   - Invoke your chain with the user query to answer.\n",
    "     - Invoking the RAG: <br>\n",
    "       You can use the `.invoke()` method on the chain you created, passing in the query as its only argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gasoline Particular Filter Full warning indicates that the gasoline particular filter is full. You should consult an MG Authorised Repairer as soon as possible to address this issue.\n"
     ]
    }
   ],
   "source": [
    "# Set your API key to a variable\n",
    "import os\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Import the required packages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load the HTML as a LangChain document loader\n",
    "loader = UnstructuredHTMLLoader(file_path=\"./datasets/mg-zs-warning-messages.html\")\n",
    "car_docs = loader.load()\n",
    "\n",
    "\n",
    "# Split the document\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Split the document into chunks\n",
    "car_docs_split = splitter.split_documents(car_docs)\n",
    "\n",
    "\n",
    "# Create the embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key, model='text-embedding-3-small')\n",
    "\n",
    "# Store the embeddings in a Chroma vector database\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=car_docs_split,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./datasets/\")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":2}\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=openai_api_key,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define a prompt template\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\n",
    "Context: {context}\\n\n",
    "Question: {question}\\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create a chain using LCEL\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "query = (\"The Gasoline Particular Filter Full warning has appeared. What does this mean and what should I do about it?\")\n",
    "answer = chain.invoke(query).content\n",
    "\n",
    "# Print the result\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
