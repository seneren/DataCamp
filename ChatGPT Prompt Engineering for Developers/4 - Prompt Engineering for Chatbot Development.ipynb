{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompt engineering for chatbot development**\n",
    "\n",
    "### **The need for prompt engineering for chatbots**\n",
    "\n",
    "Why is prompt engineering vital for chatbot development?\n",
    "\n",
    "- Difficult to predict user questions\n",
    "- Challenge to guarantee effective responses\n",
    "- Prompt engineering guides chatbot behavior\n",
    "\n",
    "### **Chatbot prompt engineering with OpenAI API**\n",
    "\n",
    "Until now, we've only examined user messages due to the emphasis on user prompts.\n",
    "- Each message has a designated role\n",
    "- Focus has been on user prompts\n",
    "\n",
    "We'll focus on system messages to build a chatbot since these guide the model behavior when answering users.\n",
    "\n",
    "- System prompts guide chatbot's behavior\n",
    "\n",
    "### **Changing get_response() for chatbot**\n",
    "\n",
    "Previously, we wrapped code into the `get_response()` function for a single prompt. Now, we will modify it to receive two prompts: a system and a user prompt. We will have to pass two messages to the function whenever we call it.\n",
    "\n",
    "```python\n",
    "def get_response(system_prompt, user_prompt):\n",
    "    messages= [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"ccontent\": user_prompt}\n",
    "    ]\n",
    "    response =client.chat.completions.create(\n",
    "        model= \"gpt-3.5-turbo\",\n",
    "        messages = messages,\n",
    "        temperature - 0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "```\n",
    "\n",
    "### **System message: define purpose**\n",
    "\n",
    "- Allow chatbot to offer domain-accurate assistance\n",
    "- Not defining purpose might lead to contextually irrelevant answers\n",
    "\n",
    "### **System message: response guidelines**\n",
    "\n",
    "- Specify audience, tone, length, structure\n",
    "\n",
    "We won't need to define everything but should be specific about what suits our use case. \n",
    "\n",
    "### **System message: behavior guidance**\n",
    "\n",
    "- Conditional prompts to respond to questions\n",
    "\n",
    "For example, we might not want the financial chatbot to answer queries not related to finance.\n",
    "\n",
    "```python\n",
    "system_prompt = \"\"\"You are a chatbot that answers financial questions.\n",
    "Your answers should be precise, formal and objective.\n",
    "If the question you receive is within the financial field, answer it to the best of your knowledge.\n",
    "Otherwise, answer with 'Sorry, I only know about finance.'\"\"\"\n",
    "\n",
    "user_prompt = \"How's the weather today?\"\n",
    "\n",
    "print(get_response(system_prompt, user_prompt))\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "`Sorry, I only know about finance.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Role-playing prompts for chatbots**\n",
    "\n",
    "### **Role-playing prompts**\n",
    "- Tell chatbot to play a specific role\n",
    "- Chatbot -> actor in a play\n",
    "- Chatbot adjusts to match role\n",
    "- Tailored language and content to fit the persona\n",
    "- More effective interactions\n",
    "\n",
    "This approach ensures effective interactions, especially for domain-specific chatbots. The chatbot learns these roles from its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Incorporating external context**\n",
    "\n",
    "### **The need for external context**\n",
    "\n",
    "We use a pre-trained language model when building a chatbot using the OpenAI API.\n",
    "\n",
    "- Pre-trained language models recognize information they are trained on\n",
    "- Need to provide more context\n",
    "- More accuracy and effectiveness\n",
    "\n",
    "### **How to give extra information**\n",
    "\n",
    "- Sample pevious conversations\n",
    "- System's prompt\n",
    "\n",
    "__Sample Conversations__\n",
    "\n",
    "- Disadvantage: might need a lot of samples\n",
    "\n",
    "<img src='./images/sample-conversation.png' width=70% height=70%>\n",
    "\n",
    "### **System prompt**\n",
    "\n",
    "A more effective way of doing this is to provide the context inside the system_prompt. \n",
    "\n",
    "<img src='./images/system-prompt.png' width=70% height=70%>\n",
    "\n",
    "### **Final Note**\n",
    "\n",
    "One thing to note is that these methods will work well for relatively small contexts because any LLM will have some limitations in the amount of context it can handle. When a large amount of context is needed, more sophisticated techniques should be applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
