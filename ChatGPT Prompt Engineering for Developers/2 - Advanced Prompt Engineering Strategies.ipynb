{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Few-shot prompting**\n",
    "\n",
    "Few-shot prompting involves giving language models examples within the prompt. We create a prompt with example question-answer pairs and the question we want the model to answer.\n",
    "\n",
    "We fed this prompt to the model, and we get the answer to our question in return. With this approach, the model learns how to answer the given question from the examples.\n",
    "\n",
    "<img src='./images/few-shots-prompting.png' width=60% height=60%>\n",
    "\n",
    "* Number of examples:\n",
    "  * Zero → Zero-shot prompting\n",
    "  * One → One-shot prompting\n",
    "  * More than one → Few-shot prompting\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"Today the weather is fantastic\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"positive\"},\n",
    "        {\"role\": \"user\", \"content\": \"I don't like your attitude\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"negative\"},\n",
    "        {\"role\": \"user\", \"content\": \"That shot selection was awful\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "`negative`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-step prompting**\n",
    "\n",
    "__Writing a travel blog post__\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <!-- Left Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "        <ul>\n",
    "            <li>Break down an end gol into series of steps</li>\n",
    "            <li>Model goes through each step to improve accuracy for the final output</li>\n",
    "            <li>Multi-step prompts are used for:</li>\n",
    "            <ul>\n",
    "                <li>Sequential tasks; like generating coherent text from an outline</li>\n",
    "                <li>Cognitive tasks; such as evaluating a solution's correctness, as theyinvolve problem-solving and decision-making processes</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <!-- Right Column -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "        <img src='./images/multi-step-prompt.png' width=25%>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "```python\n",
    "prompt= \"\"\"Compose a travel blog as follows:\n",
    "Step 1: Introduce the destination.\n",
    "Step 2: Share personal adventures during the trip.\n",
    "Step 3: Summarize the journey.\n",
    "\"\"\"\n",
    "\n",
    "print(get_response(prompt))\n",
    "```\n",
    "Output: <br>\n",
    "<img src='./images/travel-blog-output.png' width=75% height=75%>\n",
    "\n",
    "__Analyzing solution correctness__\n",
    "\n",
    "As a starting point, we may start with a single-step prompt asking the model to assess the code's correctness. However, a simple 'yes' doesn't explain the evaluation. We may need specific criteria or domain knowledge to refine the evaluation. In this case, while the syntax is correct, the divide function doesn't handle division by zero, which should be considered in the evaluation.\n",
    "\n",
    "```python\n",
    "calculator = \"\"\"\n",
    "def add(a, b):\n",
    "return a + b\n",
    "def subtract(a, b):\n",
    "return a - b\n",
    "def multiply(a, b):\n",
    "return a * b\n",
    "def divide(a, b):\n",
    "return a / b\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Determine the correctness of the code delimited by triple backticks as follows:\n",
    "Step 1: Check the code correctness in each function.\n",
    "Step 2: Verify if the divide function handles the case when dividing by 0.\n",
    "Code:\n",
    "```{calculator}```\"\"\"\n",
    "\n",
    "print(get_response(prompt))\n",
    "```\n",
    "output: <br>\n",
    "<img src='./images/calculator-output.png' width=75% height=75%>\n",
    "\n",
    "__Multi-step vs. few-shot prompt__\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <!-- Left Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "        <div style=\"text-align: center;\">\n",
    "            Multi-step prompt\n",
    "        </div>\n",
    "        <ul>\n",
    "            <li>Explicitly tell model what to do. They act as a roadmap for the model by providing specific guidance.</li>\n",
    "        </ul>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src='./images/multi-step-prompt-full.png' width=25%>\n",
    "        </div>\n",
    "    </div>\n",
    "    <!-- Right Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "        <div style=\"text-align: center;\">\n",
    "            Few-shot prompt\n",
    "        </div>\n",
    "        <ul>\n",
    "            <li>Questions and answets model loeans from. They demonstrate how the model should respond to certain inputs. The model observes these examples and learns to generalize from them.</li>\n",
    "        </ul>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src='./images/few-shot-prompt.png' width=24%>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chain-of-thought and self-consistency prompting**\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <!-- Left Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "        <ul>\n",
    "            <li>Requires LLMs to provide reasoning steps (thoughts) before giving answer</li>\n",
    "            <li>Used for complex reasoning tasks</li>\n",
    "            <li>Help reduce model errors</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <!-- Right Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "        <img src='./images/chain-of-thought-prompt.png' width=24%>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <!-- Left Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "        <b>Standard prompting to solve a reasoning task</b>\n",
    "        <pre><code>\n",
    "prompt = \"\"\"Q: You start with 15 books in your collection. At the bookstore, you purchase 8 new books. Then, you lend 3 to your friend and 2 to your cousin. Later, you visit another bookstore and buy 5 more books. How many books do you have now?\n",
    "A: The answer is\"\"\"\n",
    "    \n",
    "print(get_response(prompt))\n",
    "</code></pre>\n",
    "    <img src='./images/25-books-reasoning.png'>\n",
    "    </div>\n",
    "    <!-- Right Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "        <b>Chain-of-thought prompting to solve a reasoning task</b>\n",
    "        <pre><code>\n",
    "prompt = \"\"\"Q: You start with 15 books in your collection. At the bookstore, you purchase 8 new books. Then, you lend 3 to your friend and 2 to your cousin. Later, you visit another bookstore and buy 5 more books. How many books do you have now?\n",
    "A: Let's think step by step\"\"\"\n",
    "\n",
    "print(get_response(prompt))\n",
    "</code></pre>\n",
    "    <img src='./images/25-books.png'>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### **Chain-of-thought prompting with few-shots**\n",
    "\n",
    "```python\n",
    "example = \"\"\"\"\n",
    "Q: The odd numbers in this group add up to an even number: 9, 10, 13, 4, 2.\n",
    "A: Adding all the odd numbers (9, 13) gives 22. The answer is True.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"Q: The odd numbers in this group add up to an even number: 15, 13, 82, 7.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = example + question\n",
    "print(get_response(prompt))\n",
    "```\n",
    "Output: <br>\n",
    "`Adding all the odd numbers (15,13,7) gives 35. The answer is False.`\n",
    "\n",
    "While multi-step prompts incorporate steps _inside the prompt_, Chain-of-thought prompts _ask_ model to generate intermediate steps.\n",
    "\n",
    "### **Limitation**\n",
    "\n",
    "* one unsuccessful thought → unsuccesfull outcome. <br>\n",
    "\n",
    "<img src='./images/unsuccessful.png' width=15%>\n",
    "\n",
    "### **Self-consistency prompting**\n",
    "\n",
    "* Generates multiple chain-of-thoughts by prompting the model several times\n",
    "* Majority vote to obtain final output\n",
    "\n",
    "<img src='./images/multiple-chain-of-thought.png' width=35%>\n",
    "\n",
    "Can be done by defining multiple prompts or a prompt generating multiple responses.\n",
    "\n",
    "```python\n",
    "self_consistency_instruction = \"Imagine three completely independent experts who reason differently are answering this question. The final answer is obtained by majority vote. The question is: \"\n",
    "\n",
    "problem_to_solve= \"If there are 10 cars in the parking lot and 3 more cars arrive. Half the original number of cars leave. Then, half of the current number of cars arrive. How many cars are there in the parking?\"\n",
    "\n",
    "prompt = self_consistency_instruction + problem_to_solve\n",
    "print(get_response(prompt))\n",
    "```\n",
    "\n",
    "Output: <br>\n",
    "<img src='./images/self-consistency-output.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Iterative prompt engineering and refinement**\n",
    "\n",
    "\n",
    "### **Iterative prompt engineering**\n",
    "* No prompt can be perfect at the beginning\n",
    "* Prompt Engineering is an iterative process where we:\n",
    "  * Build a prompt\n",
    "  * Feed it to the model\n",
    "  * Observe and analyze the output\n",
    "  * Reiterate to make the prompt better\n",
    "\n",
    "Sometimes, we must refine our prompt because the model misunderstands the request. For example, asking it to generate an Excel sheet with five students' names and grades might prompt it to create an actual Excel file, which it can't do. Refining the prompt to ask for a table that we can copy into Excel gives us a better output.\n",
    "\n",
    "  <div style=\"display: flex;\">\n",
    "    <!-- Left Column -->\n",
    "    <div style=\"width: 45%; padding: 10px;\">\n",
    "    <b>Initial prompt</b>\n",
    "    <div>\n",
    "    <img src='./images/initial-prompt.png' width=75%>\n",
    "    </div>\n",
    "    </div>\n",
    "    <!-- Right Column -->\n",
    "    <div style=\"width: 48%; padding: 10px;\">\n",
    "    <b>Refined prompt</b>\n",
    "    <div>\n",
    "    <img src='./images/refined-prompt.png' width=75%>\n",
    "    </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### **Prompt refinement for various prompt types**\n",
    "\n",
    "Prompt refinement applies to all prompt types. \n",
    "\n",
    "* Few-shot prompts: refine examples\n",
    "* Multi-step prompts: refine guiding steps\n",
    "* Chain-of-thought and self-consistency prompts: refine problem description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
